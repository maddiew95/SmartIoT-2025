{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "normal_indices = np.where(label == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.init_size = (14, 2, 2, 2)  # Initial shape before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, int(torch.prod(torch.tensor(self.init_size)))))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm3d(14),\n",
    "            nn.Upsample(scale_factor=2),  # (N, 14, 4, 4, 4)\n",
    "            nn.Conv3d(14, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),  # (N, 128, 8, 8, 8)\n",
    "            nn.Conv3d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(64, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=(2, 2, 2)),  # (N, 64, 16, 16, 16)\n",
    "            nn.Conv3d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(32, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(32, 14, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Upsample(size=(10, 15, 30)),  # Final shape\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], *self.init_size)\n",
    "        return self.conv_blocks(out)\n",
    "\n",
    "\n",
    "\n",
    "# Discriminator (Critic)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(14, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv3d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))  # Ensures fixed output size\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1)  # 512 channels after AdaptiveAvgPool3d\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Gradient Penalty\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = torch.ones_like(d_interpolates).to(device)\n",
    "    gradients = grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.reshape(gradients.size(0), -1)\n",
    "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "# Initialize models\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "lambda_gp = 20\n",
    "n_critic = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(normal_indices[0]), batch_size):\n",
    "        real_data_np = data[normal_indices[i:i+batch_size]]\n",
    "        real_data_tensor = torch.tensor(real_data_np, dtype=torch.float32)\n",
    "\n",
    "        real_data = real_data_tensor.reshape(-1, 4500, 14).permute(0, 2, 1)  # (690, 14, 4500)\n",
    "        real_data = real_data.reshape(-1, 14, 10, 15, 30).to(device)         # (690, 14, 10, 15, 30)\n",
    "       \n",
    "\n",
    "        # Train Discriminator\n",
    "        for _ in range(n_critic):\n",
    "            z = torch.randn(real_data.size(0), latent_dim).to(device)\n",
    "            fake_data = generator(z).detach()\n",
    "            d_real = discriminator(real_data)\n",
    "            d_fake = discriminator(fake_data)\n",
    "            gp = compute_gradient_penalty(discriminator, real_data, fake_data)\n",
    "            loss_D = -torch.mean(d_real) + torch.mean(d_fake) + lambda_gp * gp\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(real_data.size(0), latent_dim).to(device)\n",
    "        fake_data = generator(z)\n",
    "        loss_G = -torch.mean(discriminator(fake_data))\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{n_epochs}] Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}, GP: {gp.item():.4f}\")\n",
    "\n",
    "    # Optional: Save model checkpoints\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(generator.state_dict(), f\"generator_epoch_{epoch+1}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ee542",
   "metadata": {},
   "source": [
    "# Generate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809752",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim).to(device)\n",
    "generator.load_state_dict(torch.load(\"generator_epoch_30.pth\")) # Use weights from the last epoch\n",
    "num_samples = len(data[normal_indices]) # or however many samples you want\n",
    "z = torch.randn(num_samples, 100).to(device)\n",
    "generated_samples = generator(z).detach().cpu().numpy()\n",
    "generated_samples = generated_samples.reshape(num_samples, 14, 4500).transpose(0, 2, 1)  # (num_samples, 4500, 14)\n",
    "combine_data = np.concatenate((generated_samples, data), axis=0)  # Combine real and generated data\n",
    "combine_labels = np.concatenate((np.zeros(num_samples), label), axis=0)  # Labels: 0 for real, 0 for generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901a15",
   "metadata": {},
   "source": [
    "# Processing: Mel Spec > Resizing > Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01061dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and convert to 3-channel image\n",
    "def resize_spectrogram(spectrogram):\n",
    "    spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-6)\n",
    "    spectrogram = np.uint8(spectrogram.cpu().numpy() * 255)\n",
    "    spectrogram = np.stack([spectrogram] * 3, axis=-1)\n",
    "    image = Image.fromarray(spectrogram)\n",
    "    image = transforms.Resize((224, 224))(image)\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "# Process dataset\n",
    "def process_dataset(data):\n",
    "    num_samples, _, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, num_channels, 4096))\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=2500000, n_mels=128).to(device)\n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            img = resize_spectrogram(mel)\n",
    "            with torch.no_grad():\n",
    "                feat = model(img.unsqueeze(0).to(device))\n",
    "            features[i, j, :] = feat.squeeze().cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f39ba8",
   "metadata": {},
   "source": [
    "# AE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "\n",
    "# Train autoencoder\n",
    "def train_autoencoder(features, epochs=20, batch_size=128):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_eval(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))\n",
    "  print(\"F1 = {}\".format(f1_score(labels, predictions)))\n",
    "  print(confusion_matrix(labels, predictions))\n",
    "\n",
    "# Plot reconstruction error histogram\n",
    "def plot_reconstruction_error(model, features, percentile=95):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=64)\n",
    "    errors = []\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            batch_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            errors.extend(batch_errors.cpu().numpy())\n",
    "\n",
    "    threshold = np.percentile(errors, percentile)\n",
    "    anomalies = np.sum(np.array(errors) > threshold)\n",
    "\n",
    "    plt.hist(errors, bins=50, alpha=0.75)\n",
    "    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({percentile}%)')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reconstruction Error Histogram')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "    print(f\"Detected anomalies: {anomalies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae6f0a",
   "metadata": {},
   "source": [
    "# Cross Validation without Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "features = process_dataset(combine_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64ca03",
   "metadata": {},
   "source": [
    "# Cross Validation with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = StandardScaler().fit_transform(combine_data.reshape(-1, data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e602f",
   "metadata": {},
   "source": [
    "# Cross Validation with MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = MinMaxScaler().fit_transform(combine_data.reshape(-1, combine_data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25980ecb",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Comparing with and without normalizing data \n",
    "\n",
    "### MinMaxed scored\n",
    "\n",
    "Accuracy = 0.7126436781609196\n",
    "\n",
    "Precision = 0.1111111111111111\n",
    "\n",
    "Recall = 0.05555555555555555\n",
    "\n",
    "F1 = 0.07407407407407407\n",
    "\n",
    "[[122  16]\n",
    "\n",
    "[ 34   2]]\n",
    "\n",
    "---\n",
    "\n",
    "### StandardScaled scored\n",
    "\n",
    "\n",
    "Accuracy = 0.896551724137931\n",
    "\n",
    "Precision = 1.0\n",
    "\n",
    "Recall = 0.5\n",
    "\n",
    "F1 = 0.6666666666666666\n",
    "\n",
    "[[138   0]\n",
    "\n",
    "[ 18  18]]\n",
    "\n",
    "---\n",
    "\n",
    "### Without any normlaization scored (highest in cross-val):\n",
    "\n",
    "Accuracy = 0.896551724137931\n",
    "\n",
    "Precision = 1.0\n",
    "\n",
    "Recall = 0.5\n",
    "\n",
    "F1 = 0.6666666666666666\n",
    "\n",
    "[[138   0]\n",
    "\n",
    "[ 18  18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5a291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
