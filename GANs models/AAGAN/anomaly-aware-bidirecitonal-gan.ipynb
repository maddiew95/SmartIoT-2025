{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Anomaly Aware GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyAwareBiGAN(nn.Module):\n",
    "    def __init__(self, latent_dim=64, channels=14, seq_len=4500):\n",
    "        super(AnomalyAwareBiGAN, self).__init__()\n",
    "        self.generator = BiGANGenerator(latent_dim, channels, seq_len)\n",
    "        self.encoder = Encoder(channels, seq_len, latent_dim)\n",
    "        self.discriminator = BiGANDiscriminator(channels, seq_len, latent_dim)\n",
    "        \n",
    "        # Additional anomaly-aware components\n",
    "        self.anomaly_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def compute_anomaly_aware_loss(self, real_data, fake_data, encoded_z, random_z):\n",
    "        # Standard BiGAN losses\n",
    "        real_validity = self.discriminator(real_data, encoded_z)\n",
    "        fake_validity = self.discriminator(fake_data, random_z)\n",
    "        \n",
    "        # Anomaly-aware loss: encourage encoded_z to be distinguishable from random_z\n",
    "        anomaly_real = self.anomaly_discriminator(encoded_z)\n",
    "        anomaly_fake = self.anomaly_discriminator(random_z)\n",
    "        \n",
    "        # Additional reconstruction consistency loss\n",
    "        reconstructed = self.generator(encoded_z)\n",
    "        reconstruction_loss = torch.mean((real_data - reconstructed) ** 2)\n",
    "        \n",
    "        return {\n",
    "            'bigan_loss': torch.mean(real_validity) - torch.mean(fake_validity),\n",
    "            'anomaly_loss': torch.mean(anomaly_real) - torch.mean(anomaly_fake),\n",
    "            'reconstruction_loss': reconstruction_loss\n",
    "        }\n",
    "    \n",
    "# Data preprocessing with normalization\n",
    "def preprocess_data(data, scaler=None, fit_scaler=True):\n",
    "    \"\"\"\n",
    "    Preprocess data with normalization\n",
    "    Returns normalized data and scaler for denormalization\n",
    "    \"\"\"\n",
    "    original_shape = data.shape\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    if fit_scaler:\n",
    "        normalized_data = scaler.fit_transform(data_reshaped)\n",
    "    else:\n",
    "        normalized_data = scaler.transform(data_reshaped)\n",
    "    \n",
    "    return normalized_data.reshape(original_shape), scaler\n",
    "\n",
    "def denormalize_data(data, scaler):\n",
    "    \"\"\"\n",
    "    Denormalize data back to original scale\n",
    "    \"\"\"\n",
    "    original_shape = data.shape\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    denormalized_data = scaler.inverse_transform(data_reshaped)\n",
    "    return denormalized_data.reshape(original_shape)\n",
    "\n",
    "# Encoder for BiGAN (adapted for multivariate time series)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=14, seq_len=4500, latent_dim=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Calculate feature map sizes after convolutions\n",
    "        # After conv1: seq_len/2, After conv2: seq_len/4, After conv3: seq_len/8, After conv4: seq_len/16\n",
    "        final_seq_len = seq_len // 16  # ~281\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Second conv block  \n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size\n",
    "        self.flattened_size = 512 * final_seq_len\n",
    "        \n",
    "        # Final layers to latent space\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flattened_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Generator for BiGAN (adapted for multivariate time series)\n",
    "class BiGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, channels=14, seq_len=4500):\n",
    "        super(BiGANGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.channels = channels\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Start with a smaller sequence length and upsample\n",
    "        self.init_seq_len = seq_len // 16  # ~281\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512 * self.init_seq_len),\n",
    "            nn.BatchNorm1d(512 * self.init_seq_len),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # First deconv block\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Second deconv block\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third deconv block\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final deconv block\n",
    "            nn.ConvTranspose1d(64, channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc_layer(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_seq_len)\n",
    "        out = self.conv_blocks(out)\n",
    "        \n",
    "        # Ensure exact sequence length through interpolation if needed\n",
    "        if out.size(2) != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Joint Discriminator for BiGAN (takes both data and latent code)\n",
    "class BiGANDiscriminator(nn.Module):\n",
    "    def __init__(self, channels=14, seq_len=4500, latent_dim=100):\n",
    "        super(BiGANDiscriminator, self).__init__()\n",
    "        \n",
    "        # Data pathway - similar to your original discriminator\n",
    "        final_seq_len = seq_len // 16  # After 4 conv layers with stride 2\n",
    "        \n",
    "        self.data_path = nn.Sequential(\n",
    "            nn.Conv1d(channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.data_feature_size = 512 * final_seq_len\n",
    "        \n",
    "        # Latent pathway\n",
    "        self.latent_path = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Joint discriminator\n",
    "        self.joint_discriminator = nn.Sequential(\n",
    "            nn.Linear(self.data_feature_size + 512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        data_features = self.data_path(x)\n",
    "        latent_features = self.latent_path(z)\n",
    "        joint_features = torch.cat([data_features, latent_features], dim=1)\n",
    "        return self.joint_discriminator(joint_features)\n",
    "\n",
    "# BiGAN Model wrapper\n",
    "class BiGAN(nn.Module):\n",
    "    def __init__(self, latent_dim=100, channels=14, seq_len=4500):\n",
    "        super(BiGAN, self).__init__()\n",
    "        self.generator = BiGANGenerator(latent_dim, channels, seq_len)\n",
    "        self.encoder = Encoder(channels, seq_len, latent_dim)\n",
    "        self.discriminator = BiGANDiscriminator(channels, seq_len, latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def forward(self, x=None, z=None, mode='encode'):\n",
    "        if mode == 'encode':\n",
    "            return self.encoder(x)\n",
    "        elif mode == 'generate':\n",
    "            return self.generator(z)\n",
    "        elif mode == 'discriminate':\n",
    "            return self.discriminator(x, z)\n",
    "\n",
    "# Anomaly detection using reconstruction and encoding errors\n",
    "def compute_anomaly_scores(bigan, data_loader, device, data_scaler=None):\n",
    "    \"\"\"\n",
    "    Compute anomaly scores using both reconstruction and encoding errors\n",
    "    \"\"\"\n",
    "    bigan.eval()\n",
    "    anomaly_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, _ in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            \n",
    "            # Encode real data\n",
    "            encoded_z = bigan.encoder(batch_data)\n",
    "            \n",
    "            # Reconstruct from encoded latent\n",
    "            reconstructed_x = bigan.generator(encoded_z)\n",
    "            \n",
    "            # Reconstruction error\n",
    "            recon_error = torch.mean((batch_data - reconstructed_x) ** 2, dim=(1, 2))\n",
    "            \n",
    "            # Encoding consistency error\n",
    "            random_z = torch.randn_like(encoded_z)\n",
    "            generated_x = bigan.generator(random_z)\n",
    "            encoded_generated = bigan.encoder(generated_x)\n",
    "            encoding_error = torch.mean((random_z - encoded_generated) ** 2, dim=1)\n",
    "            \n",
    "            # Combined anomaly score (weighted combination)\n",
    "            combined_score = 0.7 * recon_error + 0.3 * encoding_error\n",
    "            anomaly_scores.extend(combined_score.cpu().numpy())\n",
    "    \n",
    "    return np.array(anomaly_scores)\n",
    "\n",
    "class FewShot1DDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Convert from (samples, seq_len, channels) to (samples, channels, seq_len) for Conv1d\n",
    "        self.data = torch.tensor(data.transpose(0, 2, 1), dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], 0  # Return dummy label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SPECIALIZED TRAINING FOR NORMAL DATA GENERATION\n",
    "# ========================================\n",
    "\n",
    "def train_bigan_for_normal_data_generation(normal_data, device, epochs=100, batch_size=16, \n",
    "                                         latent_dim=64, save_interval=20, verbose=True):\n",
    "    \"\"\"\n",
    "    Specialized training function for BiGAN focused on generating high-quality normal data\n",
    "    for anomaly detection applications.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Training BiGAN specifically for Normal Data Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Preprocessing with normalization\n",
    "    normal_data_normalized, data_scaler = preprocess_data(normal_data, fit_scaler=True)\n",
    "    \n",
    "    # Data loading with proper augmentation\n",
    "    dataset = FewShot1DDataset(normal_data_normalized)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Initialize BiGAN with optimized parameters for normal data\n",
    "    bigan = AnomalyAwareBiGAN(latent_dim=latent_dim, channels=14, seq_len=4500).to(device)\n",
    "    \n",
    "    # Optimizers with different learning rates for better stability\n",
    "    optimizer_G = optim.Adam(bigan.generator.parameters(), lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    optimizer_E = optim.Adam(bigan.encoder.parameters(), lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    optimizer_D = optim.Adam(bigan.discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999), weight_decay=1e-5)  # Slower D\n",
    "    optimizer_A = optim.Adam(bigan.anomaly_discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, mode='min', factor=0.8, patience=15, verbose=verbose)\n",
    "    scheduler_E = optim.lr_scheduler.ReduceLROnPlateau(optimizer_E, mode='min', factor=0.8, patience=15, verbose=verbose)\n",
    "    scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, mode='min', factor=0.8, patience=15, verbose=verbose)\n",
    "    \n",
    "    # Loss function with label smoothing\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    \n",
    "    # Training history\n",
    "    training_history = {\n",
    "        'g_losses': [], 'e_losses': [], 'd_losses': [], 'a_losses': [], 'recon_losses': []\n",
    "    }\n",
    "    \n",
    "    # Training loop optimized for normal data generation\n",
    "    print(f\"📊 Training on {len(dataset)} normal samples for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = {'g': 0, 'e': 0, 'd': 0, 'a': 0, 'recon': 0}\n",
    "        \n",
    "        for i, (real_samples, _) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            batch_size_current = real_samples.size(0)\n",
    "            \n",
    "            # Labels with smoothing for better stability\n",
    "            real_labels = torch.ones(batch_size_current, 1, device=device) * 0.9\n",
    "            fake_labels = torch.zeros(batch_size_current, 1, device=device) + 0.1\n",
    "            \n",
    "            # Generate random latent vectors\n",
    "            z = torch.randn(batch_size_current, latent_dim, device=device)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator (Less Frequently)\n",
    "            # ---------------------\n",
    "            if i % 2 == 0:  # Train discriminator every other iteration\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real data with encoded latent\n",
    "                encoded_z = bigan.encoder(real_samples)\n",
    "                real_validity = bigan.discriminator(real_samples, encoded_z)\n",
    "                d_real_loss = adversarial_loss(real_validity, real_labels)\n",
    "                \n",
    "                # Fake data with random latent\n",
    "                fake_samples = bigan.generator(z)\n",
    "                fake_validity = bigan.discriminator(fake_samples.detach(), z)\n",
    "                d_fake_loss = adversarial_loss(fake_validity, fake_labels)\n",
    "                \n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(bigan.discriminator.parameters(), max_norm=0.5)\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                epoch_losses['d'] += d_loss.item()\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Anomaly Discriminator\n",
    "            # ---------------------\n",
    "            if i % 3 == 0:  # Train less frequently\n",
    "                optimizer_A.zero_grad()\n",
    "                \n",
    "                encoded_z = bigan.encoder(real_samples)\n",
    "                anomaly_real = bigan.anomaly_discriminator(encoded_z.detach())\n",
    "                anomaly_fake = bigan.anomaly_discriminator(z)\n",
    "                \n",
    "                a_real_loss = adversarial_loss(anomaly_real, real_labels)\n",
    "                a_fake_loss = adversarial_loss(anomaly_fake, fake_labels)\n",
    "                a_loss = (a_real_loss + a_fake_loss) / 2\n",
    "                \n",
    "                a_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(bigan.anomaly_discriminator.parameters(), max_norm=0.5)\n",
    "                optimizer_A.step()\n",
    "                \n",
    "                epoch_losses['a'] += a_loss.item()\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator and Encoder (Focus on Quality)\n",
    "            # ---------------------\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_E.zero_grad()\n",
    "            \n",
    "            # Generate samples\n",
    "            fake_samples = bigan.generator(z)\n",
    "            encoded_z = bigan.encoder(real_samples)\n",
    "            \n",
    "            # Generator loss: fool discriminator\n",
    "            g_validity = bigan.discriminator(fake_samples, z)\n",
    "            g_loss = adversarial_loss(g_validity, real_labels)\n",
    "            \n",
    "            # Encoder loss: fool discriminator\n",
    "            e_validity = bigan.discriminator(real_samples, encoded_z)\n",
    "            e_loss = adversarial_loss(e_validity, fake_labels)\n",
    "            \n",
    "            # Reconstruction loss for quality (KEY FOR NORMAL DATA GENERATION)\n",
    "            reconstructed = bigan.generator(encoded_z)\n",
    "            recon_loss = reconstruction_loss(reconstructed, real_samples)\n",
    "            \n",
    "            # Anomaly-aware encoder loss\n",
    "            anomaly_encoder_loss = adversarial_loss(\n",
    "                bigan.anomaly_discriminator(encoded_z), fake_labels\n",
    "            )\n",
    "            \n",
    "            # Combined loss with emphasis on reconstruction quality\n",
    "            reconstruction_weight = 0.5  # Higher weight for better normal data quality\n",
    "            anomaly_weight = 0.1\n",
    "            \n",
    "            total_ge_loss = (g_loss + e_loss + \n",
    "                           reconstruction_weight * recon_loss +\n",
    "                           anomaly_weight * anomaly_encoder_loss)\n",
    "            \n",
    "            total_ge_loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(bigan.generator.parameters(), max_norm=0.5)\n",
    "            torch.nn.utils.clip_grad_norm_(bigan.encoder.parameters(), max_norm=0.5)\n",
    "            \n",
    "            optimizer_G.step()\n",
    "            optimizer_E.step()\n",
    "            \n",
    "            # Record losses\n",
    "            epoch_losses['g'] += g_loss.item()\n",
    "            epoch_losses['e'] += e_loss.item()\n",
    "            epoch_losses['recon'] += recon_loss.item()\n",
    "            \n",
    "            # Memory management\n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        num_batches = len(dataloader)\n",
    "        avg_losses = {k: v / max(1, num_batches // (2 if k == 'd' else 3 if k == 'a' else 1)) \n",
    "                     for k, v in epoch_losses.items()}\n",
    "        \n",
    "        # Store training history\n",
    "        training_history['g_losses'].append(avg_losses['g'])\n",
    "        training_history['e_losses'].append(avg_losses['e'])\n",
    "        training_history['d_losses'].append(avg_losses['d'])\n",
    "        training_history['a_losses'].append(avg_losses['a'])\n",
    "        training_history['recon_losses'].append(avg_losses['recon'])\n",
    "        \n",
    "        # Update learning rates\n",
    "        scheduler_G.step(avg_losses['g'])\n",
    "        scheduler_E.step(avg_losses['e'])\n",
    "        scheduler_D.step(avg_losses['d'])\n",
    "        \n",
    "        # Progress reporting\n",
    "        if verbose and (epoch + 1) % save_interval == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"G: {avg_losses['g']:.4f} | E: {avg_losses['e']:.4f} | \"\n",
    "                  f\"D: {avg_losses['d']:.4f} | A: {avg_losses['a']:.4f} | \"\n",
    "                  f\"Recon: {avg_losses['recon']:.4f}\")\n",
    "            \n",
    "            # Generate sample to check quality\n",
    "            with torch.no_grad():\n",
    "                test_z = torch.randn(4, latent_dim, device=device)\n",
    "                test_samples = bigan.generator(test_z)\n",
    "                sample_quality = torch.mean((test_samples - test_samples.mean()) ** 2).item()\n",
    "                print(f\"         Sample Variance: {sample_quality:.6f}\")\n",
    "        \n",
    "        # Early stopping based on reconstruction loss\n",
    "        if len(training_history['recon_losses']) > 20:\n",
    "            recent_recon = training_history['recon_losses'][-10:]\n",
    "            improvement = max(recent_recon) - min(recent_recon)\n",
    "            if improvement < 1e-6:\n",
    "                print(f\"🛑 Early stopping at epoch {epoch+1} - reconstruction loss converged\")\n",
    "                break\n",
    "    \n",
    "    print(\"✅ BiGAN training completed!\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(training_history, epochs)\n",
    "    \n",
    "    return bigan, data_scaler, training_history\n",
    "\n",
    "def plot_training_curves(history, epochs):\n",
    "    \"\"\"Plot training curves for BiGAN\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Generator loss\n",
    "    axes[0, 0].plot(history['g_losses'], label='Generator', color='blue')\n",
    "    axes[0, 0].set_title('Generator Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Encoder loss\n",
    "    axes[0, 1].plot(history['e_losses'], label='Encoder', color='green')\n",
    "    axes[0, 1].set_title('Encoder Loss')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Discriminator loss\n",
    "    axes[0, 2].plot(history['d_losses'], label='Discriminator', color='red')\n",
    "    axes[0, 2].set_title('Discriminator Loss')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Loss')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # Anomaly discriminator loss\n",
    "    axes[1, 0].plot(history['a_losses'], label='Anomaly Discriminator', color='purple')\n",
    "    axes[1, 0].set_title('Anomaly Discriminator Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Reconstruction loss (most important for normal data quality)\n",
    "    axes[1, 1].plot(history['recon_losses'], label='Reconstruction', color='orange', linewidth=2)\n",
    "    axes[1, 1].set_title('Reconstruction Loss (Key for Quality)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # Combined view\n",
    "    axes[1, 2].plot(history['g_losses'], label='Generator', alpha=0.7)\n",
    "    axes[1, 2].plot(history['e_losses'], label='Encoder', alpha=0.7)\n",
    "    axes[1, 2].plot(history['d_losses'], label='Discriminator', alpha=0.7)\n",
    "    axes[1, 2].plot(history['recon_losses'], label='Reconstruction', linewidth=2)\n",
    "    axes[1, 2].set_title('All Losses Combined')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Loss')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"🔧 Specialized BiGAN training functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# Anomaly Aware GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# OPTIMIZED BIGAN TRAINING FOR NORMAL DATA GENERATION\n",
    "# ========================================\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Enhanced preprocessing with robust normalization\n",
    "normal_data_normalized, data_scaler = preprocess_data(X_train_normal, fit_scaler=True)\n",
    "\n",
    "# Optimized training parameters for better normal data generation\n",
    "latent_dim = 64\n",
    "epochs = 200  # More epochs for better convergence\n",
    "batch_size = 32  # Increased batch size for stability\n",
    "save_interval = 20\n",
    "\n",
    "print(\"🚀 OPTIMIZED BIGAN TRAINING FOR ANOMALY DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📊 Training Configuration:\")\n",
    "print(f\"   Data shape: {normal_data_normalized.shape}\")\n",
    "print(f\"   Latent dimension: {latent_dim}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Epochs: {epochs}\")\n",
    "\n",
    "# Enhanced data loading with data augmentation\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, data, augment_prob=0.3):\n",
    "        # Convert from (samples, seq_len, channels) to (samples, channels, seq_len)\n",
    "        self.data = torch.tensor(data.transpose(0, 2, 1), dtype=torch.float32)\n",
    "        self.augment_prob = augment_prob\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # Simple data augmentation for time series\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            # Add small amount of noise\n",
    "            noise = torch.randn_like(sample) * 0.01\n",
    "            sample = sample + noise\n",
    "            \n",
    "            # Random scaling\n",
    "            scale = torch.rand(sample.size(0), 1) * 0.1 + 0.95  # Range [0.95, 1.05]\n",
    "            sample = sample * scale\n",
    "        \n",
    "        return sample, 0\n",
    "\n",
    "# Create enhanced dataset\n",
    "dataset = AugmentedDataset(normal_data_normalized, augment_prob=0.2)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Initialize BiGAN with improved architecture\n",
    "bigan = AnomalyAwareBiGAN(latent_dim=latent_dim, channels=14, seq_len=4500).to(device)\n",
    "\n",
    "# Check model size\n",
    "total_params = sum(p.numel() for p in bigan.parameters())\n",
    "print(f\"🔧 Model parameters: {total_params:,}\")\n",
    "\n",
    "# Optimized optimizers with different learning rates and schedules\n",
    "optimizer_G = optim.Adam(bigan.generator.parameters(), lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "optimizer_E = optim.Adam(bigan.encoder.parameters(), lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "optimizer_D = optim.Adam(bigan.discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999), weight_decay=1e-5)  # Slower discriminator\n",
    "optimizer_A = optim.Adam(bigan.anomaly_discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "\n",
    "# Learning rate schedulers for adaptive training\n",
    "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=epochs, eta_min=1e-6)\n",
    "scheduler_E = optim.lr_scheduler.CosineAnnealingLR(optimizer_E, T_max=epochs, eta_min=1e-6)\n",
    "scheduler_D = optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "# Enhanced loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "reconstruction_loss = nn.MSELoss()\n",
    "perceptual_loss = nn.L1Loss()\n",
    "\n",
    "# Training tracking\n",
    "training_history = {\n",
    "    'g_losses': [], 'e_losses': [], 'd_losses': [], 'a_losses': [], \n",
    "    'recon_losses': [], 'perceptual_losses': []\n",
    "}\n",
    "\n",
    "# Advanced training loop with focus on quality\n",
    "print(\"📈 Starting optimized training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = {'g': 0, 'e': 0, 'd': 0, 'a': 0, 'recon': 0, 'perceptual': 0}\n",
    "    \n",
    "    for i, (real_samples, _) in enumerate(dataloader):\n",
    "        real_samples = real_samples.to(device)\n",
    "        batch_size_current = real_samples.size(0)\n",
    "        \n",
    "        # Enhanced label smoothing\n",
    "        real_labels = torch.ones(batch_size_current, 1, device=device) * 0.9\n",
    "        fake_labels = torch.zeros(batch_size_current, 1, device=device) + 0.1\n",
    "        \n",
    "        # Generate random latent vectors\n",
    "        z = torch.randn(batch_size_current, latent_dim, device=device)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator (Less Frequently for Better Balance)\n",
    "        # ---------------------\n",
    "        if i % 2 == 0:  # Train discriminator every other iteration\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real data with encoded latent\n",
    "            with torch.no_grad():\n",
    "                encoded_z = bigan.encoder(real_samples)\n",
    "            real_validity = bigan.discriminator(real_samples, encoded_z)\n",
    "            d_real_loss = adversarial_loss(real_validity, real_labels)\n",
    "            \n",
    "            # Fake data with random latent\n",
    "            with torch.no_grad():\n",
    "                fake_samples = bigan.generator(z)\n",
    "            fake_validity = bigan.discriminator(fake_samples, z)\n",
    "            d_fake_loss = adversarial_loss(fake_validity, fake_labels)\n",
    "            \n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(bigan.discriminator.parameters(), max_norm=0.5)\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            epoch_losses['d'] += d_loss.item()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Anomaly Discriminator (Even Less Frequently)\n",
    "        # ---------------------\n",
    "        if i % 3 == 0:\n",
    "            optimizer_A.zero_grad()\n",
    "            \n",
    "            encoded_z = bigan.encoder(real_samples)\n",
    "            anomaly_real = bigan.anomaly_discriminator(encoded_z.detach())\n",
    "            anomaly_fake = bigan.anomaly_discriminator(z)\n",
    "            \n",
    "            a_real_loss = adversarial_loss(anomaly_real, real_labels)\n",
    "            a_fake_loss = adversarial_loss(anomaly_fake, fake_labels)\n",
    "            a_loss = (a_real_loss + a_fake_loss) / 2\n",
    "            \n",
    "            a_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(bigan.anomaly_discriminator.parameters(), max_norm=0.5)\n",
    "            optimizer_A.step()\n",
    "            \n",
    "            epoch_losses['a'] += a_loss.item()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator and Encoder (Focus on Quality)\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_E.zero_grad()\n",
    "        \n",
    "        # Generate samples and encode real data\n",
    "        fake_samples = bigan.generator(z)\n",
    "        encoded_z = bigan.encoder(real_samples)\n",
    "        \n",
    "        # Adversarial losses\n",
    "        g_validity = bigan.discriminator(fake_samples, z)\n",
    "        g_loss = adversarial_loss(g_validity, real_labels)\n",
    "        \n",
    "        e_validity = bigan.discriminator(real_samples, encoded_z)\n",
    "        e_loss = adversarial_loss(e_validity, fake_labels)\n",
    "        \n",
    "        # CRITICAL: Enhanced reconstruction loss for normal data quality\n",
    "        reconstructed = bigan.generator(encoded_z)\n",
    "        recon_loss = reconstruction_loss(reconstructed, real_samples)\n",
    "        \n",
    "        # Perceptual loss (feature-level similarity)\n",
    "        perceptual_recon_loss = perceptual_loss(reconstructed, real_samples)\n",
    "        \n",
    "        # Anomaly-aware loss\n",
    "        anomaly_encoder_loss = adversarial_loss(\n",
    "            bigan.anomaly_discriminator(encoded_z), fake_labels\n",
    "        )\n",
    "        \n",
    "        # Enhanced loss combination with higher emphasis on reconstruction quality\n",
    "        reconstruction_weight = 1.0  # Increased weight for better normal data quality\n",
    "        perceptual_weight = 0.5      # Additional perceptual similarity\n",
    "        anomaly_weight = 0.1\n",
    "        \n",
    "        total_ge_loss = (g_loss + e_loss + \n",
    "                        reconstruction_weight * recon_loss +\n",
    "                        perceptual_weight * perceptual_recon_loss +\n",
    "                        anomaly_weight * anomaly_encoder_loss)\n",
    "        \n",
    "        total_ge_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(bigan.generator.parameters(), max_norm=0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(bigan.encoder.parameters(), max_norm=0.5)\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        optimizer_E.step()\n",
    "        \n",
    "        # Record losses\n",
    "        epoch_losses['g'] += g_loss.item()\n",
    "        epoch_losses['e'] += e_loss.item()\n",
    "        epoch_losses['recon'] += recon_loss.item()\n",
    "        epoch_losses['perceptual'] += perceptual_recon_loss.item()\n",
    "        \n",
    "        # Memory management\n",
    "        if i % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Update learning rates\n",
    "    scheduler_G.step()\n",
    "    scheduler_E.step()\n",
    "    scheduler_D.step()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    num_batches = len(dataloader)\n",
    "    avg_losses = {\n",
    "        'g': epoch_losses['g'] / num_batches,\n",
    "        'e': epoch_losses['e'] / num_batches,\n",
    "        'd': epoch_losses['d'] / max(1, num_batches // 2),\n",
    "        'a': epoch_losses['a'] / max(1, num_batches // 3),\n",
    "        'recon': epoch_losses['recon'] / num_batches,\n",
    "        'perceptual': epoch_losses['perceptual'] / num_batches\n",
    "    }\n",
    "    \n",
    "    # Store training history\n",
    "    training_history['g_losses'].append(avg_losses['g'])\n",
    "    training_history['e_losses'].append(avg_losses['e'])\n",
    "    training_history['d_losses'].append(avg_losses['d'])\n",
    "    training_history['a_losses'].append(avg_losses['a'])\n",
    "    training_history['recon_losses'].append(avg_losses['recon'])\n",
    "    training_history['perceptual_losses'].append(avg_losses['perceptual'])\n",
    "    \n",
    "    # Enhanced progress reporting\n",
    "    if (epoch + 1) % save_interval == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "              f\"G: {avg_losses['g']:.4f} | E: {avg_losses['e']:.4f} | \"\n",
    "              f\"D: {avg_losses['d']:.4f} | A: {avg_losses['a']:.4f} | \"\n",
    "              f\"Recon: {avg_losses['recon']:.4f} | Perc: {avg_losses['perceptual']:.4f}\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        with torch.no_grad():\n",
    "            test_z = torch.randn(4, latent_dim, device=device)\n",
    "            test_samples = bigan.generator(test_z)\n",
    "            sample_std = torch.std(test_samples).item()\n",
    "            sample_mean = torch.mean(test_samples).item()\n",
    "            print(f\"         Sample stats: μ={sample_mean:.4f}, σ={sample_std:.4f}\")\n",
    "    \n",
    "    # Early stopping based on reconstruction quality\n",
    "    if len(training_history['recon_losses']) > 30:\n",
    "        recent_recon = training_history['recon_losses'][-15:]\n",
    "        if max(recent_recon) - min(recent_recon) < 1e-6:\n",
    "            print(f\"🛑 Early stopping at epoch {epoch+1} - reconstruction converged\")\n",
    "            break\n",
    "\n",
    "print(\"✅ Optimized BiGAN training completed!\")\n",
    "\n",
    "# Enhanced visualization of training progress\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Individual loss plots\n",
    "losses_to_plot = [\n",
    "    ('g_losses', 'Generator Loss', 'blue'),\n",
    "    ('e_losses', 'Encoder Loss', 'green'),\n",
    "    ('d_losses', 'Discriminator Loss', 'red'),\n",
    "    ('a_losses', 'Anomaly Discriminator Loss', 'purple'),\n",
    "    ('recon_losses', 'Reconstruction Loss (Critical)', 'orange'),\n",
    "    ('perceptual_losses', 'Perceptual Loss', 'brown')\n",
    "]\n",
    "\n",
    "for idx, (loss_key, title, color) in enumerate(losses_to_plot):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    axes[row, col].plot(training_history[loss_key], color=color, linewidth=2)\n",
    "    axes[row, col].set_title(title)\n",
    "    axes[row, col].set_xlabel('Epoch')\n",
    "    axes[row, col].set_ylabel('Loss')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate high-quality samples for anomaly detection\n",
    "print(\"🎨 Generating optimized normal data samples...\")\n",
    "\n",
    "def generate_enhanced_bigan_samples(bigan, num_samples, data_scaler, latent_dim=64, quality_filter=True):\n",
    "    \"\"\"\n",
    "    Generate high-quality samples with optional quality filtering\n",
    "    \"\"\"\n",
    "    device = next(bigan.parameters()).device\n",
    "    bigan.eval()\n",
    "    \n",
    "    # Generate more samples than needed for quality filtering\n",
    "    samples_to_generate = int(num_samples * 1.3) if quality_filter else num_samples\n",
    "    batch_size = 16\n",
    "    all_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, samples_to_generate, batch_size):\n",
    "            end = min(start + batch_size, samples_to_generate)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            # Use controlled latent sampling for better quality\n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device) * 0.8  # Reduce variance\n",
    "            batch_samples = bigan.generator(z)\n",
    "            all_samples.append(batch_samples.cpu())\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    generated_tensor = torch.cat(all_samples, dim=0)\n",
    "    generated_data = generated_tensor.numpy().transpose(0, 2, 1)  # Convert to (n_samples, 4500, 14)\n",
    "    \n",
    "    # Quality filtering based on statistical properties\n",
    "    if quality_filter and len(generated_data) > num_samples:\n",
    "        print(f\"   🔍 Applying quality filter (from {len(generated_data)} to {num_samples})...\")\n",
    "        \n",
    "        # Calculate quality scores based on similarity to real normal data\n",
    "        quality_scores = []\n",
    "        for sample in generated_data:\n",
    "            # Simple quality metric: distance from normal data statistics\n",
    "            sample_mean = np.mean(sample)\n",
    "            sample_std = np.std(sample)\n",
    "            real_mean = np.mean(normal_data_normalized)\n",
    "            real_std = np.std(normal_data_normalized)\n",
    "            \n",
    "            mean_diff = abs(sample_mean - real_mean) / (real_std + 1e-8)\n",
    "            std_diff = abs(sample_std - real_std) / (real_std + 1e-8)\n",
    "            quality_score = 1 / (1 + mean_diff + std_diff)\n",
    "            quality_scores.append(quality_score)\n",
    "        \n",
    "        # Select top quality samples\n",
    "        quality_indices = np.argsort(quality_scores)[-num_samples:]\n",
    "        generated_data = generated_data[quality_indices]\n",
    "        print(f\"   ✅ Selected {len(generated_data)} highest quality samples\")\n",
    "    \n",
    "    # Denormalize to original scale\n",
    "    generated_data_denorm = denormalize_data(generated_data, data_scaler)\n",
    "    \n",
    "    return generated_data_denorm\n",
    "\n",
    "# Generate enhanced samples\n",
    "num_samples = len(X_train_normal)\n",
    "generated_data = generate_enhanced_bigan_samples(\n",
    "    bigan, \n",
    "    num_samples=num_samples, \n",
    "    data_scaler=data_scaler, \n",
    "    latent_dim=latent_dim,\n",
    "    quality_filter=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Generated {len(generated_data)} high-quality normal samples\")\n",
    "print(f\"📊 Generated data statistics:\")\n",
    "print(f\"   Shape: {generated_data.shape}\")\n",
    "print(f\"   Mean: {generated_data.mean():.6f}\")\n",
    "print(f\"   Std: {generated_data.std():.6f}\")\n",
    "print(f\"   Range: [{generated_data.min():.6f}, {generated_data.max():.6f}]\")\n",
    "\n",
    "# Store the optimized generated data\n",
    "print(\"💾 Storing optimized generated data for enhanced anomaly detection...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DATA QUALITY ASSESSMENT FOR GENERATED NORMAL SAMPLES\n",
    "# ========================================\n",
    "\n",
    "def assess_generated_data_quality(real_data, generated_data, title=\"Data Quality Assessment\"):\n",
    "    \"\"\"\n",
    "    Comprehensive assessment of generated data quality for anomaly detection\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic statistics comparison\n",
    "    print(\"📊 Statistical Comparison:\")\n",
    "    print(f\"   Real Data Shape: {real_data.shape}\")\n",
    "    print(f\"   Generated Data Shape: {generated_data.shape}\")\n",
    "    print(f\"   Real Mean: {real_data.mean():.6f}, Std: {real_data.std():.6f}\")\n",
    "    print(f\"   Generated Mean: {generated_data.mean():.6f}, Std: {generated_data.std():.6f}\")\n",
    "    print(f\"   Mean Difference: {abs(real_data.mean() - generated_data.mean()):.6f}\")\n",
    "    print(f\"   Std Difference: {abs(real_data.std() - generated_data.std()):.6f}\")\n",
    "    \n",
    "    # Distribution comparison using statistical tests\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Flatten data for statistical tests\n",
    "    real_flat = real_data.reshape(-1)\n",
    "    gen_flat = generated_data.reshape(-1)\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_p = stats.ks_2samp(real_flat, gen_flat)\n",
    "    print(f\"\\n🧪 Statistical Tests:\")\n",
    "    print(f\"   KS Test: statistic={ks_stat:.4f}, p-value={ks_p:.6f}\")\n",
    "    print(f\"   KS Interpretation: {'✅ Similar distributions' if ks_p > 0.05 else '⚠️ Different distributions'}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Overall distribution comparison\n",
    "    axes[0, 0].hist(real_flat, bins=100, alpha=0.7, label='Real Data', color='blue', density=True)\n",
    "    axes[0, 0].hist(gen_flat, bins=100, alpha=0.7, label='Generated Data', color='red', density=True)\n",
    "    axes[0, 0].set_title('Overall Value Distribution')\n",
    "    axes[0, 0].set_xlabel('Value')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Channel-wise mean comparison\n",
    "    real_channel_means = real_data.mean(axis=1).mean(axis=0)  # Mean across time and samples\n",
    "    gen_channel_means = generated_data.mean(axis=1).mean(axis=0)\n",
    "    \n",
    "    x_channels = range(len(real_channel_means))\n",
    "    axes[0, 1].plot(x_channels, real_channel_means, 'o-', label='Real Data', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(x_channels, gen_channel_means, 's-', label='Generated Data', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Channel-wise Mean Values')\n",
    "    axes[0, 1].set_xlabel('Channel')\n",
    "    axes[0, 1].set_ylabel('Mean Value')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Channel-wise standard deviation comparison\n",
    "    real_channel_stds = real_data.std(axis=1).mean(axis=0)\n",
    "    gen_channel_stds = generated_data.std(axis=1).mean(axis=0)\n",
    "    \n",
    "    axes[0, 2].plot(x_channels, real_channel_stds, 'o-', label='Real Data', color='blue', linewidth=2)\n",
    "    axes[0, 2].plot(x_channels, gen_channel_stds, 's-', label='Generated Data', color='red', linewidth=2)\n",
    "    axes[0, 2].set_title('Channel-wise Standard Deviation')\n",
    "    axes[0, 2].set_xlabel('Channel')\n",
    "    axes[0, 2].set_ylabel('Standard Deviation')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sample time series comparison (first 3 channels)\n",
    "    sample_idx = np.random.randint(0, min(real_data.shape[0], generated_data.shape[0]))\n",
    "    time_steps = range(min(500, real_data.shape[1]))  # Show first 500 time steps\n",
    "    \n",
    "    for ch in range(min(3, real_data.shape[2])):\n",
    "        axes[1, ch].plot(time_steps, real_data[sample_idx, :len(time_steps), ch], \n",
    "                        label='Real', color='blue', alpha=0.8)\n",
    "        axes[1, ch].plot(time_steps, generated_data[sample_idx, :len(time_steps), ch], \n",
    "                        label='Generated', color='red', alpha=0.8)\n",
    "        axes[1, ch].set_title(f'Channel {ch+1} Time Series Comparison')\n",
    "        axes[1, ch].set_xlabel('Time Step')\n",
    "        axes[1, ch].set_ylabel('Value')\n",
    "        axes[1, ch].legend()\n",
    "        axes[1, ch].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quality score calculation\n",
    "    mean_diff_norm = abs(real_data.mean() - generated_data.mean()) / (real_data.std() + 1e-8)\n",
    "    std_diff_norm = abs(real_data.std() - generated_data.std()) / (real_data.std() + 1e-8)\n",
    "    \n",
    "    quality_score = max(0, 1 - (mean_diff_norm + std_diff_norm) / 2)\n",
    "    \n",
    "    print(f\"\\n🎯 Data Quality Score: {quality_score:.4f} (0=Poor, 1=Perfect)\")\n",
    "    if quality_score > 0.8:\n",
    "        print(\"   ✅ Excellent quality - Generated data very similar to real data\")\n",
    "    elif quality_score > 0.6:\n",
    "        print(\"   🔄 Good quality - Generated data reasonably similar to real data\")\n",
    "    elif quality_score > 0.4:\n",
    "        print(\"   ⚠️ Moderate quality - Some differences in data distributions\")\n",
    "    else:\n",
    "        print(\"   ❌ Poor quality - Significant differences in data distributions\")\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "# Assess the quality of your generated data\n",
    "print(\"🔍 Assessing BiGAN Generated Normal Data Quality...\")\n",
    "quality_score = assess_generated_data_quality(\n",
    "    real_data=normal_data[:len(generated_data)],  # Use same number of samples for comparison\n",
    "    generated_data=generated_data,\n",
    "    title=\"BiGAN Generated Normal Data Assessment\"\n",
    ")\n",
    "\n",
    "# Store quality metrics\n",
    "quality_metrics = {\n",
    "    'overall_quality_score': quality_score,\n",
    "    'real_data_stats': {\n",
    "        'mean': normal_data.mean(),\n",
    "        'std': normal_data.std(),\n",
    "        'shape': normal_data.shape\n",
    "    },\n",
    "    'generated_data_stats': {\n",
    "        'mean': generated_data.mean(),\n",
    "        'std': generated_data.std(),\n",
    "        'shape': generated_data.shape\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n💾 Quality metrics stored for further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = generated_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"❌ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fd442",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, generated_data, epochs=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
