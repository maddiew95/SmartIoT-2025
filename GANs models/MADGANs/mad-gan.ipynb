{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca19d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:31:36.481603Z",
     "iopub.status.busy": "2025-07-13T08:31:36.480750Z",
     "iopub.status.idle": "2025-07-13T08:31:42.889235Z",
     "shell.execute_reply": "2025-07-13T08:31:42.888114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA A30\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np, os\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2d7663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:31:42.895131Z",
     "iopub.status.busy": "2025-07-13T08:31:42.894570Z",
     "iopub.status.idle": "2025-07-13T08:33:04.878511Z",
     "shell.execute_reply": "2025-07-13T08:33:04.877327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Memory-Efficient MADGAN...\n",
      "Original data shape: (552, 4500, 14)\n",
      "Flattened data shape: (552, 63000)\n",
      "Input dimension: 63000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MADGAN on 552 normal samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] D_Loss: 0.0901, G_Loss: 18.8981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] D_Loss: 0.1854, G_Loss: 16.5946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/200] D_Loss: 0.1163, G_Loss: 17.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/200] D_Loss: 0.2239, G_Loss: 15.3821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/200] D_Loss: 0.2929, G_Loss: 14.5839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/200] D_Loss: 0.2058, G_Loss: 13.9395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/200] D_Loss: 0.2176, G_Loss: 12.5746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/200] D_Loss: 0.1949, G_Loss: 11.8417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/200] D_Loss: 0.2354, G_Loss: 12.0785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/200] D_Loss: 0.1910, G_Loss: 11.0734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic normal data...\n",
      "Generated data shape: (552, 4500, 14)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# MADGAN ARCHITECTURE FOR MULTIVARIATE TIME SERIES ANOMALY DETECTION\n",
    "# ===============================\n",
    "\n",
    "class MemoryEfficientMADGAN(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64, sequence_length=None):\n",
    "        super(MemoryEfficientMADGAN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length or input_dim\n",
    "        \n",
    "        # Memory-efficient Generator with residual connections\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Lightweight Discriminator\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Encoder for anomaly detection\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Memory-efficient training function\n",
    "def train_madgan_memory_efficient(model, normal_data, epochs=100, batch_size=32, lr=0.0002):\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizers with gradient clipping for stability\n",
    "    optimizer_G = optim.Adam(model.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(model.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_E = optim.Adam(model.encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    # Create DataLoader for memory efficiency\n",
    "    normal_tensor = torch.FloatTensor(normal_data)\n",
    "    dataset = TensorDataset(normal_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_d_loss = 0\n",
    "        epoch_g_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_data, in dataloader:\n",
    "            batch_data = batch_data.to(device, non_blocking=True)\n",
    "            batch_size_actual = batch_data.size(0)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if num_batches % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            real_labels = torch.ones(batch_size_actual, 1).to(device)\n",
    "            real_output = model.discriminator(batch_data)\n",
    "            d_loss_real = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Fake data\n",
    "            z = torch.randn(batch_size_actual, model.latent_dim).to(device)\n",
    "            fake_data = model.generator(z).detach()\n",
    "            fake_labels = torch.zeros(batch_size_actual, 1).to(device)\n",
    "            fake_output = model.discriminator(fake_data)\n",
    "            d_loss_fake = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.discriminator.parameters(), 1.0)\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train Generator and Encoder\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_E.zero_grad()\n",
    "            \n",
    "            # Generator loss\n",
    "            z = torch.randn(batch_size_actual, model.latent_dim).to(device)\n",
    "            fake_data = model.generator(z)\n",
    "            fake_output = model.discriminator(fake_data)\n",
    "            g_loss_adv = criterion(fake_output, real_labels)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            encoded = model.encoder(batch_data)\n",
    "            reconstructed = model.generator(encoded)\n",
    "            reconstruction_loss = mse_loss(reconstructed, batch_data)\n",
    "            \n",
    "            # Feature matching loss\n",
    "            with torch.no_grad():\n",
    "                real_features = model.discriminator(batch_data)\n",
    "            fake_features = model.discriminator(fake_data)\n",
    "            feature_loss = mse_loss(fake_features, real_features)\n",
    "            \n",
    "            # Combined loss\n",
    "            g_loss = g_loss_adv + 10 * reconstruction_loss + 5 * feature_loss\n",
    "            g_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.generator.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.encoder.parameters(), 1.0)\n",
    "            \n",
    "            optimizer_G.step()\n",
    "            optimizer_E.step()\n",
    "            \n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}] D_Loss: {epoch_d_loss/num_batches:.4f}, G_Loss: {epoch_g_loss/num_batches:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Anomaly detection function\n",
    "def detect_anomalies_madgan(model, test_data, threshold_percentile=95):\n",
    "    model.eval()\n",
    "    anomaly_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.FloatTensor(test_data).to(device)\n",
    "        \n",
    "        # Reconstruction-based anomaly score\n",
    "        encoded = model.encoder(test_tensor)\n",
    "        reconstructed = model.generator(encoded)\n",
    "        reconstruction_errors = torch.mean((test_tensor - reconstructed) ** 2, dim=1)\n",
    "        \n",
    "        # Discriminator-based anomaly score\n",
    "        discriminator_scores = model.discriminator(test_tensor).squeeze()\n",
    "        \n",
    "        # Combined anomaly score\n",
    "        combined_scores = reconstruction_errors + (1 - discriminator_scores)\n",
    "        anomaly_scores = combined_scores.cpu().numpy()\n",
    "    \n",
    "    # Dynamic threshold\n",
    "    threshold = np.percentile(anomaly_scores, threshold_percentile)\n",
    "    predictions = (anomaly_scores > threshold).astype(int)\n",
    "    \n",
    "    return predictions, anomaly_scores, threshold\n",
    "\n",
    "# Initialize and train the model\n",
    "print(\"Initializing Memory-Efficient MADGAN...\")\n",
    "\n",
    "# Flatten the data to 2D if it's 3D\n",
    "if len(X_train_normal.shape) == 3:\n",
    "    print(f\"Original data shape: {X_train_normal.shape}\")\n",
    "    X_train_flattened = X_train_normal.reshape(X_train_normal.shape[0], -1)\n",
    "    print(f\"Flattened data shape: {X_train_flattened.shape}\")\n",
    "    input_dim = X_train_flattened.shape[1]\n",
    "else:\n",
    "    X_train_flattened = X_train_normal\n",
    "    input_dim = X_train_normal.shape[1]\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "madgan_model = MemoryEfficientMADGAN(input_dim=input_dim, latent_dim=64)\n",
    "\n",
    "print(f\"Training MADGAN on {X_train_flattened.shape[0]} normal samples...\")\n",
    "trained_madgan = train_madgan_memory_efficient(\n",
    "    madgan_model, \n",
    "    X_train_flattened, \n",
    "    epochs=200, \n",
    "    batch_size=32,\n",
    "    lr=0.0002\n",
    ")\n",
    "\n",
    "# Generate synthetic data for downstream tasks\n",
    "print(\"Generating synthetic normal data...\")\n",
    "trained_madgan.eval()\n",
    "with torch.no_grad():\n",
    "    num_samples = len(X_train_normal)  # Memory-efficient generation\n",
    "    z = torch.randn(num_samples, trained_madgan.latent_dim).to(device)\n",
    "    memory_generated_data = trained_madgan.generator(z).cpu().numpy()\n",
    "\n",
    "\n",
    "# Reshape generated data to match original input shape (n, 4500, 14)\n",
    "memory_generated_data = memory_generated_data.reshape(-1, X_train_normal.shape[1], X_train_normal.shape[2])\n",
    "\n",
    "print(f\"Generated data shape: {memory_generated_data.shape}\")\n",
    "torch.cuda.empty_cache()  # Clear GPU memory\n",
    "\n",
    "normal_combine = np.concatenate((X_train_normal, memory_generated_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afc52fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:33:04.883126Z",
     "iopub.status.busy": "2025-07-13T08:33:04.882846Z",
     "iopub.status.idle": "2025-07-13T08:33:04.888076Z",
     "shell.execute_reply": "2025-07-13T08:33:04.887226Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# # Test the simplified FID calculation\n",
    "# print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# # Use smaller subsets for testing\n",
    "# test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "# test_generated = memory_generated_data[:100]\n",
    "\n",
    "# print(f\"Test real data shape: {test_real.shape}\")\n",
    "# print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# # Calculate FID score\n",
    "# fid_score = calculate_fid_score(\n",
    "#     real_data=test_real,\n",
    "#     fake_data=test_generated,\n",
    "#     device=device,\n",
    "#     sample_rate=1000,\n",
    "# )\n",
    "\n",
    "# if fid_score is not None:\n",
    "#     print(f\"\\nðŸŽ‰ SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "#     # Interpret the score\n",
    "#     if fid_score < 10:\n",
    "#         quality = \"Excellent\"\n",
    "#     elif fid_score < 25:\n",
    "#         quality = \"Good\"\n",
    "#     elif fid_score < 50:\n",
    "#         quality = \"Fair\"\n",
    "#     elif fid_score < 100:\n",
    "#         quality = \"Poor\"\n",
    "#     else:\n",
    "#         quality = \"Very Poor\"\n",
    "    \n",
    "#     print(f\"Quality Assessment: {quality}\")\n",
    "# else:\n",
    "#     print(\"âŒ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65e1d1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T08:33:04.891541Z",
     "iopub.status.busy": "2025-07-13T08:33:04.891221Z",
     "iopub.status.idle": "2025-07-13T09:09:29.139599Z",
     "shell.execute_reply": "2025-07-13T09:09:29.138484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maddie/SmartIoT-2025/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/maddie/SmartIoT-2025/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-fold cross-validation...\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.245584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.076128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.036002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.034002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.027568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8857, Prec=0.6364, Rec=1.0000, F1=0.7778\n",
      "Optimal threshold: 0.028576\n",
      "\n",
      "Fold 2/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.245993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.084712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.036084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.034869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.034125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.6000, Prec=0.3158, Rec=0.8571, F1=0.4615\n",
      "Optimal threshold: 0.033598\n",
      "\n",
      "Fold 3/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.245509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.076865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.035879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.034851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.034337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9429, Prec=1.0000, Rec=0.7143, F1=0.8333\n",
      "Optimal threshold: 0.037407\n",
      "\n",
      "Fold 4/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.245572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.095047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.036311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.034939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.034654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9143, Prec=0.8571, Rec=0.7500, F1=0.8000\n",
      "Optimal threshold: 0.037901\n",
      "\n",
      "Fold 5/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.245675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.087026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.036410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.034956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.034098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8571, Prec=1.0000, Rec=0.3750, F1=0.5455\n",
      "Optimal threshold: 0.037061\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "FOLD-BY-FOLD RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Fold   Accuracy   Precision   Recall   F1-Score  Threshold   \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.8857     0.6364      1.0000   0.7778    0.028576    \n",
      "2      0.6000     0.3158      0.8571   0.4615    0.033598    \n",
      "3      0.9429     1.0000      0.7143   0.8333    0.037407    \n",
      "4      0.9143     0.8571      0.7500   0.8000    0.037901    \n",
      "5      0.8571     1.0000      0.3750   0.5455    0.037061    \n",
      "\n",
      "STATISTICAL SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "Metric       Mean     Std      Min      Max      Median  \n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy     0.8400   0.1234   0.6000   0.9429   0.8857  \n",
      "Precision    0.7619   0.2597   0.3158   1.0000   0.8571  \n",
      "Recall       0.7393   0.2075   0.3750   1.0000   0.7500  \n",
      "F1           0.6836   0.1505   0.4615   0.8333   0.7778  \n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "  Mean F1-Score: 0.6836 Â± 0.1505\n",
      "  F1-Score Range: [0.4615, 0.8333]\n",
      "  Mean Threshold: 0.034909 Â± 0.003511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fold_results': [{'fold': 1,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.02756833080202341,\n",
       "   'optimal_threshold': 0.028576423724492393,\n",
       "   'accuracy': 0.8857142857142857,\n",
       "   'precision': 0.6363636363636364,\n",
       "   'recall': 1.0,\n",
       "   'f1': 0.7777777777777778},\n",
       "  {'fold': 2,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.03412547204643488,\n",
       "   'optimal_threshold': 0.0335975190003713,\n",
       "   'accuracy': 0.6,\n",
       "   'precision': 0.3157894736842105,\n",
       "   'recall': 0.8571428571428571,\n",
       "   'f1': 0.46153846153846156},\n",
       "  {'fold': 3,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.03433691449463368,\n",
       "   'optimal_threshold': 0.037407499427596726,\n",
       "   'accuracy': 0.9428571428571428,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.7142857142857143,\n",
       "   'f1': 0.8333333333333333},\n",
       "  {'fold': 4,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.03465422373265028,\n",
       "   'optimal_threshold': 0.037900945002382454,\n",
       "   'accuracy': 0.9142857142857143,\n",
       "   'precision': 0.8571428571428571,\n",
       "   'recall': 0.75,\n",
       "   'f1': 0.7999999999999999},\n",
       "  {'fold': 5,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.034097889252007005,\n",
       "   'optimal_threshold': 0.037060942688975675,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.375,\n",
       "   'f1': 0.5454545454545454}],\n",
       " 'statistics': {'accuracy': {'mean': 0.8400000000000001,\n",
       "   'std': 0.12335447511384516,\n",
       "   'min': 0.6,\n",
       "   'max': 0.9428571428571428,\n",
       "   'median': 0.8857142857142857,\n",
       "   'values': array([0.88571429, 0.6       , 0.94285714, 0.91428571, 0.85714286])},\n",
       "  'precision': {'mean': 0.7618591934381408,\n",
       "   'std': 0.25970305881743067,\n",
       "   'min': 0.3157894736842105,\n",
       "   'max': 1.0,\n",
       "   'median': 0.8571428571428571,\n",
       "   'values': array([0.63636364, 0.31578947, 1.        , 0.85714286, 1.        ])},\n",
       "  'recall': {'mean': 0.7392857142857143,\n",
       "   'std': 0.20751198637619928,\n",
       "   'min': 0.375,\n",
       "   'max': 1.0,\n",
       "   'median': 0.75,\n",
       "   'values': array([1.        , 0.85714286, 0.71428571, 0.75      , 0.375     ])},\n",
       "  'f1': {'mean': 0.6836208236208235,\n",
       "   'std': 0.15048852198465584,\n",
       "   'min': 0.46153846153846156,\n",
       "   'max': 0.8333333333333333,\n",
       "   'median': 0.7777777777777778,\n",
       "   'values': array([0.77777778, 0.46153846, 0.83333333, 0.8       , 0.54545455])},\n",
       "  'optimal_threshold': {'mean': 0.03490866596876371,\n",
       "   'std': 0.0035113130240194456,\n",
       "   'min': 0.028576423724492393,\n",
       "   'max': 0.037900945002382454,\n",
       "   'median': 0.037060942688975675,\n",
       "   'values': array([0.02857642, 0.03359752, 0.0374075 , 0.03790095, 0.03706094])},\n",
       "  'train_loss': {'mean': 0.03295656606554985,\n",
       "   'std': 0.0027014604347849478,\n",
       "   'min': 0.02756833080202341,\n",
       "   'max': 0.03465422373265028,\n",
       "   'median': 0.03412547204643488,\n",
       "   'values': array([0.02756833, 0.03412547, 0.03433691, 0.03465422, 0.03409789])}},\n",
       " 'all_metrics': {'accuracy': [0.8857142857142857,\n",
       "   0.6,\n",
       "   0.9428571428571428,\n",
       "   0.9142857142857143,\n",
       "   0.8571428571428571],\n",
       "  'precision': [0.6363636363636364,\n",
       "   0.3157894736842105,\n",
       "   1.0,\n",
       "   0.8571428571428571,\n",
       "   1.0],\n",
       "  'recall': [1.0, 0.8571428571428571, 0.7142857142857143, 0.75, 0.375],\n",
       "  'f1': [0.7777777777777778,\n",
       "   0.46153846153846156,\n",
       "   0.8333333333333333,\n",
       "   0.7999999999999999,\n",
       "   0.5454545454545454],\n",
       "  'optimal_threshold': [0.028576423724492393,\n",
       "   0.0335975190003713,\n",
       "   0.037407499427596726,\n",
       "   0.037900945002382454,\n",
       "   0.037060942688975675],\n",
       "  'train_loss': [0.02756833080202341,\n",
       "   0.03412547204643488,\n",
       "   0.03433691449463368,\n",
       "   0.03465422373265028,\n",
       "   0.034097889252007005]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline_with_cv(normal_combine, X_test_normal, X_test_faulty, \n",
    "                    device=device, batch_size=64, num_epochs=20, cv_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
