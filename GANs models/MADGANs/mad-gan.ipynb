{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np, os\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# TRUE MAD-GAN IMPLEMENTATION\n",
    "# ===============================\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MADGANEncoder(nn.Module):\n",
    "    \"\"\"Encoder for reconstruction-based anomaly detection\"\"\"\n",
    "    def __init__(self, num_features=14, hidden_dim=128, latent_dim=64, seq_len=4500):\n",
    "        super(MADGANEncoder, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # LSTM for temporal modeling (key MAD-GAN component)\n",
    "        self.lstm = nn.LSTM(num_features, hidden_dim, num_layers=2, \n",
    "                           batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism for important feature selection\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8, \n",
    "                                             dropout=0.1, batch_first=True)\n",
    "        \n",
    "        # Latent space projection\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Encode multivariate time series to latent space\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # LSTM encoding\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Self-attention for important features\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Global average pooling over time dimension\n",
    "        pooled = torch.mean(attn_out, dim=1)  # [batch, hidden_dim*2]\n",
    "        \n",
    "        # Project to latent space\n",
    "        latent = self.to_latent(pooled)\n",
    "        \n",
    "        return latent\n",
    "\n",
    "class MADGANDecoder(nn.Module):\n",
    "    \"\"\"Decoder for reconstruction\"\"\"\n",
    "    def __init__(self, latent_dim=64, hidden_dim=128, num_features=14, seq_len=4500):\n",
    "        super(MADGANDecoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Expand latent to initial hidden state\n",
    "        self.latent_to_hidden = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # LSTM decoder\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=2, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Output projection\n",
    "        self.to_output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        \"\"\"Decode latent representation back to time series\"\"\"\n",
    "        batch_size = latent.size(0)\n",
    "        \n",
    "        # Expand latent to hidden representation\n",
    "        hidden = self.latent_to_hidden(latent)  # [batch, hidden_dim]\n",
    "        \n",
    "        # Repeat for sequence length\n",
    "        hidden_seq = hidden.unsqueeze(1).repeat(1, self.seq_len, 1)  # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # LSTM decoding\n",
    "        lstm_out, _ = self.lstm(hidden_seq)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.to_output(lstm_out)  # [batch, seq_len, num_features]\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MADGANGenerator(nn.Module):\n",
    "    \"\"\"Generator component of MAD-GAN\"\"\"\n",
    "    def __init__(self, noise_dim=100, hidden_dim=128, num_features=14, seq_len=4500):\n",
    "        super(MADGANGenerator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Use decoder architecture for generation\n",
    "        self.decoder = MADGANDecoder(noise_dim, hidden_dim, num_features, seq_len)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class MADGANDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator with temporal modeling\"\"\"\n",
    "    def __init__(self, num_features=14, hidden_dim=128, seq_len=4500):\n",
    "        super(MADGANDiscriminator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # LSTM for temporal analysis\n",
    "        self.lstm = nn.LSTM(num_features, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4,\n",
    "                                             dropout=0.1, batch_first=True)\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Discriminate between real and fake sequences\"\"\"\n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Global average pooling\n",
    "        pooled = torch.mean(attn_out, dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TrueMADGAN(nn.Module):\n",
    "    \"\"\"Complete MAD-GAN system for anomaly detection\"\"\"\n",
    "    def __init__(self, num_features=14, hidden_dim=128, latent_dim=64, \n",
    "                 noise_dim=100, seq_len=4500):\n",
    "        super(TrueMADGAN, self).__init__()\n",
    "        \n",
    "        # Core components\n",
    "        self.encoder = MADGANEncoder(num_features, hidden_dim, latent_dim, seq_len)\n",
    "        self.decoder = MADGANDecoder(latent_dim, hidden_dim, num_features, seq_len)\n",
    "        self.generator = MADGANGenerator(noise_dim, hidden_dim, num_features, seq_len)\n",
    "        self.discriminator = MADGANDiscriminator(num_features, hidden_dim, seq_len)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        \n",
    "    def encode_decode(self, x):\n",
    "        \"\"\"Encode and decode for reconstruction\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent\n",
    "    \n",
    "    def generate(self, batch_size, device):\n",
    "        \"\"\"Generate fake samples\"\"\"\n",
    "        z = torch.randn(batch_size, self.noise_dim, device=device)\n",
    "        fake = self.generator(z)\n",
    "        return fake\n",
    "    \n",
    "    def compute_anomaly_score(self, x, lambda_rec=0.1):\n",
    "        \"\"\"\n",
    "        Compute MAD-GAN anomaly score\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences [batch, seq_len, features]\n",
    "            lambda_rec: Weight for reconstruction loss\n",
    "        \n",
    "        Returns:\n",
    "            anomaly_scores: Combined anomaly scores\n",
    "            disc_scores: Discrimination scores\n",
    "            rec_errors: Reconstruction errors\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Reconstruct input\n",
    "            reconstructed, _ = self.encode_decode(x)\n",
    "            \n",
    "            # Discrimination score (probability of being fake)\n",
    "            disc_logits = self.discriminator(x)\n",
    "            disc_probs = torch.sigmoid(disc_logits).squeeze()\n",
    "            \n",
    "            # Reconstruction error\n",
    "            rec_error = F.mse_loss(reconstructed, x, reduction='none')\n",
    "            rec_error = rec_error.mean(dim=(1, 2))  # Average over time and features\n",
    "            \n",
    "            # Normalize reconstruction error\n",
    "            rec_error_norm = (rec_error - rec_error.min()) / (rec_error.max() - rec_error.min() + 1e-8)\n",
    "            \n",
    "            # Combined anomaly score (higher = more anomalous)\n",
    "            # High discrimination probability + high reconstruction error = anomaly\n",
    "            anomaly_scores = (1 - disc_probs) + lambda_rec * rec_error_norm\n",
    "            \n",
    "        return anomaly_scores, disc_probs, rec_error\n",
    "    \n",
    "    def get_model_parameters(self):\n",
    "        \"\"\"Get parameter counts for each component\"\"\"\n",
    "        return {\n",
    "            'encoder': sum(p.numel() for p in self.encoder.parameters()),\n",
    "            'decoder': sum(p.numel() for p in self.decoder.parameters()),\n",
    "            'generator': sum(p.numel() for p in self.generator.parameters()),\n",
    "            'discriminator': sum(p.numel() for p in self.discriminator.parameters()),\n",
    "            'total': sum(p.numel() for p in self.parameters())\n",
    "        }\n",
    "\n",
    "# Initialize TRUE MAD-GAN\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING TRUE MAD-GAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Parameters\n",
    "num_features = X_train.shape[2]  # 14\n",
    "seq_len = X_train.shape[1]       # 4500\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "noise_dim = 100\n",
    "\n",
    "# Create true MAD-GAN\n",
    "true_madgan = TrueMADGAN(\n",
    "    num_features=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    noise_dim=noise_dim,\n",
    "    seq_len=seq_len\n",
    ").to(device)\n",
    "\n",
    "# Print model information\n",
    "params = true_madgan.get_model_parameters()\n",
    "print(f\"Model Components:\")\n",
    "print(f\"  Encoder parameters: {params['encoder']:,}\")\n",
    "print(f\"  Decoder parameters: {params['decoder']:,}\")\n",
    "print(f\"  Generator parameters: {params['generator']:,}\")\n",
    "print(f\"  Discriminator parameters: {params['discriminator']:,}\")\n",
    "print(f\"  Total parameters: {params['total']:,}\")\n",
    "\n",
    "print(f\"\\nData Information:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Features: {num_features}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRUE MAD-GAN READY FOR TRAINING\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# TRUE MAD-GAN TRAINING FUNCTION\n",
    "# ===============================\n",
    "\n",
    "def train_true_madgan(madgan_model, train_data, device, epochs=100, batch_size=16,\n",
    "                     lr_enc=0.0001, lr_dec=0.0001, lr_gen=0.0002, lr_disc=0.0001,\n",
    "                     lambda_rec=0.1, lambda_enc=0.1):\n",
    "    \"\"\"\n",
    "    Train the true MAD-GAN with all components\n",
    "    \n",
    "    Args:\n",
    "        madgan_model: TrueMADGAN instance\n",
    "        train_data: Training data (normal samples only)\n",
    "        lambda_rec: Weight for reconstruction loss\n",
    "        lambda_enc: Weight for encoder adversarial loss\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting TRUE MAD-GAN Training...\")\n",
    "    \n",
    "    # Separate optimizers for each component\n",
    "    optimizer_enc = optim.Adam(madgan_model.encoder.parameters(), lr=lr_enc, betas=(0.5, 0.999))\n",
    "    optimizer_dec = optim.Adam(madgan_model.decoder.parameters(), lr=lr_dec, betas=(0.5, 0.999))\n",
    "    optimizer_gen = optim.Adam(madgan_model.generator.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n",
    "    optimizer_disc = optim.Adam(madgan_model.discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    \n",
    "    # Data loader\n",
    "    dataset = TensorDataset(torch.tensor(train_data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'disc_loss': [], 'gen_loss': [], 'enc_loss': [], 'dec_loss': [], 'rec_loss': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  Epochs: {epochs}, Batch size: {batch_size}\")\n",
    "    print(f\"  Learning rates - Enc: {lr_enc}, Dec: {lr_dec}, Gen: {lr_gen}, Disc: {lr_disc}\")\n",
    "    print(f\"  Loss weights - Reconstruction: {lambda_rec}, Encoder: {lambda_enc}\")\n",
    "    print(f\"  Total batches per epoch: {len(dataloader)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = {'disc': [], 'gen': [], 'enc': [], 'dec': [], 'rec': []}\n",
    "        \n",
    "        madgan_model.train()\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            current_batch_size = real_data.size(0)\n",
    "            \n",
    "            # Labels for adversarial training\n",
    "            real_labels = torch.ones(current_batch_size, 1, device=device) * 0.9  # Label smoothing\n",
    "            fake_labels = torch.zeros(current_batch_size, 1, device=device) + 0.1\n",
    "            \n",
    "            # =====================================\n",
    "            # 1. Train Discriminator\n",
    "            # =====================================\n",
    "            optimizer_disc.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            real_pred = madgan_model.discriminator(real_data)\n",
    "            d_real_loss = adversarial_loss(real_pred, real_labels)\n",
    "            \n",
    "            # Fake data from generator\n",
    "            fake_data = madgan_model.generate(current_batch_size, device)\n",
    "            fake_pred = madgan_model.discriminator(fake_data.detach())\n",
    "            d_fake_loss = adversarial_loss(fake_pred, fake_labels)\n",
    "            \n",
    "            # Reconstructed data\n",
    "            reconstructed, _ = madgan_model.encode_decode(real_data)\n",
    "            recon_pred = madgan_model.discriminator(reconstructed.detach())\n",
    "            d_recon_loss = adversarial_loss(recon_pred, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss + d_recon_loss) / 3\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(madgan_model.discriminator.parameters(), 1.0)\n",
    "            optimizer_disc.step()\n",
    "            \n",
    "            epoch_losses['disc'].append(d_loss.item())\n",
    "            \n",
    "            # =====================================\n",
    "            # 2. Train Generator\n",
    "            # =====================================\n",
    "            if batch_idx % 2 == 0:  # Train generator every 2 batches\n",
    "                optimizer_gen.zero_grad()\n",
    "                \n",
    "                fake_data = madgan_model.generate(current_batch_size, device)\n",
    "                fake_pred = madgan_model.discriminator(fake_data)\n",
    "                g_loss = adversarial_loss(fake_pred, real_labels)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(madgan_model.generator.parameters(), 1.0)\n",
    "                optimizer_gen.step()\n",
    "                \n",
    "                epoch_losses['gen'].append(g_loss.item())\n",
    "            \n",
    "            # =====================================\n",
    "            # 3. Train Encoder + Decoder (Reconstruction)\n",
    "            # =====================================\n",
    "            if batch_idx % 2 == 0:  # Train reconstruction every 2 batches\n",
    "                optimizer_enc.zero_grad()\n",
    "                optimizer_dec.zero_grad()\n",
    "                \n",
    "                # Reconstruction\n",
    "                reconstructed, latent = madgan_model.encode_decode(real_data)\n",
    "                \n",
    "                # Reconstruction loss\n",
    "                rec_loss = reconstruction_loss(reconstructed, real_data)\n",
    "                \n",
    "                # Encoder adversarial loss (fool discriminator with reconstructions)\n",
    "                recon_pred = madgan_model.discriminator(reconstructed)\n",
    "                enc_adv_loss = adversarial_loss(recon_pred, real_labels)\n",
    "                \n",
    "                # Combined encoder/decoder loss\n",
    "                total_rec_loss = lambda_rec * rec_loss + lambda_enc * enc_adv_loss\n",
    "                total_rec_loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(madgan_model.encoder.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(madgan_model.decoder.parameters(), 1.0)\n",
    "                optimizer_enc.step()\n",
    "                optimizer_dec.step()\n",
    "                \n",
    "                epoch_losses['enc'].append(enc_adv_loss.item())\n",
    "                epoch_losses['dec'].append(total_rec_loss.item())\n",
    "                epoch_losses['rec'].append(rec_loss.item())\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_losses = {k: np.mean(v) if v else 0 for k, v in epoch_losses.items()}\n",
    "        \n",
    "        # Store history\n",
    "        for k, v in avg_losses.items():\n",
    "            history[f'{k}_loss'].append(v)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"D: {avg_losses['disc']:.4f} | \"\n",
    "                  f\"G: {avg_losses['gen']:.4f} | \"\n",
    "                  f\"Rec: {avg_losses['rec']:.4f} | \"\n",
    "                  f\"E: {avg_losses['enc']:.4f}\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"✅ TRUE MAD-GAN Training Completed!\")\n",
    "    return madgan_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MEMORY-OPTIMIZED MAD-GAN\n",
    "# ===============================\n",
    "\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import gc\n",
    "\n",
    "class MemoryEfficientMADGANEncoder(nn.Module):\n",
    "    \"\"\"Memory-optimized encoder with chunked processing\"\"\"\n",
    "    def __init__(self, num_features=14, hidden_dim=64, latent_dim=32, seq_len=4500):\n",
    "        super(MemoryEfficientMADGANEncoder, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.chunk_size = 500  # Process in smaller chunks\n",
    "        \n",
    "        # Smaller LSTM\n",
    "        self.lstm = nn.LSTM(num_features, hidden_dim, num_layers=1, \n",
    "                           batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Simpler attention\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, \n",
    "                                             dropout=0.1, batch_first=True)\n",
    "        \n",
    "        # Compact latent projection\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Memory-efficient forward pass with chunking\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Process in chunks to save memory\n",
    "        chunk_outputs = []\n",
    "        \n",
    "        for i in range(0, self.seq_len, self.chunk_size):\n",
    "            end_idx = min(i + self.chunk_size, self.seq_len)\n",
    "            x_chunk = x[:, i:end_idx, :]\n",
    "            \n",
    "            # Use gradient checkpointing to save memory\n",
    "            def chunk_forward(chunk):\n",
    "                lstm_out, _ = self.lstm(chunk)\n",
    "                attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "                return torch.mean(attn_out, dim=1)\n",
    "            \n",
    "            chunk_output = checkpoint.checkpoint(chunk_forward, x_chunk, use_reentrant=False)\n",
    "            chunk_outputs.append(chunk_output)\n",
    "            \n",
    "            # Clear intermediate tensors\n",
    "            del x_chunk\n",
    "        \n",
    "        # Combine chunks\n",
    "        combined = torch.stack(chunk_outputs, dim=1).mean(dim=1)\n",
    "        latent = self.to_latent(combined)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_outputs, combined\n",
    "        \n",
    "        return latent\n",
    "\n",
    "class MemoryEfficientMADGANDecoder(nn.Module):\n",
    "    \"\"\"Memory-optimized decoder\"\"\"\n",
    "    def __init__(self, latent_dim=32, hidden_dim=64, num_features=14, seq_len=4500):\n",
    "        super(MemoryEfficientMADGANDecoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        self.chunk_size = 500\n",
    "        \n",
    "        # Compact expansion\n",
    "        self.latent_to_hidden = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Smaller LSTM\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, \n",
    "                           batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Simple output projection\n",
    "        self.to_output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, latent):\n",
    "        \"\"\"Memory-efficient chunked decoding\"\"\"\n",
    "        batch_size = latent.size(0)\n",
    "        hidden = self.latent_to_hidden(latent)\n",
    "        \n",
    "        # Generate in chunks\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(0, self.seq_len, self.chunk_size):\n",
    "            end_idx = min(i + self.chunk_size, self.seq_len)\n",
    "            chunk_len = end_idx - i\n",
    "            \n",
    "            # Create hidden sequence for this chunk\n",
    "            hidden_chunk = hidden.unsqueeze(1).repeat(1, chunk_len, 1)\n",
    "            \n",
    "            # Process chunk\n",
    "            def decode_chunk(h_chunk):\n",
    "                lstm_out, _ = self.lstm(h_chunk)\n",
    "                return self.to_output(lstm_out)\n",
    "            \n",
    "            output_chunk = checkpoint.checkpoint(decode_chunk, hidden_chunk, use_reentrant=False)\n",
    "            outputs.append(output_chunk)\n",
    "            \n",
    "            # Clear memory\n",
    "            del hidden_chunk\n",
    "        \n",
    "        result = torch.cat(outputs, dim=1)\n",
    "        del outputs\n",
    "        \n",
    "        return result\n",
    "\n",
    "class MemoryEfficientMADGANDiscriminator(nn.Module):\n",
    "    \"\"\"Memory-optimized discriminator\"\"\"\n",
    "    def __init__(self, num_features=14, hidden_dim=64, seq_len=4500):\n",
    "        super(MemoryEfficientMADGANDiscriminator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.chunk_size = 500\n",
    "        \n",
    "        # Smaller LSTM\n",
    "        self.lstm = nn.LSTM(num_features, hidden_dim, num_layers=1,\n",
    "                           batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Simple classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Memory-efficient chunked discrimination\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        chunk_features = []\n",
    "        \n",
    "        for i in range(0, seq_len, self.chunk_size):\n",
    "            end_idx = min(i + self.chunk_size, seq_len)\n",
    "            x_chunk = x[:, i:end_idx, :]\n",
    "            \n",
    "            def process_chunk(chunk):\n",
    "                lstm_out, _ = self.lstm(chunk)\n",
    "                return torch.mean(lstm_out, dim=1)\n",
    "            \n",
    "            chunk_feature = checkpoint.checkpoint(process_chunk, x_chunk, use_reentrant=False)\n",
    "            chunk_features.append(chunk_feature)\n",
    "            \n",
    "            del x_chunk\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.stack(chunk_features, dim=1).mean(dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        del chunk_features, combined\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MemoryOptimizedMADGAN(nn.Module):\n",
    "    \"\"\"Memory-optimized complete MAD-GAN system\"\"\"\n",
    "    def __init__(self, num_features=14, hidden_dim=64, latent_dim=32, \n",
    "                 noise_dim=64, seq_len=4500):\n",
    "        super(MemoryOptimizedMADGAN, self).__init__()\n",
    "        \n",
    "        # Smaller components\n",
    "        self.encoder = MemoryEfficientMADGANEncoder(num_features, hidden_dim, latent_dim, seq_len)\n",
    "        self.decoder = MemoryEfficientMADGANDecoder(latent_dim, hidden_dim, num_features, seq_len)\n",
    "        self.discriminator = MemoryEfficientMADGANDiscriminator(num_features, hidden_dim, seq_len)\n",
    "        \n",
    "        # Generator uses decoder\n",
    "        self.generator_projection = nn.Sequential(\n",
    "            nn.Linear(noise_dim, latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        \n",
    "    def encode_decode(self, x):\n",
    "        \"\"\"Encode and decode with memory management\"\"\"\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent\n",
    "    \n",
    "    def generate(self, batch_size, device):\n",
    "        \"\"\"Generate with memory efficiency\"\"\"\n",
    "        z = torch.randn(batch_size, self.noise_dim, device=device)\n",
    "        latent = self.generator_projection(z)\n",
    "        fake = self.decoder(latent)\n",
    "        del z, latent\n",
    "        return fake\n",
    "    \n",
    "    def compute_anomaly_score(self, x, lambda_rec=0.1):\n",
    "        \"\"\"Memory-efficient anomaly scoring\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Process in smaller batches to save memory\n",
    "            batch_size = x.size(0)\n",
    "            if batch_size > 4:  # Split large batches\n",
    "                mid = batch_size // 2\n",
    "                scores1, disc1, rec1 = self.compute_anomaly_score(x[:mid], lambda_rec)\n",
    "                scores2, disc2, rec2 = self.compute_anomaly_score(x[mid:], lambda_rec)\n",
    "                \n",
    "                scores = torch.cat([scores1, scores2])\n",
    "                disc_scores = torch.cat([disc1, disc2])\n",
    "                rec_errors = torch.cat([rec1, rec2])\n",
    "                \n",
    "                return scores, disc_scores, rec_errors\n",
    "            \n",
    "            # Reconstruct\n",
    "            reconstructed, _ = self.encode_decode(x)\n",
    "            \n",
    "            # Discriminate\n",
    "            disc_logits = self.discriminator(x)\n",
    "            disc_probs = torch.sigmoid(disc_logits).squeeze()\n",
    "            \n",
    "            # Reconstruction error\n",
    "            rec_error = F.mse_loss(reconstructed, x, reduction='none')\n",
    "            rec_error = rec_error.mean(dim=(1, 2))\n",
    "            \n",
    "            # Combined score\n",
    "            anomaly_scores = (1 - disc_probs) + lambda_rec * rec_error\n",
    "            \n",
    "            # Clean up\n",
    "            del reconstructed, disc_logits\n",
    "            \n",
    "        return anomaly_scores, disc_probs, rec_error\n",
    "    \n",
    "    def get_model_parameters(self):\n",
    "        \"\"\"Get parameter counts\"\"\"\n",
    "        return {\n",
    "            'encoder': sum(p.numel() for p in self.encoder.parameters()),\n",
    "            'decoder': sum(p.numel() for p in self.decoder.parameters()),\n",
    "            'discriminator': sum(p.numel() for p in self.discriminator.parameters()),\n",
    "            'generator_proj': sum(p.numel() for p in self.generator_projection.parameters()),\n",
    "            'total': sum(p.numel() for p in self.parameters())\n",
    "        }\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Initialize memory-optimized MAD-GAN\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING MEMORY-OPTIMIZED MAD-GAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear memory first\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Smaller parameters for memory efficiency\n",
    "num_features = X_train.shape[2]  # 14\n",
    "seq_len = X_train.shape[1]       # 4500\n",
    "hidden_dim = 64      # Reduced from 128\n",
    "latent_dim = 32      # Reduced from 64\n",
    "noise_dim = 64       # Reduced from 100\n",
    "\n",
    "# Create memory-optimized MAD-GAN\n",
    "memory_madgan = MemoryOptimizedMADGAN(\n",
    "    num_features=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    noise_dim=noise_dim,\n",
    "    seq_len=seq_len\n",
    ").to(device)\n",
    "\n",
    "# Print model information\n",
    "params = memory_madgan.get_model_parameters()\n",
    "print(f\"Memory-Optimized Model Components:\")\n",
    "print(f\"  Encoder parameters: {params['encoder']:,}\")\n",
    "print(f\"  Decoder parameters: {params['decoder']:,}\")\n",
    "print(f\"  Discriminator parameters: {params['discriminator']:,}\")\n",
    "print(f\"  Generator projection parameters: {params['generator_proj']:,}\")\n",
    "print(f\"  Total parameters: {params['total']:,}\")\n",
    "\n",
    "# Compare with original\n",
    "print(f\"\\nParameter Reduction:\")\n",
    "original_total = sum(p.numel() for p in true_madgan.parameters())\n",
    "reduction = (1 - params['total'] / original_total) * 100\n",
    "print(f\"  Original model: {original_total:,} parameters\")\n",
    "print(f\"  Optimized model: {params['total']:,} parameters\")\n",
    "print(f\"  Reduction: {reduction:.1f}%\")\n",
    "\n",
    "print(f\"\\nMemory Usage Check:\")\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "    print(f\"  Allocated: {memory_allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {memory_reserved:.2f} GB\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MEMORY-OPTIMIZED MAD-GAN READY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faae24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MEMORY-EFFICIENT TRAINING FUNCTION\n",
    "# ===============================\n",
    "\n",
    "def train_memory_efficient_madgan(madgan_model, train_data, device, \n",
    "                                 epochs=50, batch_size=4, accumulation_steps=4,\n",
    "                                 lr_enc=0.0001, lr_dec=0.0001, lr_gen=0.0002, lr_disc=0.0001,\n",
    "                                 lambda_rec=0.1, lambda_enc=0.1):\n",
    "    \"\"\"\n",
    "    Memory-efficient training with gradient accumulation and frequent cleanup\n",
    "    \n",
    "    Args:\n",
    "        accumulation_steps: Accumulate gradients over multiple mini-batches\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Memory-Efficient MAD-GAN Training...\")\n",
    "    \n",
    "    # Use gradient accumulation to simulate larger batch sizes\n",
    "    effective_batch_size = batch_size * accumulation_steps\n",
    "    \n",
    "    # Optimizers with lower memory usage\n",
    "    optimizer_enc = optim.Adam(madgan_model.encoder.parameters(), lr=lr_enc, betas=(0.5, 0.999))\n",
    "    optimizer_dec = optim.Adam(madgan_model.decoder.parameters(), lr=lr_dec, betas=(0.5, 0.999))\n",
    "    optimizer_gen = optim.Adam(madgan_model.generator_projection.parameters(), lr=lr_gen, betas=(0.5, 0.999))\n",
    "    optimizer_disc = optim.Adam(madgan_model.discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    \n",
    "    # Data loader with small batch size\n",
    "    dataset = TensorDataset(torch.tensor(train_data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'disc_loss': [], 'gen_loss': [], 'enc_loss': [], 'dec_loss': [], 'rec_loss': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Mini-batch size: {batch_size}\")\n",
    "    print(f\"  Accumulation steps: {accumulation_steps}\")\n",
    "    print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "    print(f\"  Learning rates - Enc: {lr_enc}, Dec: {lr_dec}, Gen: {lr_gen}, Disc: {lr_disc}\")\n",
    "    print(f\"  Total batches per epoch: {len(dataloader)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = {'disc': [], 'gen': [], 'enc': [], 'dec': [], 'rec': []}\n",
    "        \n",
    "        # Zero gradients at start of epoch\n",
    "        optimizer_disc.zero_grad()\n",
    "        optimizer_gen.zero_grad()\n",
    "        optimizer_enc.zero_grad()\n",
    "        optimizer_dec.zero_grad()\n",
    "        \n",
    "        madgan_model.train()\n",
    "        \n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            current_batch_size = real_data.size(0)\n",
    "            \n",
    "            # Labels\n",
    "            real_labels = torch.ones(current_batch_size, 1, device=device) * 0.9\n",
    "            fake_labels = torch.zeros(current_batch_size, 1, device=device) + 0.1\n",
    "            \n",
    "            # =====================================\n",
    "            # Train Discriminator (with accumulation)\n",
    "            # =====================================\n",
    "            \n",
    "            # Real data\n",
    "            real_pred = madgan_model.discriminator(real_data)\n",
    "            d_real_loss = adversarial_loss(real_pred, real_labels) / accumulation_steps\n",
    "            d_real_loss.backward()\n",
    "            \n",
    "            # Fake data from generator\n",
    "            with torch.no_grad():\n",
    "                fake_data = madgan_model.generate(current_batch_size, device)\n",
    "            fake_pred = madgan_model.discriminator(fake_data)\n",
    "            d_fake_loss = adversarial_loss(fake_pred, fake_labels) / accumulation_steps\n",
    "            d_fake_loss.backward()\n",
    "            \n",
    "            # Reconstructed data\n",
    "            with torch.no_grad():\n",
    "                reconstructed, _ = madgan_model.encode_decode(real_data)\n",
    "            recon_pred = madgan_model.discriminator(reconstructed)\n",
    "            d_recon_loss = adversarial_loss(recon_pred, fake_labels) / accumulation_steps\n",
    "            d_recon_loss.backward()\n",
    "            \n",
    "            # Step discriminator every accumulation_steps\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(madgan_model.discriminator.parameters(), 1.0)\n",
    "                optimizer_disc.step()\n",
    "                optimizer_disc.zero_grad()\n",
    "            \n",
    "            d_loss_total = (d_real_loss + d_fake_loss + d_recon_loss) * accumulation_steps\n",
    "            epoch_losses['disc'].append(d_loss_total.item())\n",
    "            \n",
    "            # Clear intermediate tensors\n",
    "            del real_pred, fake_data, fake_pred, reconstructed, recon_pred\n",
    "            \n",
    "            # =====================================\n",
    "            # Train Generator (less frequently)\n",
    "            # =====================================\n",
    "            if batch_idx % (accumulation_steps * 2) == 0:\n",
    "                fake_data = madgan_model.generate(current_batch_size, device)\n",
    "                fake_pred = madgan_model.discriminator(fake_data)\n",
    "                g_loss = adversarial_loss(fake_pred, real_labels) / accumulation_steps\n",
    "                g_loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(madgan_model.generator_projection.parameters(), 1.0)\n",
    "                    optimizer_gen.step()\n",
    "                    optimizer_gen.zero_grad()\n",
    "                \n",
    "                epoch_losses['gen'].append(g_loss.item() * accumulation_steps)\n",
    "                del fake_data, fake_pred\n",
    "            \n",
    "            # =====================================\n",
    "            # Train Encoder + Decoder (Reconstruction)\n",
    "            # =====================================\n",
    "            if batch_idx % accumulation_steps == 0:\n",
    "                # Reconstruction\n",
    "                reconstructed, latent = madgan_model.encode_decode(real_data)\n",
    "                \n",
    "                # Reconstruction loss\n",
    "                rec_loss = reconstruction_loss(reconstructed, real_data) / accumulation_steps\n",
    "                \n",
    "                # Encoder adversarial loss\n",
    "                recon_pred = madgan_model.discriminator(reconstructed)\n",
    "                enc_adv_loss = adversarial_loss(recon_pred, real_labels) / accumulation_steps\n",
    "                \n",
    "                # Combined loss\n",
    "                total_rec_loss = lambda_rec * rec_loss + lambda_enc * enc_adv_loss\n",
    "                total_rec_loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(madgan_model.encoder.parameters(), 1.0)\n",
    "                    torch.nn.utils.clip_grad_norm_(madgan_model.decoder.parameters(), 1.0)\n",
    "                    optimizer_enc.step()\n",
    "                    optimizer_dec.step()\n",
    "                    optimizer_enc.zero_grad()\n",
    "                    optimizer_dec.zero_grad()\n",
    "                \n",
    "                epoch_losses['enc'].append(enc_adv_loss.item() * accumulation_steps)\n",
    "                epoch_losses['dec'].append(total_rec_loss.item() * accumulation_steps)\n",
    "                epoch_losses['rec'].append(rec_loss.item() * accumulation_steps)\n",
    "                \n",
    "                del reconstructed, latent, recon_pred\n",
    "            \n",
    "            # Aggressive memory cleanup\n",
    "            if batch_idx % 5 == 0:\n",
    "                clear_gpu_memory()\n",
    "            \n",
    "            # Memory monitoring\n",
    "            if batch_idx % 20 == 0 and torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "                if memory_used > 6.0:  # Warning if using more than 6GB\n",
    "                    print(f\"    WARNING: High memory usage: {memory_used:.2f}GB at batch {batch_idx}\")\n",
    "                    clear_gpu_memory()\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_losses = {k: np.mean(v) if v else 0 for k, v in epoch_losses.items()}\n",
    "        \n",
    "        # Store history\n",
    "        for k, v in avg_losses.items():\n",
    "            history[f'{k}_loss'].append(v)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"D: {avg_losses['disc']:.4f} | \"\n",
    "                  f\"G: {avg_losses['gen']:.4f} | \"\n",
    "                  f\"Rec: {avg_losses['rec']:.4f} | \"\n",
    "                  f\"E: {avg_losses['enc']:.4f}\")\n",
    "            \n",
    "            # Memory status\n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "                print(f\"         Memory: {memory_used:.2f}GB\")\n",
    "        \n",
    "        # Aggressive cleanup at end of epoch\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    print(\"✅ Memory-Efficient Training Completed!\")\n",
    "    return madgan_model, history\n",
    "\n",
    "# Test memory usage before training\n",
    "def test_memory_usage():\n",
    "    \"\"\"Test memory usage with current model\"\"\"\n",
    "    print(\"🔍 Testing Memory Usage...\")\n",
    "    \n",
    "    # Test with small batch\n",
    "    test_batch_size = 2\n",
    "    test_data = torch.tensor(X_train[:test_batch_size], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Clear memory first\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Monitor initial memory\n",
    "    initial_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    print(f\"Initial memory: {initial_memory:.2f}GB\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    memory_madgan.eval()\n",
    "    with torch.no_grad():\n",
    "        # Test encoding\n",
    "        latent = memory_madgan.encoder(test_data)\n",
    "        memory_after_encode = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        print(f\"After encoding: {memory_after_encode:.2f}GB\")\n",
    "        \n",
    "        # Test decoding\n",
    "        reconstructed = memory_madgan.decoder(latent)\n",
    "        memory_after_decode = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        print(f\"After decoding: {memory_after_decode:.2f}GB\")\n",
    "        \n",
    "        # Test discrimination\n",
    "        disc_output = memory_madgan.discriminator(test_data)\n",
    "        memory_after_disc = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        print(f\"After discrimination: {memory_after_disc:.2f}GB\")\n",
    "        \n",
    "        # Test generation\n",
    "        fake_data = memory_madgan.generate(test_batch_size, device)\n",
    "        memory_after_gen = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        print(f\"After generation: {memory_after_gen:.2f}GB\")\n",
    "        \n",
    "        # Test anomaly scoring\n",
    "        anomaly_scores, _, _ = memory_madgan.compute_anomaly_score(test_data)\n",
    "        memory_after_scoring = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        print(f\"After anomaly scoring: {memory_after_scoring:.2f}GB\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    final_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    print(f\"After cleanup: {final_memory:.2f}GB\")\n",
    "    \n",
    "    print(\"✅ Memory test completed successfully!\")\n",
    "    \n",
    "    return memory_after_scoring < 8.0  # Return True if under 8GB\n",
    "\n",
    "# Run memory test\n",
    "memory_ok = test_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71bfd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# EXECUTE MEMORY-EFFICIENT TRAINING\n",
    "# ===============================\n",
    "\n",
    "if memory_ok:\n",
    "    print(\"✅ Memory test passed. Starting training...\")\n",
    "    \n",
    "    # Clear all memory before training\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Train with very conservative settings\n",
    "    try:\n",
    "        trained_memory_madgan, memory_training_history = train_memory_efficient_madgan(\n",
    "            madgan_model=memory_madgan,\n",
    "            train_data=X_train,\n",
    "            device=device,\n",
    "            epochs=200,              # Reduced epochs\n",
    "            batch_size=2,           # Very small batch size\n",
    "            accumulation_steps=8,   # Effective batch size = 16\n",
    "            lr_enc=0.0001,\n",
    "            lr_dec=0.0001,\n",
    "            lr_gen=0.0002,\n",
    "            lr_disc=0.0001,\n",
    "            lambda_rec=0.1,\n",
    "            lambda_enc=0.1\n",
    "        )\n",
    "        \n",
    "        print(\"🎉 Training completed successfully!\")\n",
    "        \n",
    "        # Generate synthetic data with memory management\n",
    "        print(\"\\n🔄 Generating synthetic data...\")\n",
    "        \n",
    "        trained_memory_madgan.eval()\n",
    "        generated_samples = []\n",
    "        generation_batch_size = len(X_train)  # Very small for generation\n",
    "        \n",
    "        num_samples_to_generate = len(X_train)  # Limit generation\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, num_samples_to_generate, generation_batch_size):\n",
    "                current_batch = min(generation_batch_size, num_samples_to_generate - i)\n",
    "                fake_data = trained_memory_madgan.generate(current_batch, device)\n",
    "                generated_samples.append(fake_data.cpu().numpy())\n",
    "                \n",
    "                # Clear GPU memory after each batch\n",
    "                del fake_data\n",
    "                if i % 20 == 0:\n",
    "                    clear_gpu_memory()\n",
    "        \n",
    "        # Combine generated samples\n",
    "        memory_generated_data = np.concatenate(generated_samples, axis=0)\n",
    "        \n",
    "        print(f\"✅ Generated {len(memory_generated_data)} samples\")\n",
    "        print(f\"Generated data shape: {memory_generated_data.shape}\")\n",
    "        \n",
    "        # Plot training history with memory-efficient plotting\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 3, 1)\n",
    "        if memory_training_history['disc_loss']:\n",
    "            plt.plot(memory_training_history['disc_loss'], label='Discriminator', alpha=0.8)\n",
    "        plt.title('Discriminator Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        if memory_training_history['gen_loss']:\n",
    "            plt.plot(memory_training_history['gen_loss'], label='Generator', color='orange', alpha=0.8)\n",
    "        plt.title('Generator Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 3)\n",
    "        if memory_training_history['rec_loss']:\n",
    "            plt.plot(memory_training_history['rec_loss'], label='Reconstruction', color='green', alpha=0.8)\n",
    "        plt.title('Reconstruction Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 4)\n",
    "        if memory_training_history['enc_loss']:\n",
    "            plt.plot(memory_training_history['enc_loss'], label='Encoder Adv', color='red', alpha=0.8)\n",
    "        plt.title('Encoder Adversarial Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        if memory_training_history['dec_loss']:\n",
    "            plt.plot(memory_training_history['dec_loss'], label='Decoder Total', color='purple', alpha=0.8)\n",
    "        plt.title('Decoder Total Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        # Plot key losses only to avoid clutter\n",
    "        if memory_training_history['disc_loss']:\n",
    "            plt.plot(memory_training_history['disc_loss'], label='Discriminator', alpha=0.7)\n",
    "        if memory_training_history['rec_loss']:\n",
    "            plt.plot(memory_training_history['rec_loss'], label='Reconstruction', alpha=0.7)\n",
    "        plt.title('Key Losses')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final statistics\n",
    "        print(\"\\n📊 Final Training Statistics:\")\n",
    "        if memory_training_history['disc_loss']:\n",
    "            print(f\"   Final Discriminator Loss: {memory_training_history['disc_loss'][-1]:.4f}\")\n",
    "        if memory_training_history['gen_loss']:\n",
    "            print(f\"   Final Generator Loss: {memory_training_history['gen_loss'][-1]:.4f}\")\n",
    "        if memory_training_history['rec_loss']:\n",
    "            print(f\"   Final Reconstruction Loss: {memory_training_history['rec_loss'][-1]:.4f}\")\n",
    "        \n",
    "        # Final memory check\n",
    "        final_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        print(f\"   Final GPU Memory Usage: {final_memory:.2f}GB\")\n",
    "        \n",
    "        # Test anomaly detection on a few samples\n",
    "        print(\"\\n🔍 Testing Anomaly Detection...\")\n",
    "        test_samples = torch.tensor(X_train[:5], dtype=torch.float32).to(device)\n",
    "        anomaly_scores, disc_scores, rec_errors = trained_memory_madgan.compute_anomaly_score(test_samples)\n",
    "        \n",
    "        print(\"Normal samples anomaly scores:\")\n",
    "        for i, (ascore, dscore, rerror) in enumerate(zip(anomaly_scores, disc_scores, rec_errors)):\n",
    "            print(f\"   Sample {i+1}: Anomaly={ascore:.4f}, Disc={dscore:.4f}, RecErr={rerror:.4f}\")\n",
    "        \n",
    "        clear_gpu_memory()\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"❌ Still running out of memory. Trying even smaller settings...\")\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # Ultra-conservative fallback\n",
    "            try:\n",
    "                trained_memory_madgan, memory_training_history = train_memory_efficient_madgan(\n",
    "                    madgan_model=memory_madgan,\n",
    "                    train_data=X_train,  # Use only subset of data\n",
    "                    device=device,\n",
    "                    epochs=200,\n",
    "                    batch_size=1,           # Minimum batch size\n",
    "                    accumulation_steps=16,  # Larger accumulation\n",
    "                    lr_enc=0.0001,\n",
    "                    lr_dec=0.0001,\n",
    "                    lr_gen=0.0002,\n",
    "                    lr_disc=0.0001,\n",
    "                    lambda_rec=0.1,\n",
    "                    lambda_enc=0.1\n",
    "                )\n",
    "                print(\"✅ Ultra-conservative training completed!\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"❌ Training failed even with minimal settings: {e2}\")\n",
    "                print(\"💡 Suggestions:\")\n",
    "                print(\"   - Use CPU training (slower but will work)\")\n",
    "                print(\"   - Reduce sequence length (truncate time series)\")\n",
    "                print(\"   - Use even smaller model dimensions\")\n",
    "        else:\n",
    "            print(f\"❌ Training failed with error: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ Memory test failed. GPU memory insufficient.\")\n",
    "    print(\"💡 Recommendations:\")\n",
    "    print(\"   1. Reduce sequence length (currently 4500 timesteps)\")\n",
    "    print(\"   2. Use CPU training instead of GPU\")\n",
    "    print(\"   3. Process data in smaller chunks\")\n",
    "    print(\"   4. Use simpler model architecture\")\n",
    "\n",
    "# Final cleanup\n",
    "clear_gpu_memory()\n",
    "print(\"\\n🧹 Final memory cleanup completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train[:100]  # Use 100 samples for testing\n",
    "test_generated = memory_generated_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"❌ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_train, faulty_data, device, memory_generated_data, epochs=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
