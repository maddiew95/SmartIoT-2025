{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np, os\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MADGAN ARCHITECTURE FOR MULTIVARIATE TIME SERIES ANOMALY DETECTION\n",
    "# ===============================\n",
    "\n",
    "class MemoryEfficientMADGAN(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64, sequence_length=None):\n",
    "        super(MemoryEfficientMADGAN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_length = sequence_length or input_dim\n",
    "        \n",
    "        # Memory-efficient Generator with residual connections\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Lightweight Discriminator\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Encoder for anomaly detection\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Memory-efficient training function\n",
    "def train_madgan_memory_efficient(model, normal_data, epochs=100, batch_size=32, lr=0.0002):\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizers with gradient clipping for stability\n",
    "    optimizer_G = optim.Adam(model.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(model.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_E = optim.Adam(model.encoder.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    # Create DataLoader for memory efficiency\n",
    "    normal_tensor = torch.FloatTensor(normal_data)\n",
    "    dataset = TensorDataset(normal_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_d_loss = 0\n",
    "        epoch_g_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_data, in dataloader:\n",
    "            batch_data = batch_data.to(device, non_blocking=True)\n",
    "            batch_size_actual = batch_data.size(0)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if num_batches % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            real_labels = torch.ones(batch_size_actual, 1).to(device)\n",
    "            real_output = model.discriminator(batch_data)\n",
    "            d_loss_real = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Fake data\n",
    "            z = torch.randn(batch_size_actual, model.latent_dim).to(device)\n",
    "            fake_data = model.generator(z).detach()\n",
    "            fake_labels = torch.zeros(batch_size_actual, 1).to(device)\n",
    "            fake_output = model.discriminator(fake_data)\n",
    "            d_loss_fake = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.discriminator.parameters(), 1.0)\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train Generator and Encoder\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_E.zero_grad()\n",
    "            \n",
    "            # Generator loss\n",
    "            z = torch.randn(batch_size_actual, model.latent_dim).to(device)\n",
    "            fake_data = model.generator(z)\n",
    "            fake_output = model.discriminator(fake_data)\n",
    "            g_loss_adv = criterion(fake_output, real_labels)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            encoded = model.encoder(batch_data)\n",
    "            reconstructed = model.generator(encoded)\n",
    "            reconstruction_loss = mse_loss(reconstructed, batch_data)\n",
    "            \n",
    "            # Feature matching loss\n",
    "            with torch.no_grad():\n",
    "                real_features = model.discriminator(batch_data)\n",
    "            fake_features = model.discriminator(fake_data)\n",
    "            feature_loss = mse_loss(fake_features, real_features)\n",
    "            \n",
    "            # Combined loss\n",
    "            g_loss = g_loss_adv + 10 * reconstruction_loss + 5 * feature_loss\n",
    "            g_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.generator.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model.encoder.parameters(), 1.0)\n",
    "            \n",
    "            optimizer_G.step()\n",
    "            optimizer_E.step()\n",
    "            \n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}] D_Loss: {epoch_d_loss/num_batches:.4f}, G_Loss: {epoch_g_loss/num_batches:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Anomaly detection function\n",
    "def detect_anomalies_madgan(model, test_data, threshold_percentile=95):\n",
    "    model.eval()\n",
    "    anomaly_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.FloatTensor(test_data).to(device)\n",
    "        \n",
    "        # Reconstruction-based anomaly score\n",
    "        encoded = model.encoder(test_tensor)\n",
    "        reconstructed = model.generator(encoded)\n",
    "        reconstruction_errors = torch.mean((test_tensor - reconstructed) ** 2, dim=1)\n",
    "        \n",
    "        # Discriminator-based anomaly score\n",
    "        discriminator_scores = model.discriminator(test_tensor).squeeze()\n",
    "        \n",
    "        # Combined anomaly score\n",
    "        combined_scores = reconstruction_errors + (1 - discriminator_scores)\n",
    "        anomaly_scores = combined_scores.cpu().numpy()\n",
    "    \n",
    "    # Dynamic threshold\n",
    "    threshold = np.percentile(anomaly_scores, threshold_percentile)\n",
    "    predictions = (anomaly_scores > threshold).astype(int)\n",
    "    \n",
    "    return predictions, anomaly_scores, threshold\n",
    "\n",
    "# Initialize and train the model\n",
    "print(\"Initializing Memory-Efficient MADGAN...\")\n",
    "\n",
    "# Flatten the data to 2D if it's 3D\n",
    "if len(X_train_normal.shape) == 3:\n",
    "    print(f\"Original data shape: {X_train_normal.shape}\")\n",
    "    X_train_flattened = X_train_normal.reshape(X_train_normal.shape[0], -1)\n",
    "    print(f\"Flattened data shape: {X_train_flattened.shape}\")\n",
    "    input_dim = X_train_flattened.shape[1]\n",
    "else:\n",
    "    X_train_flattened = X_train_normal\n",
    "    input_dim = X_train_normal.shape[1]\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "madgan_model = MemoryEfficientMADGAN(input_dim=input_dim, latent_dim=64)\n",
    "\n",
    "print(f\"Training MADGAN on {X_train_flattened.shape[0]} normal samples...\")\n",
    "trained_madgan = train_madgan_memory_efficient(\n",
    "    madgan_model, \n",
    "    X_train_flattened, \n",
    "    epochs=200, \n",
    "    batch_size=32,\n",
    "    lr=0.0002\n",
    ")\n",
    "\n",
    "# Generate synthetic data for downstream tasks\n",
    "print(\"Generating synthetic normal data...\")\n",
    "trained_madgan.eval()\n",
    "with torch.no_grad():\n",
    "    num_samples = len(X_train_normal)  # Memory-efficient generation\n",
    "    z = torch.randn(num_samples, trained_madgan.latent_dim).to(device)\n",
    "    memory_generated_data = trained_madgan.generator(z).cpu().numpy()\n",
    "\n",
    "\n",
    "# Reshape generated data to match original input shape (n, 4500, 14)\n",
    "memory_generated_data = memory_generated_data.reshape(-1, X_train_normal.shape[1], X_train_normal.shape[2])\n",
    "\n",
    "print(f\"Generated data shape: {memory_generated_data.shape}\")\n",
    "torch.cuda.empty_cache()  # Clear GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = memory_generated_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"âŒ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, memory_generated_data, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c483e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
