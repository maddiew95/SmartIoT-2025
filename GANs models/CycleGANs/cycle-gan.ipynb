{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca19d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:28:25.636432Z",
     "iopub.status.busy": "2025-07-13T07:28:25.635653Z",
     "iopub.status.idle": "2025-07-13T07:28:32.318310Z",
     "shell.execute_reply": "2025-07-13T07:28:32.317412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A30\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aec1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:28:32.322153Z",
     "iopub.status.busy": "2025-07-13T07:28:32.321731Z",
     "iopub.status.idle": "2025-07-13T07:28:32.360406Z",
     "shell.execute_reply": "2025-07-13T07:28:32.359339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Improved Residual Block for Time Series\n",
    "class TimeSeriesResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=5):  # Changed from 3 to 5 for better temporal patterns\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.norm1 = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.norm2 = nn.BatchNorm1d(channels)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.norm1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "# Enhanced Generator for Time Series\n",
    "class TimeSeriesGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=14, hidden_dim=64, n_residual_blocks=4):  # Reduced complexity for long sequences\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution with larger kernel for temporal context\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_dim//2, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Downsampling layers with stride=4 for long sequences\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim//2, hidden_dim, kernel_size=5, stride=4, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=5, stride=4, padding=2),\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks for feature refinement\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            TimeSeriesResidualBlock(hidden_dim*2) for _ in range(n_residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Upsampling layers with stride=4 to match downsampling\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dim*2, hidden_dim, kernel_size=5, stride=4, padding=2, output_padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dim, hidden_dim//2, kernel_size=5, stride=4, padding=2, output_padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim//2, input_channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        original_size = x.size(2)\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        \n",
    "        # Ensure exact sequence length restoration\n",
    "        if x.size(2) != original_size:\n",
    "            x = nn.functional.interpolate(x, size=original_size, mode='linear', align_corners=False)\n",
    "        \n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "# Enhanced Discriminator for Time Series\n",
    "class TimeSeriesDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=14, hidden_dim=32):  # Reduced for memory efficiency\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Progressive downsampling for long sequences\n",
    "            spectral_norm(nn.Conv1d(input_channels, hidden_dim, kernel_size=5, stride=4, padding=2)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=5, stride=4, padding=2)),\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*2, hidden_dim*4, kernel_size=5, stride=4, padding=2)),\n",
    "            nn.BatchNorm1d(hidden_dim*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*4, hidden_dim*8, kernel_size=5, stride=4, padding=2)),\n",
    "            nn.BatchNorm1d(hidden_dim*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*8, 1, kernel_size=5, padding=2)),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# Enhanced CycleGAN training function\n",
    "def train_cyclegan_timeseries_stable(normal_data, device, epochs=200, batch_size=8, lr=2e-4):\n",
    "    \"\"\"\n",
    "    Enhanced CycleGAN training optimized for multivariate time series\n",
    "    \"\"\"\n",
    "    print(f\"Training CycleGAN on data shape: {normal_data.shape}\")\n",
    "    \n",
    "    # Better data preprocessing for time series\n",
    "    data_mean = np.mean(normal_data, axis=(0, 1), keepdims=True)\n",
    "    data_std = np.std(normal_data, axis=(0, 1), keepdims=True) + 1e-8\n",
    "    \n",
    "    # Normalize to [-1, 1] for Tanh output\n",
    "    normalized_data = (normal_data - data_mean) / (3 * data_std)\n",
    "    normalized_data = np.clip(normalized_data, -1, 1)\n",
    "    \n",
    "    # Split data into two temporal domains (first half vs second half)\n",
    "    mid_point = len(normalized_data) // 2\n",
    "    domain_A = normalized_data[:mid_point]\n",
    "    domain_B = normalized_data[mid_point:]\n",
    "    \n",
    "    # Convert to tensors (batch, channels, seq_len)\n",
    "    tensor_A = torch.tensor(domain_A, dtype=torch.float32).permute(0, 2, 1)\n",
    "    tensor_B = torch.tensor(domain_B, dtype=torch.float32).permute(0, 2, 1)\n",
    "    \n",
    "    dataset = TensorDataset(tensor_A, tensor_B)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Initialize networks with optimized parameters\n",
    "    G_AB = TimeSeriesGenerator().to(device)\n",
    "    G_BA = TimeSeriesGenerator().to(device)\n",
    "    D_A = TimeSeriesDiscriminator().to(device)\n",
    "    D_B = TimeSeriesDiscriminator().to(device)\n",
    "    \n",
    "    # Balanced optimizers for time series\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(G_AB.parameters()) + list(G_BA.parameters()),\n",
    "        lr=lr, betas=(0.5, 0.999)\n",
    "    )\n",
    "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr/2, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr/2, betas=(0.5, 0.999))\n",
    "        \n",
    "    def adversarial_loss_smooth(pred, target_is_real):\n",
    "        if target_is_real:\n",
    "            target = torch.ones_like(pred) * (0.8 + 0.2 * torch.rand_like(pred))\n",
    "        else:\n",
    "            target = torch.zeros_like(pred) + 0.2 * torch.rand_like(pred)\n",
    "        return nn.MSELoss()(pred, target)\n",
    "    \n",
    "    cycle_loss = nn.L1Loss()\n",
    "    identity_loss = nn.L1Loss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'G_loss': [], 'D_A_loss': [], 'D_B_loss': [],\n",
    "        'cycle_loss': [], 'identity_loss': []\n",
    "    }\n",
    "    \n",
    "    print(\"Starting optimized training for time series...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_G_loss = 0\n",
    "        epoch_D_A_loss = 0\n",
    "        epoch_D_B_loss = 0\n",
    "        epoch_cycle_loss = 0\n",
    "        epoch_identity_loss = 0\n",
    "        \n",
    "        for i, (real_A, real_B) in enumerate(dataloader):\n",
    "            real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "            \n",
    "            # Train discriminators less frequently for balance\n",
    "            if i % 2 == 0:\n",
    "                # ============ Train Discriminator A ============\n",
    "                optimizer_D_A.zero_grad()\n",
    "                \n",
    "                fake_A = G_BA(real_B).detach()\n",
    "                \n",
    "                pred_real_A = D_A(real_A)\n",
    "                pred_fake_A = D_A(fake_A)\n",
    "                \n",
    "                loss_D_real_A = adversarial_loss_smooth(pred_real_A, True)\n",
    "                loss_D_fake_A = adversarial_loss_smooth(pred_fake_A, False)\n",
    "                \n",
    "                loss_D_A = (loss_D_real_A + loss_D_fake_A) * 0.5\n",
    "                loss_D_A.backward()\n",
    "                optimizer_D_A.step()\n",
    "                \n",
    "                # ============ Train Discriminator B ============\n",
    "                optimizer_D_B.zero_grad()\n",
    "                \n",
    "                fake_B = G_AB(real_A).detach()\n",
    "                \n",
    "                pred_real_B = D_B(real_B)\n",
    "                pred_fake_B = D_B(fake_B)\n",
    "                \n",
    "                loss_D_real_B = adversarial_loss_smooth(pred_real_B, True)\n",
    "                loss_D_fake_B = adversarial_loss_smooth(pred_fake_B, False)\n",
    "                \n",
    "                loss_D_B = (loss_D_real_B + loss_D_fake_B) * 0.5\n",
    "                loss_D_B.backward()\n",
    "                optimizer_D_B.step()\n",
    "            \n",
    "            # ============ Train Generators ============\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Identity loss (reduced weight for time series)\n",
    "            identity_B = G_AB(real_B)\n",
    "            identity_A = G_BA(real_A)\n",
    "            loss_identity = (identity_loss(identity_B, real_B) + \n",
    "                           identity_loss(identity_A, real_A)) * 1.0\n",
    "            \n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            fake_A = G_BA(real_B)\n",
    "            \n",
    "            pred_fake_B = D_B(fake_B)\n",
    "            pred_fake_A = D_A(fake_A)\n",
    "            \n",
    "            loss_GAN_AB = adversarial_loss_smooth(pred_fake_B, True)\n",
    "            loss_GAN_BA = adversarial_loss_smooth(pred_fake_A, True)\n",
    "            \n",
    "            # Cycle consistency loss (higher weight for time series)\n",
    "            recovered_A = G_BA(fake_B)\n",
    "            recovered_B = G_AB(fake_A)\n",
    "            loss_cycle = (cycle_loss(recovered_A, real_A) + \n",
    "                         cycle_loss(recovered_B, real_B)) * 15.0\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss_G = loss_GAN_AB + loss_GAN_BA + loss_cycle + loss_identity\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            epoch_G_loss += loss_G.item()\n",
    "            epoch_D_A_loss += loss_D_A.item() if i % 2 == 0 else 0\n",
    "            epoch_D_B_loss += loss_D_B.item() if i % 2 == 0 else 0\n",
    "            epoch_cycle_loss += loss_cycle.item()\n",
    "            epoch_identity_loss += loss_identity.item()\n",
    "        \n",
    "        # Average losses\n",
    "        num_batches = len(dataloader)\n",
    "        epoch_G_loss /= num_batches\n",
    "        epoch_D_A_loss /= (num_batches // 2)\n",
    "        epoch_D_B_loss /= (num_batches // 2)\n",
    "        epoch_cycle_loss /= num_batches\n",
    "        epoch_identity_loss /= num_batches\n",
    "        \n",
    "        # Store history\n",
    "        history['G_loss'].append(epoch_G_loss)\n",
    "        history['D_A_loss'].append(epoch_D_A_loss)\n",
    "        history['D_B_loss'].append(epoch_D_B_loss)\n",
    "        history['cycle_loss'].append(epoch_cycle_loss)\n",
    "        history['identity_loss'].append(epoch_identity_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "                  f\"G: {epoch_G_loss:.4f}, D_A: {epoch_D_A_loss:.4f}, D_B: {epoch_D_B_loss:.4f}, \"\n",
    "                  f\"Cycle: {epoch_cycle_loss:.4f}, Identity: {epoch_identity_loss:.4f}\")\n",
    "\n",
    "    # Store normalization parameters\n",
    "    data_stats = (data_mean, data_std)\n",
    "    return G_AB, G_BA, history, data_stats\n",
    "\n",
    "# Generate synthetic data using CycleGAN\n",
    "def generate_synthetic_data(generator, original_data, data_stats, device, num_samples=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data using CycleGAN domain translation\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        num_samples = len(original_data)\n",
    "    \n",
    "    generator.eval()\n",
    "    data_mean, data_std = data_stats\n",
    "    \n",
    "    # Normalize input data\n",
    "    normalized_input = (original_data - data_mean) / (3 * data_std)\n",
    "    normalized_input = np.clip(normalized_input, -1, 1)\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor format (batch, channels, seq_len)\n",
    "        tensor_data = torch.tensor(normalized_input[:num_samples], dtype=torch.float32).permute(0, 2, 1)\n",
    "        \n",
    "        # Generate in batches for memory efficiency\n",
    "        batch_size = 16\n",
    "        for i in range(0, len(tensor_data), batch_size):\n",
    "            batch = tensor_data[i:i+batch_size].to(device)\n",
    "            synthetic_batch = generator(batch)\n",
    "            \n",
    "            # Denormalize back to original scale\n",
    "            synthetic_batch = synthetic_batch.cpu().permute(0, 2, 1).numpy()\n",
    "            synthetic_batch = synthetic_batch * (3 * data_std) + data_mean\n",
    "            \n",
    "            synthetic_samples.append(synthetic_batch)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        synthetic_data = np.concatenate(synthetic_samples, axis=0)\n",
    "    \n",
    "    return synthetic_data[:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0267ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:28:32.364190Z",
     "iopub.status.busy": "2025-07-13T07:28:32.363841Z",
     "iopub.status.idle": "2025-07-13T07:33:19.398840Z",
     "shell.execute_reply": "2025-07-13T07:33:19.397980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CycleGAN training for time series data...\n",
      "Training CycleGAN on data shape: (552, 4500, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized training for time series...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] - G: 10.5956, D_A: 0.8873, D_B: 0.7309, Cycle: 8.0927, Identity: 0.5895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/200] - G: 5.4318, D_A: 0.0985, D_B: 0.0819, Cycle: 4.3086, Identity: 0.3172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/200] - G: 2.6424, D_A: 0.1492, D_B: 0.1763, Cycle: 2.0412, Identity: 0.1543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/200] - G: 2.1883, D_A: 0.1422, D_B: 0.1322, Cycle: 1.5541, Identity: 0.1139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/200] - G: 1.8018, D_A: 0.2472, D_B: 0.2175, Cycle: 1.2781, Identity: 0.0888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/200] - G: 1.5795, D_A: 0.1617, D_B: 0.1561, Cycle: 1.1155, Identity: 0.0780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/200] - G: 1.5493, D_A: 0.1566, D_B: 0.1562, Cycle: 1.0799, Identity: 0.0765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/200] - G: 1.5152, D_A: 0.1409, D_B: 0.1511, Cycle: 1.0302, Identity: 0.0760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/200] - G: 1.5640, D_A: 0.1521, D_B: 0.1835, Cycle: 0.9846, Identity: 0.0866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/200] - G: 1.5882, D_A: 0.1290, D_B: 0.1049, Cycle: 1.0103, Identity: 0.0962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/200] - G: 2.0645, D_A: 0.1010, D_B: 0.1166, Cycle: 1.2449, Identity: 0.0908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/200] - G: 1.6474, D_A: 0.1306, D_B: 0.1239, Cycle: 1.0471, Identity: 0.0834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [121/200] - G: 1.6695, D_A: 0.1177, D_B: 0.1172, Cycle: 0.9886, Identity: 0.0939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [131/200] - G: 2.4364, D_A: 0.0534, D_B: 0.0891, Cycle: 1.4618, Identity: 0.1139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/200] - G: 1.7049, D_A: 0.0990, D_B: 0.1228, Cycle: 0.9268, Identity: 0.0988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/200] - G: 1.5953, D_A: 0.1050, D_B: 0.1050, Cycle: 0.9144, Identity: 0.0952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [161/200] - G: 1.5963, D_A: 0.1228, D_B: 0.1094, Cycle: 0.9031, Identity: 0.0978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/200] - G: 1.6935, D_A: 0.1131, D_B: 0.1232, Cycle: 0.8829, Identity: 0.0933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/200] - G: 1.5876, D_A: 0.1269, D_B: 0.1292, Cycle: 0.8725, Identity: 0.1023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/200] - G: 1.6110, D_A: 0.1072, D_B: 0.0902, Cycle: 0.8604, Identity: 0.0839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/200] - G: 1.5181, D_A: 0.1194, D_B: 0.1200, Cycle: 0.8336, Identity: 0.0901\n",
      "Generating synthetic data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (690, 4500, 14)\n",
      "Synthetic data shape: (552, 4500, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "print(\"Starting CycleGAN training for time series data...\")\n",
    "G_AB, G_BA, history, data_stats = train_cyclegan_timeseries_stable(\n",
    "    X_train_normal, \n",
    "    device, \n",
    "    epochs=200,  # Reduced for testing\n",
    "    batch_size=32,  # Smaller batch size for your data\n",
    "    lr=0.005  # Slightly lower learning rate\n",
    ")\n",
    "\n",
    "# Generate synthetic data using the returned data_stats\n",
    "print(\"Generating synthetic data...\")\n",
    "synthetic_data = generate_synthetic_data(G_AB, X_train_normal, data_stats, device, num_samples=len(X_train_normal))\n",
    "\n",
    "print(f\"Original data shape: {normal_data.shape}\")\n",
    "print(f\"Synthetic data shape: {synthetic_data.shape}\")\n",
    "\n",
    "normal_combine = np.concatenate((X_train_normal, synthetic_data), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d488746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:33:19.402669Z",
     "iopub.status.busy": "2025-07-13T07:33:19.402287Z",
     "iopub.status.idle": "2025-07-13T07:33:19.407465Z",
     "shell.execute_reply": "2025-07-13T07:33:19.406700Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# # Test the simplified FID calculation\n",
    "# print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# # Use smaller subsets for testing\n",
    "# test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "# test_generated = synthetic_data[:100]\n",
    "\n",
    "# print(f\"Test real data shape: {test_real.shape}\")\n",
    "# print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# # Calculate FID score\n",
    "# fid_score = calculate_fid_score(\n",
    "#     real_data=test_real,\n",
    "#     fake_data=test_generated,\n",
    "#     device=device,\n",
    "#     sample_rate=1000,\n",
    "# )\n",
    "\n",
    "# if fid_score is not None:\n",
    "#     print(f\"\\nðŸŽ‰ SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "#     # Interpret the score\n",
    "#     if fid_score < 10:\n",
    "#         quality = \"Excellent\"\n",
    "#     elif fid_score < 25:\n",
    "#         quality = \"Good\"\n",
    "#     elif fid_score < 50:\n",
    "#         quality = \"Fair\"\n",
    "#     elif fid_score < 100:\n",
    "#         quality = \"Poor\"\n",
    "#     else:\n",
    "#         quality = \"Very Poor\"\n",
    "    \n",
    "#     print(f\"Quality Assessment: {quality}\")\n",
    "# else:\n",
    "#     print(\"âŒ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd8c7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:33:19.410818Z",
     "iopub.status.busy": "2025-07-13T07:33:19.410481Z",
     "iopub.status.idle": "2025-07-13T08:12:11.154719Z",
     "shell.execute_reply": "2025-07-13T08:12:11.153717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maddie/SmartIoT-2025/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/maddie/SmartIoT-2025/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-fold cross-validation...\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.258495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.086342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.043111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.042166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.041943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8857, Prec=0.6667, Rec=0.8571, F1=0.7500\n",
      "Optimal threshold: 0.039176\n",
      "\n",
      "Fold 2/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.258779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.101265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.043626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.042203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.042142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8571, Prec=1.0000, Rec=0.2857, F1=0.4444\n",
      "Optimal threshold: 0.042410\n",
      "\n",
      "Fold 3/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.258678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.088107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.043153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.037091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.025547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9429, Prec=1.0000, Rec=0.7143, F1=0.8333\n",
      "Optimal threshold: 0.026399\n",
      "\n",
      "Fold 4/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.258031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.097259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.043422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.042003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.039186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9143, Prec=1.0000, Rec=0.6250, F1=0.7692\n",
      "Optimal threshold: 0.037111\n",
      "\n",
      "Fold 5/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.258581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.107501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.043860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.042188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.042008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8571, Prec=0.8000, Rec=0.5000, F1=0.6154\n",
      "Optimal threshold: 0.040771\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "FOLD-BY-FOLD RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Fold   Accuracy   Precision   Recall   F1-Score  Threshold   \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.8857     0.6667      0.8571   0.7500    0.039176    \n",
      "2      0.8571     1.0000      0.2857   0.4444    0.042410    \n",
      "3      0.9429     1.0000      0.7143   0.8333    0.026399    \n",
      "4      0.9143     1.0000      0.6250   0.7692    0.037111    \n",
      "5      0.8571     0.8000      0.5000   0.6154    0.040771    \n",
      "\n",
      "STATISTICAL SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "Metric       Mean     Std      Min      Max      Median  \n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy     0.8914   0.0333   0.8571   0.9429   0.8857  \n",
      "Precision    0.8933   0.1373   0.6667   1.0000   1.0000  \n",
      "Recall       0.5964   0.1942   0.2857   0.8571   0.6250  \n",
      "F1           0.6825   0.1386   0.4444   0.8333   0.7500  \n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "  Mean F1-Score: 0.6825 Â± 0.1386\n",
      "  F1-Score Range: [0.4444, 0.8333]\n",
      "  Mean Threshold: 0.037174 Â± 0.005665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fold_results': [{'fold': 1,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.041943001374602315,\n",
       "   'optimal_threshold': 0.03917638167287364,\n",
       "   'accuracy': 0.8857142857142857,\n",
       "   'precision': 0.6666666666666666,\n",
       "   'recall': 0.8571428571428571,\n",
       "   'f1': 0.75},\n",
       "  {'fold': 2,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.042141678370535375,\n",
       "   'optimal_threshold': 0.04241039603948593,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.2857142857142857,\n",
       "   'f1': 0.4444444444444445},\n",
       "  {'fold': 3,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.025546976923942567,\n",
       "   'optimal_threshold': 0.02639894601371553,\n",
       "   'accuracy': 0.9428571428571428,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.7142857142857143,\n",
       "   'f1': 0.8333333333333333},\n",
       "  {'fold': 4,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.03918638844043017,\n",
       "   'optimal_threshold': 0.03711147591321155,\n",
       "   'accuracy': 0.9142857142857143,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.625,\n",
       "   'f1': 0.7692307692307693},\n",
       "  {'fold': 5,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.04200824163854122,\n",
       "   'optimal_threshold': 0.040771120143207634,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'precision': 0.8,\n",
       "   'recall': 0.5,\n",
       "   'f1': 0.6153846153846154}],\n",
       " 'statistics': {'accuracy': {'mean': 0.8914285714285715,\n",
       "   'std': 0.03331972511340173,\n",
       "   'min': 0.8571428571428571,\n",
       "   'max': 0.9428571428571428,\n",
       "   'median': 0.8857142857142857,\n",
       "   'values': array([0.88571429, 0.85714286, 0.94285714, 0.91428571, 0.85714286])},\n",
       "  'precision': {'mean': 0.8933333333333333,\n",
       "   'std': 0.13727506854649335,\n",
       "   'min': 0.6666666666666666,\n",
       "   'max': 1.0,\n",
       "   'median': 1.0,\n",
       "   'values': array([0.66666667, 1.        , 1.        , 1.        , 0.8       ])},\n",
       "  'recall': {'mean': 0.5964285714285714,\n",
       "   'std': 0.19417538884383123,\n",
       "   'min': 0.2857142857142857,\n",
       "   'max': 0.8571428571428571,\n",
       "   'median': 0.625,\n",
       "   'values': array([0.85714286, 0.28571429, 0.71428571, 0.625     , 0.5       ])},\n",
       "  'f1': {'mean': 0.6824786324786325,\n",
       "   'std': 0.13856701690942597,\n",
       "   'min': 0.4444444444444445,\n",
       "   'max': 0.8333333333333333,\n",
       "   'median': 0.75,\n",
       "   'values': array([0.75      , 0.44444444, 0.83333333, 0.76923077, 0.61538462])},\n",
       "  'optimal_threshold': {'mean': 0.03717366395649886,\n",
       "   'std': 0.005665233380453477,\n",
       "   'min': 0.02639894601371553,\n",
       "   'max': 0.04241039603948593,\n",
       "   'median': 0.03917638167287364,\n",
       "   'values': array([0.03917638, 0.0424104 , 0.02639895, 0.03711148, 0.04077112])},\n",
       "  'train_loss': {'mean': 0.03816525734961033,\n",
       "   'std': 0.006404927976244791,\n",
       "   'min': 0.025546976923942567,\n",
       "   'max': 0.042141678370535375,\n",
       "   'median': 0.041943001374602315,\n",
       "   'values': array([0.041943  , 0.04214168, 0.02554698, 0.03918639, 0.04200824])}},\n",
       " 'all_metrics': {'accuracy': [0.8857142857142857,\n",
       "   0.8571428571428571,\n",
       "   0.9428571428571428,\n",
       "   0.9142857142857143,\n",
       "   0.8571428571428571],\n",
       "  'precision': [0.6666666666666666, 1.0, 1.0, 1.0, 0.8],\n",
       "  'recall': [0.8571428571428571,\n",
       "   0.2857142857142857,\n",
       "   0.7142857142857143,\n",
       "   0.625,\n",
       "   0.5],\n",
       "  'f1': [0.75,\n",
       "   0.4444444444444445,\n",
       "   0.8333333333333333,\n",
       "   0.7692307692307693,\n",
       "   0.6153846153846154],\n",
       "  'optimal_threshold': [0.03917638167287364,\n",
       "   0.04241039603948593,\n",
       "   0.02639894601371553,\n",
       "   0.03711147591321155,\n",
       "   0.040771120143207634],\n",
       "  'train_loss': [0.041943001374602315,\n",
       "   0.042141678370535375,\n",
       "   0.025546976923942567,\n",
       "   0.03918638844043017,\n",
       "   0.04200824163854122]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline_with_cv(normal_combine, X_test_normal, X_test_faulty, \n",
    "                    device=device, batch_size=64, num_epochs=20, cv_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
