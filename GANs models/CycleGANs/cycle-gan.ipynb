{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Residual Block for Time Series\n",
    "class TimeSeriesResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.norm1 = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.norm2 = nn.BatchNorm1d(channels)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.norm1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "# Enhanced Generator for Time Series\n",
    "class TimeSeriesGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=14, hidden_dim=128, n_residual_blocks=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_dim//2, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Downsampling layers\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim//2, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            TimeSeriesResidualBlock(hidden_dim*2) for _ in range(n_residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dim*2, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dim, hidden_dim//2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim//2, input_channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Enhanced Discriminator for Time Series\n",
    "class TimeSeriesDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=14, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Add spectral normalization to prevent discriminator from becoming too strong\n",
    "            spectral_norm(nn.Conv1d(input_channels, hidden_dim, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*2, hidden_dim*4, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm1d(hidden_dim*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*4, hidden_dim*8, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm1d(hidden_dim*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*8, 1, kernel_size=4, padding=1)),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# Enhanced CycleGAN training function\n",
    "def train_cyclegan_timeseries_stable(normal_data, device, epochs=200, batch_size=8, lr=2e-4):\n",
    "    \"\"\"\n",
    "    Enhanced CycleGAN training with improved stability\n",
    "    \"\"\"\n",
    "    print(f\"Training CycleGAN on data shape: {normal_data.shape}\")\n",
    "    \n",
    "    # Split data into two domains\n",
    "    mid_point = len(normal_data) // 2\n",
    "    domain_A = normal_data[:mid_point]\n",
    "    domain_B = normal_data[mid_point:]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    tensor_A = torch.tensor(domain_A, dtype=torch.float32).permute(0, 2, 1)\n",
    "    tensor_B = torch.tensor(domain_B, dtype=torch.float32).permute(0, 2, 1)\n",
    "    \n",
    "    dataset = TensorDataset(tensor_A, tensor_B)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Initialize networks\n",
    "    G_AB = TimeSeriesGenerator().to(device)\n",
    "    G_BA = TimeSeriesGenerator().to(device)\n",
    "    D_A = TimeSeriesDiscriminator().to(device)\n",
    "    D_B = TimeSeriesDiscriminator().to(device)\n",
    "    \n",
    "    # **IMPROVED OPTIMIZERS** - Different learning rates for G and D\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(G_AB.parameters()) + list(G_BA.parameters()),\n",
    "        lr=lr, betas=(0.5, 0.999)\n",
    "    )\n",
    "    # Slower learning rate for discriminators to prevent collapse\n",
    "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr/2, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr/2, betas=(0.5, 0.999))\n",
    "        \n",
    "    def adversarial_loss_smooth(pred, target_is_real):\n",
    "        if target_is_real:\n",
    "            # Use random labels between 0.8-1.0 for more robust training\n",
    "            target = torch.ones_like(pred) * (0.8 + 0.2 * torch.rand_like(pred))\n",
    "        else:\n",
    "            # Use random labels between 0.0-0.2\n",
    "            target = torch.zeros_like(pred) + 0.2 * torch.rand_like(pred)\n",
    "        return nn.MSELoss()(pred, target)\n",
    "    \n",
    "    # Add gradient penalty for discriminators\n",
    "    def gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "        batch_size = real_data.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1).to(device)\n",
    "        \n",
    "        interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolated.requires_grad_(True)\n",
    "        \n",
    "        pred = discriminator(interpolated)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=pred, inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(pred),\n",
    "            create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return penalty\n",
    "    \n",
    "    cycle_loss = nn.L1Loss()\n",
    "    identity_loss = nn.L1Loss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'G_loss': [], 'D_A_loss': [], 'D_B_loss': [],\n",
    "        'cycle_loss': [], 'identity_loss': []\n",
    "    }\n",
    "    \n",
    "    print(\"Starting stable training...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_G_loss = 0\n",
    "        epoch_D_A_loss = 0\n",
    "        epoch_D_B_loss = 0\n",
    "        epoch_cycle_loss = 0\n",
    "        epoch_identity_loss = 0\n",
    "        \n",
    "        for i, (real_A, real_B) in enumerate(dataloader):\n",
    "            real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "            \n",
    "            # **TRAIN DISCRIMINATORS MORE FREQUENTLY**\n",
    "            for _ in range(2):  # Train discriminators twice per generator update\n",
    "                \n",
    "                # ============ Train Discriminator A ============\n",
    "                optimizer_D_A.zero_grad()\n",
    "                \n",
    "                # Generate fake samples\n",
    "                fake_A = G_BA(real_B).detach()\n",
    "                \n",
    "                pred_real_A = D_A(real_A)\n",
    "                pred_fake_A = D_A(fake_A)\n",
    "                \n",
    "                loss_D_real_A = adversarial_loss_smooth(pred_real_A, True)\n",
    "                loss_D_fake_A = adversarial_loss_smooth(pred_fake_A, False)\n",
    "                \n",
    "                loss_D_A = (loss_D_real_A + loss_D_fake_A) * 0.5\n",
    "                loss_D_A.backward()\n",
    "                optimizer_D_A.step()\n",
    "                \n",
    "                # ============ Train Discriminator B ============\n",
    "                optimizer_D_B.zero_grad()\n",
    "                \n",
    "                fake_B = G_AB(real_A).detach()\n",
    "                \n",
    "                pred_real_B = D_B(real_B)\n",
    "                pred_fake_B = D_B(fake_B)\n",
    "                \n",
    "                loss_D_real_B = adversarial_loss_smooth(pred_real_B, True)\n",
    "                loss_D_fake_B = adversarial_loss_smooth(pred_fake_B, False)\n",
    "                \n",
    "                loss_D_B = (loss_D_real_B + loss_D_fake_B) * 0.5\n",
    "                loss_D_B.backward()\n",
    "                optimizer_D_B.step()\n",
    "            \n",
    "            # ============ Train Generators ============\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Identity loss (reduced weight)\n",
    "            identity_B = G_AB(real_B)\n",
    "            identity_A = G_BA(real_A)\n",
    "            loss_identity = (identity_loss(identity_B, real_B) + \n",
    "                           identity_loss(identity_A, real_A)) * 2.0  # Reduced from 5.0\n",
    "            \n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            fake_A = G_BA(real_B)\n",
    "            \n",
    "            pred_fake_B = D_B(fake_B)\n",
    "            pred_fake_A = D_A(fake_A)\n",
    "            \n",
    "            loss_GAN_AB = adversarial_loss_smooth(pred_fake_B, True)\n",
    "            loss_GAN_BA = adversarial_loss_smooth(pred_fake_A, True)\n",
    "            \n",
    "            # Cycle consistency loss\n",
    "            recovered_A = G_BA(fake_B)\n",
    "            recovered_B = G_AB(fake_A)\n",
    "            loss_cycle = (cycle_loss(recovered_A, real_A) + \n",
    "                         cycle_loss(recovered_B, real_B)) * 10.0\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss_G = loss_GAN_AB + loss_GAN_BA + loss_cycle + loss_identity\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            epoch_G_loss += loss_G.item()\n",
    "            epoch_D_A_loss += loss_D_A.item()\n",
    "            epoch_D_B_loss += loss_D_B.item()\n",
    "            epoch_cycle_loss += loss_cycle.item()\n",
    "            epoch_identity_loss += loss_identity.item()\n",
    "        \n",
    "        # Average losses\n",
    "        num_batches = len(dataloader)\n",
    "        epoch_G_loss /= num_batches\n",
    "        epoch_D_A_loss /= num_batches\n",
    "        epoch_D_B_loss /= num_batches\n",
    "        epoch_cycle_loss /= num_batches\n",
    "        epoch_identity_loss /= num_batches\n",
    "        \n",
    "        # Store history\n",
    "        history['G_loss'].append(epoch_G_loss)\n",
    "        history['D_A_loss'].append(epoch_D_A_loss)\n",
    "        history['D_B_loss'].append(epoch_D_B_loss)\n",
    "        history['cycle_loss'].append(epoch_cycle_loss)\n",
    "        history['identity_loss'].append(epoch_identity_loss)\n",
    "        \n",
    "        # Print progress and check for instability\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "                f\"G: {epoch_G_loss:.4f}, D_A: {epoch_D_A_loss:.4f}, D_B: {epoch_D_B_loss:.4f}, \"\n",
    "                f\"Cycle: {epoch_cycle_loss:.4f}, Identity: {epoch_identity_loss:.4f}\")\n",
    "            \n",
    "            # **STABILITY CHECK**\n",
    "        if epoch_D_A_loss < 0.01 and epoch_D_B_loss < 0.01:\n",
    "            print(\"Warning: Discriminator losses too low! Potential collapse detected.\")\n",
    "            # Optionally restart discriminators or adjust learning rates\n",
    "\n",
    "    return G_AB, G_BA, history\n",
    "\n",
    "# Generate synthetic data using CycleGAN\n",
    "def generate_synthetic_data(generator, original_data, normalizer, device, num_samples=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data using CycleGAN domain translation\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        num_samples = len(original_data)\n",
    "    \n",
    "    generator.eval()\n",
    "    synthetic_samples = []\n",
    "    \n",
    "    # Use original data directly (already normalized in preprocessing)\n",
    "    input_data = original_data[:num_samples]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor format (batch, channels, seq_len)\n",
    "        tensor_data = torch.tensor(input_data, dtype=torch.float32).permute(0, 2, 1)\n",
    "        \n",
    "        # Generate in batches\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(tensor_data), batch_size):\n",
    "            batch = tensor_data[i:i+batch_size].to(device)\n",
    "            # Use CycleGAN generator to transform domain\n",
    "            synthetic_batch = generator(batch)\n",
    "            synthetic_samples.append(synthetic_batch.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        synthetic_tensor = torch.cat(synthetic_samples, dim=0)\n",
    "        \n",
    "        # Convert back to original format (batch, seq_len, channels)\n",
    "        synthetic_data = synthetic_tensor.permute(0, 2, 1).numpy()\n",
    "    \n",
    "    # Return requested number of samples\n",
    "    return synthetic_data[:num_samples]\n",
    "\n",
    "# Plotting function\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(history['G_loss'])\n",
    "    axes[0, 0].set_title('Generator Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    \n",
    "    axes[0, 1].plot(history['D_A_loss'], label='D_A')\n",
    "    axes[0, 1].plot(history['D_B_loss'], label='D_B')\n",
    "    axes[0, 1].set_title('Discriminator Losses')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    axes[1, 0].plot(history['cycle_loss'])\n",
    "    axes[1, 0].set_title('Cycle Consistency Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    \n",
    "    axes[1, 1].plot(history['identity_loss'])\n",
    "    axes[1, 1].set_title('Identity Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "print(\"Starting CycleGAN training for time series data...\")\n",
    "G_AB, G_BA, history = train_cyclegan_timeseries_stable(\n",
    "    X_train_normal, \n",
    "    device, \n",
    "    epochs=100,  # Reduced for testing\n",
    "    batch_size=32,  # Smaller batch size for your data\n",
    "    lr=0.005  # Slightly lower learning rate\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Create a simple normalizer for data generation\n",
    "normalizer = StandardScaler()\n",
    "normalizer.fit(normal_data.reshape(-1, normal_data.shape[-1]))\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"Generating synthetic data...\")\n",
    "synthetic_data = generate_synthetic_data(G_AB, normal_data, normalizer, device, num_samples=len(normal_data))\n",
    "\n",
    "print(f\"Original data shape: {normal_data.shape}\")\n",
    "print(f\"Synthetic data shape: {synthetic_data.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d488746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = synthetic_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"❌ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, synthetic_data, epochs=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
