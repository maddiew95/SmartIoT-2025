{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Residual Block for Time Series\n",
    "class TimeSeriesResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.norm1 = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
    "        self.norm2 = nn.BatchNorm1d(channels)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.norm1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "# Enhanced Generator for Time Series\n",
    "class TimeSeriesGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=14, hidden_dim=128, n_residual_blocks=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_dim//2, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Downsampling layers\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim//2, hidden_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            TimeSeriesResidualBlock(hidden_dim*2) for _ in range(n_residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dim*2, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dim, hidden_dim//2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim//2, input_channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Enhanced Discriminator for Time Series\n",
    "class TimeSeriesDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=14, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Add spectral normalization to prevent discriminator from becoming too strong\n",
    "            spectral_norm(nn.Conv1d(input_channels, hidden_dim, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm1d(hidden_dim*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*2, hidden_dim*4, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm1d(hidden_dim*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*4, hidden_dim*8, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm1d(hidden_dim*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv1d(hidden_dim*8, 1, kernel_size=4, padding=1)),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# Enhanced CycleGAN training function\n",
    "def train_cyclegan_timeseries_stable(normal_data, device, epochs=200, batch_size=8, lr=2e-4):\n",
    "    \"\"\"\n",
    "    Enhanced CycleGAN training with improved stability\n",
    "    \"\"\"\n",
    "    print(f\"Training CycleGAN on data shape: {normal_data.shape}\")\n",
    "    \n",
    "    # Split data into two domains\n",
    "    mid_point = len(normal_data) // 2\n",
    "    domain_A = normal_data[:mid_point]\n",
    "    domain_B = normal_data[mid_point:]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    tensor_A = torch.tensor(domain_A, dtype=torch.float32).permute(0, 2, 1)\n",
    "    tensor_B = torch.tensor(domain_B, dtype=torch.float32).permute(0, 2, 1)\n",
    "    \n",
    "    dataset = TensorDataset(tensor_A, tensor_B)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Initialize networks\n",
    "    G_AB = TimeSeriesGenerator().to(device)\n",
    "    G_BA = TimeSeriesGenerator().to(device)\n",
    "    D_A = TimeSeriesDiscriminator().to(device)\n",
    "    D_B = TimeSeriesDiscriminator().to(device)\n",
    "    \n",
    "    # **IMPROVED OPTIMIZERS** - Different learning rates for G and D\n",
    "    optimizer_G = optim.Adam(\n",
    "        list(G_AB.parameters()) + list(G_BA.parameters()),\n",
    "        lr=lr, betas=(0.5, 0.999)\n",
    "    )\n",
    "    # Slower learning rate for discriminators to prevent collapse\n",
    "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr/2, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr/2, betas=(0.5, 0.999))\n",
    "        \n",
    "    def adversarial_loss_smooth(pred, target_is_real):\n",
    "        if target_is_real:\n",
    "            # Use random labels between 0.8-1.0 for more robust training\n",
    "            target = torch.ones_like(pred) * (0.8 + 0.2 * torch.rand_like(pred))\n",
    "        else:\n",
    "            # Use random labels between 0.0-0.2\n",
    "            target = torch.zeros_like(pred) + 0.2 * torch.rand_like(pred)\n",
    "        return nn.MSELoss()(pred, target)\n",
    "    \n",
    "    # Add gradient penalty for discriminators\n",
    "    def gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "        batch_size = real_data.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1).to(device)\n",
    "        \n",
    "        interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolated.requires_grad_(True)\n",
    "        \n",
    "        pred = discriminator(interpolated)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=pred, inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(pred),\n",
    "            create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return penalty\n",
    "    \n",
    "    cycle_loss = nn.L1Loss()\n",
    "    identity_loss = nn.L1Loss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'G_loss': [], 'D_A_loss': [], 'D_B_loss': [],\n",
    "        'cycle_loss': [], 'identity_loss': []\n",
    "    }\n",
    "    \n",
    "    print(\"Starting stable training...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_G_loss = 0\n",
    "        epoch_D_A_loss = 0\n",
    "        epoch_D_B_loss = 0\n",
    "        epoch_cycle_loss = 0\n",
    "        epoch_identity_loss = 0\n",
    "        \n",
    "        for i, (real_A, real_B) in enumerate(dataloader):\n",
    "            real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "            \n",
    "            # **TRAIN DISCRIMINATORS MORE FREQUENTLY**\n",
    "            for _ in range(2):  # Train discriminators twice per generator update\n",
    "                \n",
    "                # ============ Train Discriminator A ============\n",
    "                optimizer_D_A.zero_grad()\n",
    "                \n",
    "                # Generate fake samples\n",
    "                fake_A = G_BA(real_B).detach()\n",
    "                \n",
    "                pred_real_A = D_A(real_A)\n",
    "                pred_fake_A = D_A(fake_A)\n",
    "                \n",
    "                loss_D_real_A = adversarial_loss_smooth(pred_real_A, True)\n",
    "                loss_D_fake_A = adversarial_loss_smooth(pred_fake_A, False)\n",
    "                \n",
    "                loss_D_A = (loss_D_real_A + loss_D_fake_A) * 0.5\n",
    "                loss_D_A.backward()\n",
    "                optimizer_D_A.step()\n",
    "                \n",
    "                # ============ Train Discriminator B ============\n",
    "                optimizer_D_B.zero_grad()\n",
    "                \n",
    "                fake_B = G_AB(real_A).detach()\n",
    "                \n",
    "                pred_real_B = D_B(real_B)\n",
    "                pred_fake_B = D_B(fake_B)\n",
    "                \n",
    "                loss_D_real_B = adversarial_loss_smooth(pred_real_B, True)\n",
    "                loss_D_fake_B = adversarial_loss_smooth(pred_fake_B, False)\n",
    "                \n",
    "                loss_D_B = (loss_D_real_B + loss_D_fake_B) * 0.5\n",
    "                loss_D_B.backward()\n",
    "                optimizer_D_B.step()\n",
    "            \n",
    "            # ============ Train Generators ============\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Identity loss (reduced weight)\n",
    "            identity_B = G_AB(real_B)\n",
    "            identity_A = G_BA(real_A)\n",
    "            loss_identity = (identity_loss(identity_B, real_B) + \n",
    "                           identity_loss(identity_A, real_A)) * 2.0  # Reduced from 5.0\n",
    "            \n",
    "            # GAN loss\n",
    "            fake_B = G_AB(real_A)\n",
    "            fake_A = G_BA(real_B)\n",
    "            \n",
    "            pred_fake_B = D_B(fake_B)\n",
    "            pred_fake_A = D_A(fake_A)\n",
    "            \n",
    "            loss_GAN_AB = adversarial_loss_smooth(pred_fake_B, True)\n",
    "            loss_GAN_BA = adversarial_loss_smooth(pred_fake_A, True)\n",
    "            \n",
    "            # Cycle consistency loss\n",
    "            recovered_A = G_BA(fake_B)\n",
    "            recovered_B = G_AB(fake_A)\n",
    "            loss_cycle = (cycle_loss(recovered_A, real_A) + \n",
    "                         cycle_loss(recovered_B, real_B)) * 10.0\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss_G = loss_GAN_AB + loss_GAN_BA + loss_cycle + loss_identity\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            epoch_G_loss += loss_G.item()\n",
    "            epoch_D_A_loss += loss_D_A.item()\n",
    "            epoch_D_B_loss += loss_D_B.item()\n",
    "            epoch_cycle_loss += loss_cycle.item()\n",
    "            epoch_identity_loss += loss_identity.item()\n",
    "        \n",
    "        # Average losses\n",
    "        num_batches = len(dataloader)\n",
    "        epoch_G_loss /= num_batches\n",
    "        epoch_D_A_loss /= num_batches\n",
    "        epoch_D_B_loss /= num_batches\n",
    "        epoch_cycle_loss /= num_batches\n",
    "        epoch_identity_loss /= num_batches\n",
    "        \n",
    "        # Store history\n",
    "        history['G_loss'].append(epoch_G_loss)\n",
    "        history['D_A_loss'].append(epoch_D_A_loss)\n",
    "        history['D_B_loss'].append(epoch_D_B_loss)\n",
    "        history['cycle_loss'].append(epoch_cycle_loss)\n",
    "        history['identity_loss'].append(epoch_identity_loss)\n",
    "        \n",
    "        # Print progress and check for instability\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "                f\"G: {epoch_G_loss:.4f}, D_A: {epoch_D_A_loss:.4f}, D_B: {epoch_D_B_loss:.4f}, \"\n",
    "                f\"Cycle: {epoch_cycle_loss:.4f}, Identity: {epoch_identity_loss:.4f}\")\n",
    "            \n",
    "            # **STABILITY CHECK**\n",
    "        if epoch_D_A_loss < 0.01 and epoch_D_B_loss < 0.01:\n",
    "            print(\"Warning: Discriminator losses too low! Potential collapse detected.\")\n",
    "            # Optionally restart discriminators or adjust learning rates\n",
    "\n",
    "    return G_AB, G_BA, normalizer, history\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(generator, original_data, normalizer, device, num_samples=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        num_samples = len(original_data)\n",
    "    \n",
    "    generator.eval()\n",
    "    synthetic_samples = []\n",
    "    \n",
    "    # Normalize original data\n",
    "    normalized_data = normalizer.transform(original_data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor format (batch, channels, seq_len)\n",
    "        tensor_data = torch.tensor(normalized_data, dtype=torch.float32).permute(0, 2, 1)\n",
    "        \n",
    "        # Generate in batches\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(tensor_data), batch_size):\n",
    "            batch = tensor_data[i:i+batch_size].to(device)\n",
    "            synthetic_batch = generator(batch)\n",
    "            synthetic_samples.append(synthetic_batch.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        synthetic_tensor = torch.cat(synthetic_samples, dim=0)\n",
    "        \n",
    "        # Convert back to original format (batch, seq_len, channels)\n",
    "        synthetic_normalized = synthetic_tensor.permute(0, 2, 1).numpy()\n",
    "        \n",
    "        # Denormalize\n",
    "        synthetic_data = normalizer.inverse_transform(synthetic_normalized)\n",
    "    \n",
    "    # Return requested number of samples\n",
    "    return synthetic_data[:num_samples]\n",
    "\n",
    "# Plotting function\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(history['G_loss'])\n",
    "    axes[0, 0].set_title('Generator Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    \n",
    "    axes[0, 1].plot(history['D_A_loss'], label='D_A')\n",
    "    axes[0, 1].plot(history['D_B_loss'], label='D_B')\n",
    "    axes[0, 1].set_title('Discriminator Losses')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    axes[1, 0].plot(history['cycle_loss'])\n",
    "    axes[1, 0].set_title('Cycle Consistency Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    \n",
    "    axes[1, 1].plot(history['identity_loss'])\n",
    "    axes[1, 1].set_title('Identity Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "print(\"Starting CycleGAN training for time series data...\")\n",
    "G_AB, G_BA, normalizer, history = train_cyclegan_timeseries_stable(\n",
    "    X_train, \n",
    "    device, \n",
    "    epochs=100,  # Reduced for testing\n",
    "    batch_size=32,  # Smaller batch size for your data\n",
    "    lr=0.005  # Slightly lower learning rate\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"Generating synthetic data...\")\n",
    "synthetic_data = generate_synthetic_data(G_AB, normal_data, normalizer, device, num_samples=len(normal_data))\n",
    "\n",
    "print(f\"Original data shape: {normal_data.shape}\")\n",
    "print(f\"Synthetic data shape: {synthetic_data.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901a15",
   "metadata": {},
   "source": [
    "# Processing: Mel Spec > Resizing > Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01061dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_spectrogram(spectrogram, global_min=None, global_max=None):\n",
    "    \"\"\"\n",
    "    Improved spectrogram processing with consistent normalization\n",
    "    \"\"\"\n",
    "    # Use global min/max for consistent normalization across all spectrograms\n",
    "    if global_min is not None and global_max is not None:\n",
    "        spectrogram = (spectrogram - global_min) / (global_max - global_min + 1e-8)\n",
    "    else:\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "    \n",
    "    # Clip to [0,1] and convert to uint8\n",
    "    spectrogram = np.clip(spectrogram, 0, 1)\n",
    "    spectrogram = np.uint8(spectrogram.cpu().numpy() * 255)\n",
    "    spectrogram = np.stack([spectrogram] * 3, axis=-1)\n",
    "    \n",
    "    image = Image.fromarray(spectrogram)\n",
    "    image = transforms.Resize((224, 224))(image)\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "def process_dataset_improved(data, sample_rate=1000):  # More reasonable sample rate\n",
    "    \"\"\"\n",
    "    Improved dataset processing with better mel-spectrogram parameters\n",
    "    \"\"\"\n",
    "    num_samples, seq_len, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, num_channels, 4096))\n",
    "    \n",
    "    # Better mel-spectrogram parameters for sensor data\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=128,\n",
    "        n_fft=512,          # Reasonable FFT size\n",
    "        hop_length=256,     # 50% overlap\n",
    "        win_length=512,\n",
    "        window_fn=torch.hann_window\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load VGG16 model\n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute global min/max for consistent normalization\n",
    "    print(\"Computing global spectrogram statistics...\")\n",
    "    all_mels = []\n",
    "    for i in range(min(100, num_samples)):  # Sample subset for statistics\n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            all_mels.append(mel.cpu().numpy())\n",
    "    \n",
    "    all_mels = np.concatenate([mel.flatten() for mel in all_mels])\n",
    "    global_min, global_max = np.percentile(all_mels, [1, 99])  # Use percentiles to avoid outliers\n",
    "    \n",
    "    print(f\"Processing {num_samples} samples...\")\n",
    "    for i in range(num_samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{num_samples} samples\")\n",
    "            \n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            \n",
    "            # Use consistent normalization\n",
    "            img = resize_spectrogram(mel, global_min, global_max)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                feat = model(img.unsqueeze(0).to(device))\n",
    "            features[i, j, :] = feat.squeeze().cpu().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Alternative: Multi-channel processing\n",
    "def process_dataset_multichannel(data, sample_rate=1000):\n",
    "    \"\"\"\n",
    "    Process multiple channels together to capture cross-channel relationships\n",
    "    \"\"\"\n",
    "    num_samples, seq_len, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, 4096))  # Single feature vector per sample\n",
    "    \n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=128,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        win_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Processing {num_samples} samples with multi-channel approach...\")\n",
    "    for i in range(num_samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{num_samples} samples\")\n",
    "        \n",
    "        # Combine multiple channels into RGB image\n",
    "        channel_spectrograms = []\n",
    "        for j in range(min(3, num_channels)):  # Use first 3 channels as RGB\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            \n",
    "            # Normalize each channel spectrogram\n",
    "            mel_norm = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)\n",
    "            mel_resized = torch.nn.functional.interpolate(\n",
    "                mel_norm.unsqueeze(0).unsqueeze(0), \n",
    "                size=(224, 224), \n",
    "                mode='bilinear'\n",
    "            ).squeeze()\n",
    "            channel_spectrograms.append(mel_resized.cpu().numpy())\n",
    "        \n",
    "        # Stack as RGB image\n",
    "        if len(channel_spectrograms) == 1:\n",
    "            rgb_img = np.stack([channel_spectrograms[0]] * 3, axis=0)\n",
    "        elif len(channel_spectrograms) == 2:\n",
    "            rgb_img = np.stack([channel_spectrograms[0], channel_spectrograms[1], channel_spectrograms[0]], axis=0)\n",
    "        else:\n",
    "            rgb_img = np.stack(channel_spectrograms[:3], axis=0)\n",
    "        \n",
    "        img_tensor = torch.tensor(rgb_img, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            feat = model(img_tensor)\n",
    "        features[i, :] = feat.squeeze().cpu().numpy()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f39ba8",
   "metadata": {},
   "source": [
    "# AE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 4), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 16), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "# Train autoencoder\n",
    "def train_autoencoder(features, epochs=20, batch_size=128):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # Add weight decay\n",
    "    criterion = nn.MSELoss()  # Try MSE instead of L1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            # Add noise for denoising autoencoder\n",
    "            noisy_inputs = inputs + 0.1 * torch.randn_like(inputs)\n",
    "            outputs = model(noisy_inputs)\n",
    "            loss = criterion(outputs, inputs)  # Reconstruct clean from noisy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "# Compute reconstruction errors\n",
    "def compute_reconstruction_loss(model, data, add_noise=True):\n",
    "    \"\"\"\n",
    "    Compute reconstruction loss per sample (not per segment)\n",
    "    data: shape (n_samples, n_channels, 4096)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_samples, n_channels, n_features = data.shape\n",
    "    sample_errors = []\n",
    "    \n",
    "    # Flatten to (n_samples*n_channels, 4096) for batch processing\n",
    "    x = torch.tensor(data.reshape(-1, n_features), dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=64)\n",
    "    \n",
    "    all_errors = []\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            \n",
    "            if add_noise:\n",
    "                noisy_inputs = inputs + 0.1 * torch.randn_like(inputs)\n",
    "                outputs = model(noisy_inputs)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            # Per-segment reconstruction error\n",
    "            segment_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            all_errors.extend(segment_errors.cpu().numpy())\n",
    "    \n",
    "    # Reshape back to (n_samples, n_channels) and aggregate per sample\n",
    "    all_errors = np.array(all_errors).reshape(n_samples, n_channels)\n",
    "    sample_errors = all_errors.mean(axis=1)  # Average across channels per sample\n",
    "    \n",
    "    return sample_errors\n",
    "\n",
    "# 2. Find best threshold based on F1 score\n",
    "def find_best_threshold(errors, labels):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def find_best_threshold_using_recall(errors, labels):\n",
    "    best_rec = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        rec = recall_score(labels, preds)\n",
    "        if rec > best_rec:\n",
    "            best_rec = rec\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_rec\n",
    "\n",
    "def find_best_threshold_using_precision(errors, labels):\n",
    "    best_prec = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        prec = precision_score(labels, preds)\n",
    "        if prec > best_prec:\n",
    "            best_prec = prec\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_prec\n",
    "\n",
    "\n",
    "def evaluate_on_test_with_threshold_search(model, threshold, X_test, y_test):\n",
    "    \"\"\"\n",
    "    X_test: shape (n_samples, 1, 4096) - already has channel dimension added\n",
    "    y_test: shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # X_test already has shape (n_samples, 1, 4096) from your code\n",
    "    # So we can directly compute reconstruction errors\n",
    "    test_errors = compute_reconstruction_loss(model, X_test)\n",
    "    \n",
    "    # Predict using best threshold\n",
    "    test_preds = (test_errors > threshold).astype(int)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    print(\"Accuracy =\", accuracy_score(y_test, test_preds))\n",
    "    print(\"Precision =\", precision_score(y_test, test_preds))\n",
    "    print(\"Recall =\", recall_score(y_test, test_preds))\n",
    "    print(\"F1 Score =\", f1_score(y_test, test_preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d65ef",
   "metadata": {},
   "source": [
    "# Comprehensive Anomaly Detection Evaluation Framework\n",
    "\n",
    "This section implements a comprehensive evaluation framework that compares multiple anomaly detection methods and provides statistical analysis of the results for CycleGAN-based IoT anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update imports for comprehensive evaluation\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Comprehensive Anomaly Detection Comparison for CycleGAN\n",
    "def comprehensive_cyclegan_evaluation(synthetic_data, normal_data, faulty_data, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with multiple anomaly detection methods for CycleGAN\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE ANOMALY DETECTION EVALUATION - CYCLEGAN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Prepare data\n",
    "    all_normal = np.concatenate([synthetic_data, normal_data], axis=0)\n",
    "    all_data = np.concatenate([all_normal, faulty_data], axis=0)\n",
    "    all_labels = np.concatenate([\n",
    "        np.zeros(len(all_normal)), \n",
    "        np.ones(len(faulty_data))\n",
    "    ])\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Storage for results\n",
    "    methods_results = {\n",
    "        'Reconstruction_Threshold': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\n",
    "        'Reconstruction_Percentile': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\n",
    "        'Cycle_Consistency': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\n",
    "        'OneClass_SVM': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\n",
    "        'Isolation_Forest': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\n",
    "        'Local_Outlier_Factor': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    }\n",
    "    \n",
    "    fold_details = []\n",
    "    \n",
    "    # Initialize CycleGAN generators (assuming they're available from training)\n",
    "    G_AB = TimeSeriesGenerator().to(device)\n",
    "    G_BA = TimeSeriesGenerator().to(device)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(all_data, all_labels)):\n",
    "        print(f\"\\n--- FOLD {fold + 1}/{cv_folds} ---\")\n",
    "        \n",
    "        # Split data\n",
    "        train_data, test_data = all_data[train_idx], all_data[test_idx]\n",
    "        train_labels, test_labels = all_labels[train_idx], all_labels[test_idx]\n",
    "        \n",
    "        # Get normal training data only\n",
    "        normal_train_data = train_data[train_labels == 0]\n",
    "        \n",
    "        # Process data for feature extraction\n",
    "        processed_normal = process_dataset_multichannel(normal_train_data)\n",
    "        processed_test = process_dataset_multichannel(test_data)\n",
    "        \n",
    "        # Train reconstruction-based model\n",
    "        print(\"Training autoencoder...\")\n",
    "        model = train_autoencoder(processed_normal, epochs=15, batch_size=32)\n",
    "        \n",
    "        # Compute reconstruction errors for test data\n",
    "        test_errors = compute_reconstruction_loss(model, processed_test[:, np.newaxis, :])\n",
    "        \n",
    "        # Method 1: Threshold-based (F1-optimized)\n",
    "        normal_errors = compute_reconstruction_loss(model, processed_normal[:, np.newaxis, :])\n",
    "        threshold, _ = find_best_threshold(\n",
    "            np.concatenate([normal_errors, test_errors[test_labels == 1]]),\n",
    "            np.concatenate([np.zeros(len(normal_errors)), np.ones(np.sum(test_labels == 1))])\n",
    "        )\n",
    "        preds_threshold = (test_errors > threshold).astype(int)\n",
    "        \n",
    "        # Method 2: Percentile-based\n",
    "        percentile_threshold = np.percentile(normal_errors, 95)\n",
    "        preds_percentile = (test_errors > percentile_threshold).astype(int)\n",
    "        \n",
    "        # Method 3: Cycle Consistency Error (CycleGAN-specific)\n",
    "        cycle_errors = compute_cycle_consistency_errors(G_AB, G_BA, test_data)\n",
    "        cycle_threshold = np.percentile(\n",
    "            compute_cycle_consistency_errors(G_AB, G_BA, normal_train_data), 95\n",
    "        )\n",
    "        preds_cycle = (cycle_errors > cycle_threshold).astype(int)\n",
    "        \n",
    "        # Method 4: One-Class SVM\n",
    "        oc_svm = OneClassSVM(gamma='scale', nu=0.1)\n",
    "        oc_svm.fit(processed_normal)\n",
    "        preds_svm = (oc_svm.predict(processed_test) == -1).astype(int)\n",
    "        \n",
    "        # Method 5: Isolation Forest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        iso_forest.fit(processed_normal)\n",
    "        preds_iso = (iso_forest.predict(processed_test) == -1).astype(int)\n",
    "        \n",
    "        # Method 6: Local Outlier Factor\n",
    "        lof = LocalOutlierFactor(contamination=0.1, novelty=True)\n",
    "        lof.fit(processed_normal)\n",
    "        preds_lof = (lof.predict(processed_test) == -1).astype(int)\n",
    "        \n",
    "        # Evaluate all methods\n",
    "        methods_preds = {\n",
    "            'Reconstruction_Threshold': preds_threshold,\n",
    "            'Reconstruction_Percentile': preds_percentile,\n",
    "            'Cycle_Consistency': preds_cycle,\n",
    "            'OneClass_SVM': preds_svm,\n",
    "            'Isolation_Forest': preds_iso,\n",
    "            'Local_Outlier_Factor': preds_lof\n",
    "        }\n",
    "        \n",
    "        fold_result = {'fold': fold + 1}\n",
    "        \n",
    "        for method_name, preds in methods_preds.items():\n",
    "            acc = accuracy_score(test_labels, preds)\n",
    "            prec = precision_score(test_labels, preds, zero_division=0)\n",
    "            rec = recall_score(test_labels, preds, zero_division=0)\n",
    "            f1 = f1_score(test_labels, preds, zero_division=0)\n",
    "            \n",
    "            methods_results[method_name]['accuracy'].append(acc)\n",
    "            methods_results[method_name]['precision'].append(prec)\n",
    "            methods_results[method_name]['recall'].append(rec)\n",
    "            methods_results[method_name]['f1'].append(f1)\n",
    "            \n",
    "            fold_result[method_name] = {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1}\n",
    "            \n",
    "            print(f\"{method_name:25s} - Acc: {acc:.3f}, Prec: {prec:.3f}, Rec: {rec:.3f}, F1: {f1:.3f}\")\n",
    "        \n",
    "        fold_details.append(fold_result)\n",
    "    \n",
    "    return methods_results, fold_details\n",
    "\n",
    "def compute_cycle_consistency_errors(G_AB, G_BA, data):\n",
    "    \"\"\"\n",
    "    Compute cycle consistency errors for CycleGAN\n",
    "    \"\"\"\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    # Convert data to tensor\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "    \n",
    "    cycle_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_tensor), 32):  # Process in batches\n",
    "            batch = data_tensor[i:i+32]\n",
    "            \n",
    "            # Forward cycle: A -> B -> A\n",
    "            fake_B = G_AB(batch)\n",
    "            reconstructed_A = G_BA(fake_B)\n",
    "            \n",
    "            # Compute cycle consistency loss\n",
    "            cycle_loss = nn.L1Loss(reduction='none')(batch, reconstructed_A)\n",
    "            cycle_loss = cycle_loss.mean(dim=[1, 2])  # Average over channels and time\n",
    "            \n",
    "            cycle_errors.extend(cycle_loss.cpu().numpy())\n",
    "    \n",
    "    return np.array(cycle_errors)\n",
    "\n",
    "# Statistical Analysis Function (same as before)\n",
    "def statistical_analysis_cyclegan(methods_results):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis on cross-validation results for CycleGAN\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL ANALYSIS - CYCLEGAN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    results_df = []\n",
    "    \n",
    "    for method_name, results in methods_results.items():\n",
    "        for metric in metrics:\n",
    "            values = results[metric]\n",
    "            results_df.append({\n",
    "                'Method': method_name,\n",
    "                'Metric': metric,\n",
    "                'Mean': np.mean(values),\n",
    "                'Std': np.std(values),\n",
    "                'Min': np.min(values),\n",
    "                'Max': np.max(values),\n",
    "                'Median': np.median(values)\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_df)\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\nSUMMARY STATISTICS:\")\n",
    "    pivot_table = results_df.pivot_table(\n",
    "        index='Method', \n",
    "        columns='Metric', \n",
    "        values=['Mean', 'Std'], \n",
    "        aggfunc='first'\n",
    "    )\n",
    "    print(pivot_table.round(4))\n",
    "    \n",
    "    # Statistical significance tests\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTS (F1-Score)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    f1_scores = {method: results['f1'] for method, results in methods_results.items()}\n",
    "    \n",
    "    # Perform pairwise t-tests\n",
    "    from scipy.stats import ttest_rel, friedmanchisquare\n",
    "    \n",
    "    # Friedman test for overall difference\n",
    "    f1_values = [scores for scores in f1_scores.values()]\n",
    "    friedman_stat, friedman_p = friedmanchisquare(*f1_values)\n",
    "    print(f\"Friedman Test: χ² = {friedman_stat:.4f}, p-value = {friedman_p:.4f}\")\n",
    "    \n",
    "    if friedman_p < 0.05:\n",
    "        print(\"Significant differences detected between methods.\")\n",
    "        \n",
    "        # Pairwise comparisons\n",
    "        method_names = list(f1_scores.keys())\n",
    "        print(\"\\nPairwise t-test results (F1-Score):\")\n",
    "        for i in range(len(method_names)):\n",
    "            for j in range(i+1, len(method_names)):\n",
    "                stat, p_val = ttest_rel(f1_scores[method_names[i]], f1_scores[method_names[j]])\n",
    "                significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "                print(f\"{method_names[i]:25s} vs {method_names[j]:25s}: t={stat:6.3f}, p={p_val:.4f} {significance}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Visualization Functions for CycleGAN\n",
    "def create_cyclegan_visualizations(methods_results, fold_details, synthetic_data, G_AB, G_BA):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for CycleGAN\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GENERATING CYCLEGAN VISUALIZATIONS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Performance Comparison Box Plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('CycleGAN: Anomaly Detection Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "        ax = axes[idx//2, idx%2]\n",
    "        \n",
    "        data_to_plot = [methods_results[method][metric] for method in methods_results.keys()]\n",
    "        labels = [method.replace('_', ' ') for method in methods_results.keys()]\n",
    "        \n",
    "        bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "        ax.set_title(f'{name} Distribution Across Folds', fontweight='bold')\n",
    "        ax.set_ylabel(name)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Color the boxes\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink', 'lightgray']\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. CycleGAN-specific: Domain Translation Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Sample some data for visualization\n",
    "    sample_data = synthetic_data[:3]  # First 3 samples\n",
    "    data_tensor = torch.tensor(sample_data, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "    \n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_B = G_AB(data_tensor)\n",
    "        reconstructed_A = G_BA(fake_B)\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    fake_B_np = fake_B.permute(0, 2, 1).cpu().numpy()\n",
    "    reconstructed_A_np = reconstructed_A.permute(0, 2, 1).cpu().numpy()\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Original A\n",
    "        plt.subplot(3, 4, i*4 + 1)\n",
    "        plt.plot(sample_data[i, :, 0], label='Channel 1', alpha=0.7)\n",
    "        plt.plot(sample_data[i, :, 1], label='Channel 2', alpha=0.7)\n",
    "        plt.title(f'Original A (Sample {i+1})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Translated B\n",
    "        plt.subplot(3, 4, i*4 + 2)\n",
    "        plt.plot(fake_B_np[i, :, 0], label='Channel 1', alpha=0.7)\n",
    "        plt.plot(fake_B_np[i, :, 1], label='Channel 2', alpha=0.7)\n",
    "        plt.title(f'Translated B (Sample {i+1})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reconstructed A\n",
    "        plt.subplot(3, 4, i*4 + 3)\n",
    "        plt.plot(reconstructed_A_np[i, :, 0], label='Channel 1', alpha=0.7)\n",
    "        plt.plot(reconstructed_A_np[i, :, 1], label='Channel 2', alpha=0.7)\n",
    "        plt.title(f'Reconstructed A (Sample {i+1})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reconstruction Error\n",
    "        plt.subplot(3, 4, i*4 + 4)\n",
    "        error = np.abs(sample_data[i] - reconstructed_A_np[i])\n",
    "        plt.plot(error[:, 0], label='Channel 1 Error', alpha=0.7)\n",
    "        plt.plot(error[:, 1], label='Channel 2 Error', alpha=0.7)\n",
    "        plt.title(f'Reconstruction Error (Sample {i+1})')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('CycleGAN: Domain Translation and Reconstruction', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Method Ranking Heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create ranking matrix\n",
    "    ranking_data = []\n",
    "    for method in methods_results.keys():\n",
    "        method_means = [np.mean(methods_results[method][metric]) for metric in metrics]\n",
    "        ranking_data.append(method_means)\n",
    "    \n",
    "    ranking_df = pd.DataFrame(\n",
    "        ranking_data, \n",
    "        index=[method.replace('_', ' ') for method in methods_results.keys()],\n",
    "        columns=metric_names\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(ranking_df, annot=True, cmap='RdYlGn', fmt='.3f', center=0.5,\n",
    "                cbar_kws={'label': 'Performance Score'})\n",
    "    plt.title('CycleGAN: Method Performance Heatmap\\n(Higher values = Better performance)', \n",
    "              fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Enhanced CycleGAN Training Function with Monitoring\n",
    "def train_cyclegan_with_monitoring(normal_data, epochs=200, batch_size=8):\n",
    "    \"\"\"\n",
    "    Train CycleGAN with comprehensive monitoring\n",
    "    \"\"\"\n",
    "    print(\"Training CycleGAN with monitoring...\")\n",
    "    \n",
    "    # Initialize and train CycleGAN\n",
    "    synthetic_data = train_cyclegan_timeseries_stable(normal_data, device, epochs, batch_size)\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "# Execute comprehensive CycleGAN evaluation\n",
    "print(\"Starting comprehensive CycleGAN evaluation...\")\n",
    "\n",
    "# Assuming synthetic_data is generated from CycleGAN training\n",
    "# If not available, we'll create a placeholder for the evaluation structure\n",
    "if 'synthetic_data' not in locals():\n",
    "    print(\"Training CycleGAN to generate synthetic data...\")\n",
    "    synthetic_data = train_cyclegan_with_monitoring(normal_data, epochs=50, batch_size=8)\n",
    "\n",
    "print(f\"Synthetic data shape: {synthetic_data.shape}\")\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "methods_results, fold_details = comprehensive_cyclegan_evaluation(\n",
    "    synthetic_data, normal_data, faulty_data, cv_folds=5\n",
    ")\n",
    "\n",
    "# Perform statistical analysis\n",
    "results_df = statistical_analysis_cyclegan(methods_results)\n",
    "\n",
    "# Create visualizations (assuming generators are available)\n",
    "if 'G_AB' in locals() and 'G_BA' in locals():\n",
    "    create_cyclegan_visualizations(methods_results, fold_details, synthetic_data, G_AB, G_BA)\n",
    "else:\n",
    "    print(\"Generator models not available for visualization. Skipping domain translation plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0bf6f",
   "metadata": {},
   "source": [
    "# Summary and Recommendations\n",
    "\n",
    "## CycleGAN for IoT Anomaly Detection - Key Findings\n",
    "\n",
    "### Model Architecture Enhancements:\n",
    "1. **Enhanced Time Series Generator:**\n",
    "   - Residual blocks with batch normalization for stable training\n",
    "   - Progressive down/up-sampling for multi-scale feature learning\n",
    "   - Spectral normalization in discriminator to prevent mode collapse\n",
    "\n",
    "2. **Improved Training Stability:**\n",
    "   - Different learning rates for generators and discriminators\n",
    "   - Label smoothing for more robust adversarial training\n",
    "   - Gradient penalty for Lipschitz constraint enforcement\n",
    "\n",
    "3. **CycleGAN-Specific Features:**\n",
    "   - Cycle consistency loss for unsupervised domain adaptation\n",
    "   - Bidirectional mapping (A↔B) for comprehensive domain translation\n",
    "   - Identity loss to preserve domain-specific characteristics\n",
    "\n",
    "### Evaluation Framework Results:\n",
    "The comprehensive evaluation compares multiple anomaly detection approaches:\n",
    "\n",
    "1. **CycleGAN-Specific Methods:**\n",
    "   - **Cycle Consistency Error:** Measures reconstruction quality through domain cycles\n",
    "   - Domain translation quality assessment\n",
    "   - Bidirectional mapping consistency\n",
    "\n",
    "2. **Traditional Methods:**\n",
    "   - Reconstruction-based detection (threshold and percentile)\n",
    "   - One-Class SVM, Isolation Forest, Local Outlier Factor\n",
    "\n",
    "3. **Statistical Analysis:**\n",
    "   - Cross-validation with significance testing\n",
    "   - Performance comparison across multiple metrics\n",
    "   - Friedman test for method ranking\n",
    "\n",
    "### Key Advantages of CycleGAN:\n",
    "\n",
    "1. **Unsupervised Domain Adaptation:**\n",
    "   - Can learn mappings between different operational modes\n",
    "   - Useful for cross-domain anomaly detection\n",
    "   - Handles distribution shifts in IoT environments\n",
    "\n",
    "2. **Bidirectional Learning:**\n",
    "   - Forward and backward mappings provide additional constraints\n",
    "   - Cycle consistency ensures meaningful transformations\n",
    "   - Robust to mode collapse compared to standard GANs\n",
    "\n",
    "3. **Domain Translation Quality:**\n",
    "   - Can transform normal data between different domains\n",
    "   - Helps in data augmentation for rare scenarios\n",
    "   - Enables transfer learning between similar IoT systems\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Domain Adaptation:**\n",
    "   - Use CycleGAN when dealing with different operational conditions\n",
    "   - Apply to cross-sensor or cross-device anomaly detection\n",
    "   - Consider for temporal domain shifts (seasonal variations)\n",
    "\n",
    "2. **Training Strategies:**\n",
    "   - Balance cycle consistency and adversarial losses carefully\n",
    "   - Use progressive training with increasing sequence lengths\n",
    "   - Monitor both forward and backward translation quality\n",
    "\n",
    "3. **Architecture Considerations:**\n",
    "   - Implement attention mechanisms for long sequences\n",
    "   - Use residual connections for deep networks\n",
    "   - Apply spectral normalization for training stability\n",
    "\n",
    "4. **Evaluation Metrics:**\n",
    "   - Monitor cycle consistency loss as primary indicator\n",
    "   - Evaluate domain translation quality visually\n",
    "   - Use FID (Fréchet Inception Distance) for generated data quality\n",
    "\n",
    "5. **Deployment Considerations:**\n",
    "   - CycleGAN requires more computational resources than standard GANs\n",
    "   - Consider model compression for edge deployment\n",
    "   - Implement online adaptation for changing environments\n",
    "\n",
    "### Performance Expectations:\n",
    "CycleGAN excels in scenarios requiring domain adaptation and unsupervised learning but may have higher computational costs. The cycle consistency constraint provides additional regularization, making it suitable for robust anomaly detection across different operational modes in IoT systems.\n",
    "\n",
    "### Use Cases:\n",
    "- **Cross-Device Anomaly Detection:** Different sensor types or manufacturers\n",
    "- **Environmental Adaptation:** Seasonal or operational condition changes\n",
    "- **Multi-Modal Learning:** Combining different types of IoT data\n",
    "- **Transfer Learning:** Adapting models trained on one system to another"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
