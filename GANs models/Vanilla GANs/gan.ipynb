{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Vanilla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vanilla GAN Generator\n",
    "class VanillaGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, n_features=14, seq_len=4500):\n",
    "        super(VanillaGANGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Calculate reasonable initial size\n",
    "        self.init_size = max(seq_len // 64, 32)\n",
    "        \n",
    "        # Initial projection\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * self.init_size),\n",
    "            nn.BatchNorm1d(128 * self.init_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Upsampling blocks\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 128 -> 64 channels\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 64 -> 32 channels\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 32 -> 16 channels\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 16 -> n_features channels\n",
    "            nn.ConvTranspose1d(16, n_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Project latent to initial conv size\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size)\n",
    "        \n",
    "        # Progressive upsampling\n",
    "        out = self.conv_blocks(out)\n",
    "        \n",
    "        # Ensure exact sequence length\n",
    "        if out.shape[2] != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Return as (batch_size, seq_len, n_features)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "# Vanilla GAN Discriminator\n",
    "class VanillaGANDiscriminator(nn.Module):\n",
    "    def __init__(self, n_features=14, seq_len=4500):\n",
    "        super(VanillaGANDiscriminator, self).__init__()\n",
    "        \n",
    "        # Downsampling blocks\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # n_features -> 16 channels\n",
    "            nn.Conv1d(n_features, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 16 -> 32 channels\n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 32 -> 64 channels\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 64 -> 128 channels\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._get_conv_output_size(seq_len)\n",
    "        \n",
    "        # Binary classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * self.conv_output_size, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Output probability [0, 1]\n",
    "        )\n",
    "        \n",
    "    def _get_conv_output_size(self, seq_len):\n",
    "        size = seq_len\n",
    "        for _ in range(4):  # 4 conv layers\n",
    "            size = (size - 4 + 2) // 2 + 1\n",
    "        return size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (batch_size, seq_len, n_features)\n",
    "        x = x.transpose(1, 2)  # Convert to (batch_size, n_features, seq_len)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        features = self.conv_blocks(x)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        features = features.view(features.size(0), -1)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Vanilla GAN training function\n",
    "def train_vanilla_gan(normal_data, device, n_epochs=50, batch_size=32, lr_g=2e-4, lr_d=2e-4):\n",
    "    \"\"\"\n",
    "    Train Vanilla GAN with binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    print(f\"Starting Vanilla GAN Training\")\n",
    "    print(f\"Data shape: {normal_data.shape}\")\n",
    "    print(f\"Raw data range: [{normal_data.min():.4f}, {normal_data.max():.4f}]\")\n",
    "    \n",
    "    # Data preprocessing - scale to [-1, 1] for tanh output\n",
    "    data_mean = np.mean(normal_data, axis=(0, 1), keepdims=True)\n",
    "    data_std = np.std(normal_data, axis=(0, 1), keepdims=True) + 1e-8\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    normalized_data = (normal_data - data_mean) / data_std\n",
    "    normalized_data = np.tanh(normalized_data)  # Ensure [-1, 1] range\n",
    "    \n",
    "    print(f\"Normalized data range: [{normalized_data.min():.4f}, {normalized_data.max():.4f}]\")\n",
    "    \n",
    "    # Model parameters\n",
    "    latent_dim = 100\n",
    "    n_features = normalized_data.shape[-1]\n",
    "    seq_len = normalized_data.shape[1]\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = VanillaGANGenerator(latent_dim, n_features, seq_len).to(device)\n",
    "    discriminator = VanillaGANDiscriminator(n_features, seq_len).to(device)\n",
    "    \n",
    "    # Weight initialization\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    generator.apply(init_weights)\n",
    "    discriminator.apply(init_weights)\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataset = TensorDataset(torch.tensor(normalized_data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(f\"Training Parameters:\")\n",
    "    print(f\"  Epochs: {n_epochs}, Batch Size: {batch_size}\")\n",
    "    print(f\"  Generator LR: {lr_g}, Discriminator LR: {lr_d}\")\n",
    "    \n",
    "    # Training history\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    d_accuracies = []\n",
    "    \n",
    "    print(\"\\nStarting Training...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_d_losses = []\n",
    "        epoch_g_losses = []\n",
    "        epoch_d_accuracies = []\n",
    "        \n",
    "        for i, (real_samples,) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            batch_size_actual = real_samples.size(0)\n",
    "            \n",
    "            # Real and fake labels\n",
    "            real_labels = torch.ones(batch_size_actual, 1, device=device)\n",
    "            fake_labels = torch.zeros(batch_size_actual, 1, device=device)\n",
    "            \n",
    "            # ========================\n",
    "            # Train Discriminator\n",
    "            # ========================\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real samples\n",
    "            real_output = discriminator(real_samples)\n",
    "            d_loss_real = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Generate fake samples\n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_samples = generator(z).detach()\n",
    "            fake_output = discriminator(fake_samples)\n",
    "            d_loss_fake = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Calculate discriminator accuracy\n",
    "            real_pred = (real_output > 0.5).float()\n",
    "            fake_pred = (fake_output > 0.5).float()\n",
    "            d_accuracy = ((real_pred == real_labels).sum() + (fake_pred == fake_labels).sum()).float() / (2 * batch_size_actual)\n",
    "            \n",
    "            epoch_d_losses.append(d_loss.item())\n",
    "            epoch_d_accuracies.append(d_accuracy.item())\n",
    "            \n",
    "            # ========================\n",
    "            # Train Generator\n",
    "            # ========================\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake samples\n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_samples = generator(z)\n",
    "            fake_output = discriminator(fake_samples)\n",
    "            \n",
    "            # Generator loss (wants discriminator to classify fakes as real)\n",
    "            g_loss = criterion(fake_output, real_labels)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            epoch_g_losses.append(g_loss.item())\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_d_loss = np.mean(epoch_d_losses)\n",
    "        avg_g_loss = np.mean(epoch_g_losses)\n",
    "        avg_d_accuracy = np.mean(epoch_d_accuracies)\n",
    "        \n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        d_accuracies.append(avg_d_accuracy)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            print(f\"Epoch [{epoch+1:3d}/{n_epochs}] | \"\n",
    "                  f\"D Loss: {avg_d_loss:8.4f} | \"\n",
    "                  f\"G Loss: {avg_g_loss:8.4f} | \"\n",
    "                  f\"D Acc: {avg_d_accuracy:6.3f}\")\n",
    "            \n",
    "            # Check for mode collapse or training issues\n",
    "            if avg_d_accuracy < 0.1 or avg_d_accuracy > 0.9:\n",
    "                print(\"         ⚠️  Potential training instability detected\")\n",
    "            elif 0.4 <= avg_d_accuracy <= 0.6:\n",
    "                print(\"         ✅ Training appears balanced\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Store normalization parameters for generation\n",
    "    data_stats = (data_mean, data_std)\n",
    "    \n",
    "    return generator, discriminator, d_losses, g_losses, d_accuracies, data_stats\n",
    "\n",
    "# Sample generation for Vanilla GAN\n",
    "def generate_vanilla_samples(generator, num_samples, latent_dim, device, data_stats, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate samples and denormalize them\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    data_mean, data_std = data_stats\n",
    "    \n",
    "    generated_batches = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            # Generate noise\n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            \n",
    "            # Generate samples\n",
    "            batch_generated = generator(z)\n",
    "            \n",
    "            # Denormalize from [-1, 1] back to original scale\n",
    "            batch_generated = batch_generated.cpu().numpy()\n",
    "            batch_generated = np.arctanh(np.clip(batch_generated, -0.99, 0.99))  # Inverse tanh\n",
    "            batch_generated = batch_generated * data_std + data_mean\n",
    "            \n",
    "            generated_batches.append(batch_generated)\n",
    "    \n",
    "    return np.concatenate(generated_batches, axis=0)\n",
    "\n",
    "# Visualization for Vanilla GAN\n",
    "def plot_vanilla_training_curves(d_losses, g_losses, d_accuracies):\n",
    "    \"\"\"\n",
    "    Plot training curves for Vanilla GAN\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Discriminator loss\n",
    "    axes[0].plot(d_losses, label='Discriminator Loss', color='blue', alpha=0.7)\n",
    "    axes[0].set_title('Discriminator Loss Over Time')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Generator loss\n",
    "    axes[1].plot(g_losses, label='Generator Loss', color='red', alpha=0.7)\n",
    "    axes[1].set_title('Generator Loss Over Time')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Discriminator accuracy\n",
    "    axes[2].plot(d_accuracies, label='Discriminator Accuracy', color='green', alpha=0.7)\n",
    "    axes[2].axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Random Guess')\n",
    "    axes[2].set_title('Discriminator Accuracy Over Time')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787bd0a",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ba0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ultra-stable WGAN\n",
    "print(\"Training Vanilla GAN...\")\n",
    "generator, discriminator, d_history, g_history, d_acc_history, data_stats = train_vanilla_gan(\n",
    "    X_train_normal,\n",
    "    device,\n",
    "    n_epochs=200,\n",
    "    batch_size=32,\n",
    "    lr_g=0.001,\n",
    "    lr_d=0.000005\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plot_vanilla_training_curves(d_history, g_history, d_acc_history)\n",
    "\n",
    "# Generate samples\n",
    "print(\"Generating synthetic samples...\")\n",
    "num_samples = len(X_train_normal)\n",
    "generated_data = generate_vanilla_samples(\n",
    "    generator,\n",
    "    num_samples,\n",
    "    latent_dim=100,\n",
    "    device=device,\n",
    "    data_stats=data_stats,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Generated data shape: {generated_data.shape}\")\n",
    "print(f\"Generated data range: [{generated_data.min():.4f}, {generated_data.max():.4f}]\")\n",
    "print(f\"Original data range: [{normal_data.min():.4f}, {normal_data.max():.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061561c",
   "metadata": {},
   "source": [
    "# FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a282243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = generated_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"❌ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, generated_data, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
