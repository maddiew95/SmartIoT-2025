{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca19d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:34:31.343361Z",
     "iopub.status.busy": "2025-07-13T07:34:31.343053Z",
     "iopub.status.idle": "2025-07-13T07:34:38.247102Z",
     "shell.execute_reply": "2025-07-13T07:34:38.245599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A30\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Few-Shot GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aec1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:34:38.254310Z",
     "iopub.status.busy": "2025-07-13T07:34:38.253751Z",
     "iopub.status.idle": "2025-07-13T07:34:38.305206Z",
     "shell.execute_reply": "2025-07-13T07:34:38.304169Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Attention mechanism for Few-Shot learning\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, attention_dim=64):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, attention_dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(attention_dim, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention(x)\n",
    "        return x * attention_weights\n",
    "\n",
    "# Enhanced Few-Shot Generator with Attention and Residual Connections\n",
    "class EnhancedFewShotGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, channels=14, seq_len=4500):\n",
    "        super(EnhancedFewShotGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.channels = channels\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Start with a smaller sequence length and upsample\n",
    "        self.init_seq_len = seq_len // 32  # More aggressive downsampling\n",
    "        \n",
    "        # Enhanced initial projection with batch normalization\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * self.init_seq_len),\n",
    "            nn.BatchNorm1d(256 * self.init_seq_len),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Progressive upsampling with attention and residual connections\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            # Block 1: 256 -> 128 channels\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                AttentionBlock(128)\n",
    "            ),\n",
    "            # Block 2: 128 -> 64 channels  \n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                AttentionBlock(64)\n",
    "            ),\n",
    "            # Block 3: 64 -> 32 channels\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            # Block 4: 32 -> 16 channels\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            # Final block: 16 -> channels\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(16, channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 256, self.init_seq_len)\n",
    "        \n",
    "        # Progressive upsampling with skip connections\n",
    "        for i, block in enumerate(self.conv_blocks):\n",
    "            out = block(out)\n",
    "        \n",
    "        # Ensure exact sequence length through interpolation if needed\n",
    "        if out.size(2) != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        return out  # Shape: (batch, 14, 4500)\n",
    "\n",
    "# Enhanced Few-Shot Discriminator with Multi-Scale Features\n",
    "class EnhancedFewShotDiscriminator(nn.Module):\n",
    "    def __init__(self, channels=14, seq_len=4500):\n",
    "        super(EnhancedFewShotDiscriminator, self).__init__()\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        self.scale1_conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(channels, 32, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.scale2_conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(channels, 32, kernel_size=7, stride=1, padding=3)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # Further processing\n",
    "        self.model = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.utils.spectral_norm(nn.Linear(512, 256)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.utils.spectral_norm(nn.Linear(256, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Multi-scale feature extraction\n",
    "        scale1_features = self.scale1_conv(x)\n",
    "        scale2_features = self.scale2_conv(x)\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        multi_scale_features = torch.cat([scale1_features, scale2_features], dim=1)\n",
    "        \n",
    "        # Fusion and classification\n",
    "        fused_features = self.fusion(multi_scale_features)\n",
    "        output = self.model(fused_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FewShot1DDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: numpy array of shape [n_samples, 4500, 14]\n",
    "            labels: numpy array of shape [n_samples] (optional)\n",
    "        \"\"\"\n",
    "        # Transpose to (n_samples, 14, 4500) for Conv1d\n",
    "        self.data = torch.tensor(data.transpose(0, 2, 1), dtype=torch.float32)\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]  # Shape: (14, 4500)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return sample, label\n",
    "        return sample, 0\n",
    "\n",
    "# Enhanced training function with few-shot learning capabilities\n",
    "def train_enhanced_few_shot_gan(normal_data, device, epochs=100, batch_size=32, lr_g=1e-4, lr_d=2e-4):\n",
    "    \"\"\"\n",
    "    Enhanced Few-Shot GAN training with improved stability and attention mechanisms\n",
    "    \"\"\"\n",
    "    print(f\"Training Enhanced Few-Shot GAN on data shape: {normal_data.shape}\")\n",
    "    \n",
    "    # Data loading\n",
    "    dataset = FewShot1DDataset(normal_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Initialize enhanced models\n",
    "    latent_dim = 100\n",
    "    num_channels = normal_data.shape[-1]\n",
    "    seq_length = normal_data.shape[1]\n",
    "    \n",
    "    generator = EnhancedFewShotGenerator(latent_dim=latent_dim, channels=num_channels, seq_len=seq_length).to(device)\n",
    "    discriminator = EnhancedFewShotDiscriminator(channels=num_channels, seq_len=seq_length).to(device)\n",
    "\n",
    "    # Xavier/He initialization for better stability\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.xavier_normal_(m.weight, gain=0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    generator.apply(init_weights)\n",
    "    discriminator.apply(init_weights)\n",
    "\n",
    "    # Optimizers with different learning rates for stability\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "    # Learning rate schedulers\n",
    "    scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, patience=20, factor=0.8)\n",
    "    scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, patience=20, factor=0.8)\n",
    "\n",
    "    # Enhanced loss function with label smoothing\n",
    "    def adversarial_loss_smooth(pred, target_is_real, smoothing=0.1):\n",
    "        if target_is_real:\n",
    "            target = torch.ones_like(pred) * (1.0 - smoothing) + smoothing * torch.rand_like(pred)\n",
    "        else:\n",
    "            target = torch.zeros_like(pred) + smoothing * torch.rand_like(pred)\n",
    "        return nn.BCELoss()(pred, target)\n",
    "\n",
    "    # Training history\n",
    "    g_losses, d_losses = [], []\n",
    "    \n",
    "    print(\"Starting Enhanced Few-Shot GAN training...\")\n",
    "    print(f\"Generator LR: {lr_g}, Discriminator LR: {lr_d}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss, epoch_d_loss = 0, 0\n",
    "        \n",
    "        for i, (real_samples, _) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)  # Shape: (batch, 14, 4500)\n",
    "            batch_size_actual = real_samples.size(0)\n",
    "            \n",
    "            # Add noise to real samples for robustness\n",
    "            noisy_real = real_samples + 0.05 * torch.randn_like(real_samples)\n",
    "            \n",
    "            # ========================\n",
    "            # Train Discriminator\n",
    "            # ========================\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real samples\n",
    "            real_pred = discriminator(noisy_real)\n",
    "            d_real_loss = adversarial_loss_smooth(real_pred, True)\n",
    "            \n",
    "            # Fake samples\n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_samples = generator(z).detach()\n",
    "            fake_pred = discriminator(fake_samples)\n",
    "            d_fake_loss = adversarial_loss_smooth(fake_pred, False)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 0.5)\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # ========================\n",
    "            # Train Generator\n",
    "            # ========================\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake samples\n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_samples = generator(z)\n",
    "            fake_pred = discriminator(fake_samples)\n",
    "            \n",
    "            # Generator loss (want discriminator to classify fake as real)\n",
    "            g_loss = adversarial_loss_smooth(fake_pred, True)\n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"[Epoch {epoch+1}/{epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "        # Store average losses per epoch\n",
    "        avg_g_loss = epoch_g_loss / len(dataloader)\n",
    "        avg_d_loss = epoch_d_loss / len(dataloader)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        d_losses.append(avg_d_loss)\n",
    "\n",
    "        # Update learning rates\n",
    "        scheduler_G.step(avg_g_loss)\n",
    "        scheduler_D.step(avg_d_loss)\n",
    "\n",
    "        # Enhanced stability monitoring\n",
    "        if epoch % 10 == 0:\n",
    "            monitor_gan_stability(g_losses, d_losses, window=10)\n",
    "    \n",
    "    print(\"Enhanced Few-Shot GAN training completed!\")\n",
    "    return generator, discriminator, g_losses, d_losses\n",
    "\n",
    "def generate_samples(generator, num_samples, latent_dim=100):\n",
    "    \"\"\"\n",
    "    Generate samples using the trained generator\n",
    "    Returns data in shape (num_samples, 4500, 14)\n",
    "    \"\"\"\n",
    "    device = next(generator.parameters()).device\n",
    "    generator.eval()\n",
    "    \n",
    "    batch_size = 16\n",
    "    all_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            batch_samples = generator(z)  # Shape: (batch, 14, 4500)\n",
    "            all_samples.append(batch_samples.cpu())\n",
    "    \n",
    "    generated_data = torch.cat(all_samples, dim=0).numpy()\n",
    "    \n",
    "    # Transpose back to (n_samples, 4500, 14)\n",
    "    generated_data = generated_data.transpose(0, 2, 1)\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "# Enhanced monitoring with more detailed analysis\n",
    "def monitor_gan_stability(g_losses, d_losses, window=10):\n",
    "    \"\"\"\n",
    "    Enhanced GAN training stability monitoring\n",
    "    \"\"\"\n",
    "    if len(g_losses) < window:\n",
    "        return\n",
    "    \n",
    "    # Recent losses\n",
    "    recent_g = np.mean(g_losses[-window:])\n",
    "    recent_d = np.mean(d_losses[-window:])\n",
    "    \n",
    "    # Loss ratio (should be roughly balanced)\n",
    "    ratio = recent_g / (recent_d + 1e-8)\n",
    "    \n",
    "    # Loss variance (should be stable, not oscillating wildly)\n",
    "    g_var = np.var(g_losses[-window:])\n",
    "    d_var = np.var(d_losses[-window:])\n",
    "    \n",
    "    # Loss trend analysis\n",
    "    if len(g_losses) >= window * 2:\n",
    "        g_trend = np.mean(g_losses[-window:]) - np.mean(g_losses[-window*2:-window])\n",
    "        d_trend = np.mean(d_losses[-window:]) - np.mean(d_losses[-window*2:-window])\n",
    "    else:\n",
    "        g_trend = d_trend = 0\n",
    "    \n",
    "    print(f\"G/D Ratio: {ratio:.3f} | G_var: {g_var:.4f} | D_var: {d_var:.4f}\")\n",
    "    print(f\"G_trend: {g_trend:+.4f} | D_trend: {d_trend:+.4f}\")\n",
    "    \n",
    "    # Enhanced stability warnings\n",
    "    if ratio > 5:\n",
    "        print(\"⚠️  Generator significantly overpowering Discriminator\")\n",
    "        print(\"   💡 Consider: Lower G learning rate or train D more frequently\")\n",
    "    elif ratio < 0.2:\n",
    "        print(\"⚠️  Discriminator significantly overpowering Generator\")\n",
    "        print(\"   💡 Consider: Lower D learning rate or add noise to real data\")\n",
    "    elif g_var > 1.0 or d_var > 1.0:\n",
    "        print(\"⚠️  High variance - unstable training detected\")\n",
    "        print(\"   💡 Consider: Lower learning rates or gradient clipping\")\n",
    "    elif abs(g_trend) > 0.5 or abs(d_trend) > 0.5:\n",
    "        print(\"⚠️  Significant loss trends detected\")\n",
    "        print(\"   💡 Consider: Learning rate scheduling or early stopping\")\n",
    "    else:\n",
    "        print(\"✅ Training appears stable and balanced\")\n",
    "\n",
    "# Enhanced visualization\n",
    "def plot_enhanced_training_curves(g_losses, d_losses):\n",
    "    \"\"\"\n",
    "    Plot enhanced training curves with additional analysis\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(g_losses, label='Generator Loss', alpha=0.7)\n",
    "    axes[0, 0].plot(d_losses, label='Discriminator Loss', alpha=0.7)\n",
    "    axes[0, 0].set_title('Training Losses')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss ratio\n",
    "    ratios = [g/(d+1e-8) for g, d in zip(g_losses, d_losses)]\n",
    "    axes[0, 1].plot(ratios, color='green', alpha=0.7)\n",
    "    axes[0, 1].axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Ideal Ratio')\n",
    "    axes[0, 1].set_title('Generator/Discriminator Loss Ratio')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('G_loss / D_loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Moving averages\n",
    "    window = 10\n",
    "    if len(g_losses) >= window:\n",
    "        g_ma = pd.Series(g_losses).rolling(window=window).mean()\n",
    "        d_ma = pd.Series(d_losses).rolling(window=window).mean()\n",
    "        \n",
    "        axes[1, 0].plot(g_ma, label=f'Generator MA({window})', alpha=0.7)\n",
    "        axes[1, 0].plot(d_ma, label=f'Discriminator MA({window})', alpha=0.7)\n",
    "        axes[1, 0].set_title('Moving Average Losses')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss variance\n",
    "    if len(g_losses) >= window:\n",
    "        g_var = pd.Series(g_losses).rolling(window=window).var()\n",
    "        d_var = pd.Series(d_losses).rolling(window=window).var()\n",
    "        \n",
    "        axes[1, 1].plot(g_var, label=f'Generator Var({window})', alpha=0.7)\n",
    "        axes[1, 1].plot(d_var, label=f'Discriminator Var({window})', alpha=0.7)\n",
    "        axes[1, 1].set_title('Loss Variance (Stability Indicator)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Variance')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# Few-Shot GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0267ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:34:38.311706Z",
     "iopub.status.busy": "2025-07-13T07:34:38.311292Z",
     "iopub.status.idle": "2025-07-13T07:34:38.356406Z",
     "shell.execute_reply": "2025-07-13T07:34:38.355307Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention1D(nn.Module):\n",
    "    \"\"\"Self-attention module optimized for time series\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention1D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.query = nn.Conv1d(in_channels, in_channels // 4, 1)  # Reduced for efficiency\n",
    "        self.key = nn.Conv1d(in_channels, in_channels // 4, 1)\n",
    "        self.value = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, length = x.size()\n",
    "        \n",
    "        # Subsample for long sequences to reduce computation\n",
    "        if length > 1000:\n",
    "            step = length // 500\n",
    "            x_sub = x[:, :, ::step]\n",
    "        else:\n",
    "            x_sub = x\n",
    "            \n",
    "        sub_length = x_sub.size(2)\n",
    "        \n",
    "        # Generate query, key, value on subsampled data\n",
    "        q = self.query(x_sub).view(batch_size, -1, sub_length).permute(0, 2, 1)\n",
    "        k = self.key(x_sub).view(batch_size, -1, sub_length)\n",
    "        v = self.value(x_sub).view(batch_size, -1, sub_length)\n",
    "        \n",
    "        # Attention calculation\n",
    "        attention = torch.bmm(q, k)\n",
    "        attention = self.softmax(attention)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.bmm(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, channels, sub_length)\n",
    "        \n",
    "        # Interpolate back to original length if subsampled\n",
    "        if length > 1000:\n",
    "            out = nn.functional.interpolate(out, size=length, mode='linear', align_corners=False)\n",
    "        \n",
    "        return self.gamma * out + x\n",
    "\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    \"\"\"Optimized residual block for sensor data\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 5, padding=2)  # Larger kernel for temporal patterns\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + residual)\n",
    "\n",
    "class ImprovedFewShotGenerator(nn.Module):\n",
    "    \"\"\"Enhanced generator optimized for sensor time series\"\"\"\n",
    "    def __init__(self, latent_dim=100, output_channels=14, target_length=4500):\n",
    "        super(ImprovedFewShotGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.target_length = target_length\n",
    "        \n",
    "        # Optimized initial projection for 4500 length\n",
    "        self.initial_length = 141  # 141 * 32 = 4512 ≈ 4500\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512 * self.initial_length),\n",
    "            nn.BatchNorm1d(512 * self.initial_length),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Progressive upsampling optimized for sensor data\n",
    "        self.upsample_blocks = nn.ModuleList([\n",
    "            # 141 -> 282\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(512, 256, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(True),\n",
    "                ResidualBlock1D(256)\n",
    "            ),\n",
    "            # 282 -> 564  \n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(256, 128, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(True),\n",
    "                ResidualBlock1D(128)\n",
    "            ),\n",
    "            # 564 -> 1128\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(128, 64, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(True),\n",
    "                ResidualBlock1D(64)\n",
    "            ),\n",
    "            # 1128 -> 2256\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(64, 32, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(True),\n",
    "                ResidualBlock1D(32)\n",
    "            ),\n",
    "            # 2256 -> 4512\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(32, 16, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(True),\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        # Attention for long-range temporal dependencies\n",
    "        self.attention = SelfAttention1D(16)\n",
    "        \n",
    "        # Final layers for sensor-specific patterns\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Conv1d(16, output_channels, 7, padding=3),  # Larger kernel for temporal smoothness\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Project latent vector\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), 512, self.initial_length)\n",
    "        \n",
    "        # Progressive upsampling\n",
    "        for upsample_block in self.upsample_blocks:\n",
    "            x = upsample_block(x)\n",
    "        \n",
    "        # Apply attention for temporal coherence\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Final transformation\n",
    "        x = self.final_layers(x)\n",
    "        \n",
    "        # Precise length adjustment for 4500\n",
    "        current_length = x.size(-1)\n",
    "        if current_length > self.target_length:\n",
    "            start_idx = (current_length - self.target_length) // 2\n",
    "            x = x[:, :, start_idx:start_idx + self.target_length]\n",
    "        elif current_length < self.target_length:\n",
    "            pad_left = (self.target_length - current_length) // 2\n",
    "            pad_right = self.target_length - current_length - pad_left\n",
    "            x = F.pad(x, (pad_left, pad_right), mode='reflect')\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ImprovedFewShotDiscriminator(nn.Module):\n",
    "    \"\"\"Enhanced discriminator optimized for sensor time series\"\"\"\n",
    "    def __init__(self, input_channels=14):\n",
    "        super(ImprovedFewShotDiscriminator, self).__init__()\n",
    "        \n",
    "        # Multi-scale feature extraction for different temporal patterns\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            # 4500 -> 2250\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(nn.Conv1d(input_channels, 64, 8, stride=2, padding=3)),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.1)\n",
    "            ),\n",
    "            # 2250 -> 1125\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(nn.Conv1d(64, 128, 8, stride=2, padding=3)),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.1)\n",
    "            ),\n",
    "            # 1125 -> 562\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(nn.Conv1d(128, 256, 8, stride=2, padding=3)),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ),\n",
    "            # 562 -> 281\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(nn.Conv1d(256, 512, 8, stride=2, padding=3)),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ),\n",
    "            # 281 -> 140\n",
    "            nn.Sequential(\n",
    "                nn.utils.spectral_norm(nn.Conv1d(512, 512, 8, stride=2, padding=3)),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.3)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Optimized attention for sensor patterns\n",
    "        self.attention = SelfAttention1D(512)\n",
    "        \n",
    "        # Enhanced classifier for better discrimination\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.utils.spectral_norm(nn.Linear(512, 256)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.utils.spectral_norm(nn.Linear(256, 64)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.utils.spectral_norm(nn.Linear(64, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        features = []\n",
    "        \n",
    "        # Progressive feature extraction\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "            if return_features:\n",
    "                features.append(x)\n",
    "        \n",
    "        # Apply attention for important temporal patterns\n",
    "        x = self.attention(x)\n",
    "        if return_features:\n",
    "            features.append(x)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return output, features\n",
    "        return output\n",
    "\n",
    "def feature_matching_loss(real_features, fake_features):\n",
    "    \"\"\"Enhanced feature matching loss with temporal weighting\"\"\"\n",
    "    loss = 0\n",
    "    weights = [1.0, 1.5, 2.0, 2.5, 3.0, 1.0]  # Higher weight for middle layers\n",
    "    \n",
    "    for i, (real_feat, fake_feat) in enumerate(zip(real_features, fake_features)):\n",
    "        weight = weights[i] if i < len(weights) else 1.0\n",
    "        # Use both mean and variance matching for better distribution alignment\n",
    "        mean_loss = nn.MSELoss()(fake_feat.mean(0), real_feat.mean(0))\n",
    "        var_loss = nn.MSELoss()(fake_feat.var(0), real_feat.var(0))\n",
    "        loss += weight * (mean_loss + 0.5 * var_loss)\n",
    "    return loss\n",
    "\n",
    "def gradient_penalty(discriminator, real_samples, fake_samples, device, lambda_gp=10.0):\n",
    "    \"\"\"Optimized gradient penalty for sensor data\"\"\"\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1).expand_as(real_samples).to(device)\n",
    "    \n",
    "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    interpolated.requires_grad_(True)\n",
    "    \n",
    "    d_interpolated = discriminator(interpolated)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_interpolated),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return lambda_gp * gradient_penalty\n",
    "\n",
    "def train_improved_few_shot_gan(X_train, epochs=200, batch_size=16, latent_dim=128, \n",
    "                               lr_g=0.0002, lr_d=0.0001, device='cuda'):  # Reduced learning rates\n",
    "    \"\"\"\n",
    "    Optimized training for sensor time series with better FID score\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Optimized Few-Shot GAN Training for Sensor Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data loading with optimized batch size\n",
    "    dataset = FewShot1DDataset(X_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Initialize optimized models\n",
    "    generator = ImprovedFewShotGenerator(latent_dim=latent_dim).to(device)\n",
    "    discriminator = ImprovedFewShotDiscriminator().to(device)\n",
    "    \n",
    "    # Balanced optimizers for sensor data - FIXED learning rates\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss function\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    \n",
    "    # Training tracking\n",
    "    g_losses, d_losses = [], []\n",
    "    feature_losses, gp_losses = [], []\n",
    "    \n",
    "    print(f\"📊 Optimized Configuration:\")\n",
    "    print(f\"   • Latent Dimension: {latent_dim}\")\n",
    "    print(f\"   • Batch Size: {batch_size}\")\n",
    "    print(f\"   • Learning Rates: G={lr_g}, D={lr_d}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_g_loss, epoch_d_loss = 0, 0\n",
    "        epoch_feature_loss, epoch_gp_loss = 0, 0\n",
    "        \n",
    "        for i, (real_samples, _) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            batch_size_current = real_samples.size(0)\n",
    "            \n",
    "            # ========================\n",
    "            # Balanced Discriminator Training - REDUCED FREQUENCY\n",
    "            # ========================\n",
    "            if i % 2 == 0:  # Train discriminator every 2 iterations\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real samples with STRONGER label smoothing\n",
    "                real_labels = torch.ones(batch_size_current, 1, device=device) * (0.8 + 0.15 * torch.rand(batch_size_current, 1, device=device))\n",
    "                real_pred = discriminator(real_samples)\n",
    "                real_loss = adversarial_loss(real_pred, real_labels)\n",
    "                \n",
    "                # Fake samples\n",
    "                z = torch.randn(batch_size_current, latent_dim, device=device)\n",
    "                fake_samples = generator(z).detach()\n",
    "                fake_labels = torch.zeros(batch_size_current, 1, device=device) + 0.2 * torch.rand(batch_size_current, 1, device=device)\n",
    "                fake_pred = discriminator(fake_samples)\n",
    "                fake_loss = adversarial_loss(fake_pred, fake_labels)\n",
    "                \n",
    "                # REDUCED gradient penalty\n",
    "                gp = gradient_penalty(discriminator, real_samples, fake_samples, device, lambda_gp=2.0)  # Reduced from 5.0\n",
    "                \n",
    "                # Total discriminator loss\n",
    "                d_loss = real_loss + fake_loss + gp\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # Stronger gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 0.5)  # Reduced from 1.0\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                epoch_d_loss += (real_loss.item() + fake_loss.item()) / 2\n",
    "                epoch_gp_loss += gp.item()\n",
    "            \n",
    "            # ========================\n",
    "            # Enhanced Generator Training - EVERY ITERATION\n",
    "            # ========================\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake samples\n",
    "            z = torch.randn(batch_size_current, latent_dim, device=device)\n",
    "            fake_samples = generator(z)\n",
    "            \n",
    "            # Get predictions and features\n",
    "            fake_pred, fake_features = discriminator(fake_samples, return_features=True)\n",
    "            _, real_features = discriminator(real_samples, return_features=True)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            valid_labels = torch.ones(batch_size_current, 1, device=device)\n",
    "            adv_loss = adversarial_loss(fake_pred, valid_labels)\n",
    "            \n",
    "            # Enhanced feature matching loss\n",
    "            fm_loss = feature_matching_loss(real_features, fake_features)\n",
    "            \n",
    "            # REDUCED feature matching weight\n",
    "            g_loss = adv_loss + 5.0 * fm_loss  # Reduced from 15.0\n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)  # Reduced from 1.0\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            epoch_g_loss += adv_loss.item()\n",
    "            epoch_feature_loss += fm_loss.item()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"[Epoch {epoch+1:3d}/{epochs}] [Batch {i:3d}/{len(dataloader)}] \"\n",
    "                      f\"[D: {d_loss.item() if i % 2 == 0 else 'skipped':.4f}] [G: {adv_loss.item():.4f}] \"\n",
    "                      f\"[FM: {fm_loss.item():.4f}] [GP: {gp.item() if i % 2 == 0 else 'skipped':.4f}]\")\n",
    "        \n",
    "        # Store average losses per epoch\n",
    "        g_losses.append(epoch_g_loss / len(dataloader))\n",
    "        d_losses.append(epoch_d_loss / (len(dataloader) // 2))  # Adjusted for reduced D training\n",
    "        feature_losses.append(epoch_feature_loss / len(dataloader))\n",
    "        gp_losses.append(epoch_gp_loss / (len(dataloader) // 2))  # Adjusted for reduced D training\n",
    "        \n",
    "        # Enhanced monitoring every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"\\n{'='*15} Epoch {epoch+1} Summary {'='*15}\")\n",
    "            print(f\"📈 Avg Generator Loss: {g_losses[-1]:.4f}\")\n",
    "            print(f\"📉 Avg Discriminator Loss: {d_losses[-1]:.4f}\")\n",
    "            print(f\"🎯 Avg Feature Matching Loss: {feature_losses[-1]:.4f}\")\n",
    "            print(f\"⚖️  Avg Gradient Penalty: {gp_losses[-1]:.4f}\")\n",
    "            \n",
    "            # Generate samples for quality check\n",
    "            with torch.no_grad():\n",
    "                z_test = torch.randn(32, latent_dim, device=device)\n",
    "                test_samples = generator(z_test)\n",
    "                \n",
    "                # Basic quality metrics\n",
    "                sample_mean = test_samples.mean().item()\n",
    "                sample_std = test_samples.std().item()\n",
    "                sample_range = (test_samples.max() - test_samples.min()).item()\n",
    "                \n",
    "                print(f\"📊 Generated Sample Stats:\")\n",
    "                print(f\"   • Mean: {sample_mean:.4f}\")\n",
    "                print(f\"   • Std: {sample_std:.4f}\")\n",
    "                print(f\"   • Range: {sample_range:.4f}\")\n",
    "            \n",
    "            print(\"=\" * 50)\n",
    "    \n",
    "    return generator, discriminator, g_losses, d_losses, feature_losses, gp_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ee542",
   "metadata": {},
   "source": [
    "# Generate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6809752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:34:38.360996Z",
     "iopub.status.busy": "2025-07-13T07:34:38.360661Z",
     "iopub.status.idle": "2025-07-13T07:49:05.480008Z",
     "shell.execute_reply": "2025-07-13T07:49:05.478801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Optimized Few-Shot GAN Training for Sensor Data\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Optimized Configuration:\n",
      "   • Latent Dimension: 128\n",
      "   • Batch Size: 16\n",
      "   • Learning Rates: G=0.0002, D=0.0001\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1/200] [Batch   0/35] [D: 3.3825] [G: 0.6329] [FM: 3.9737] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 1 Summary ===============\n",
      "📈 Avg Generator Loss: 0.7122\n",
      "📉 Avg Discriminator Loss: 0.7190\n",
      "🎯 Avg Feature Matching Loss: 5.3338\n",
      "⚖️  Avg Gradient Penalty: 2.1168\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: 0.0761\n",
      "   • Std: 0.3420\n",
      "   • Range: 1.9663\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   2/200] [Batch   0/35] [D: 3.2986] [G: 0.7805] [FM: 7.9759] [GP: 1.9994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   3/200] [Batch   0/35] [D: 3.1545] [G: 0.9054] [FM: 8.5570] [GP: 1.9994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   4/200] [Batch   0/35] [D: 3.0658] [G: 1.0528] [FM: 4.9982] [GP: 1.9993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   5/200] [Batch   0/35] [D: 2.9538] [G: 1.1926] [FM: 4.5491] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   6/200] [Batch   0/35] [D: 2.8018] [G: 1.5233] [FM: 4.8653] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   7/200] [Batch   0/35] [D: 2.8969] [G: 2.0165] [FM: 6.5069] [GP: 1.9965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   8/200] [Batch   0/35] [D: 2.7374] [G: 2.1600] [FM: 8.3356] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   9/200] [Batch   0/35] [D: 2.7511] [G: 2.1751] [FM: 5.6983] [GP: 1.9963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  10/200] [Batch   0/35] [D: 2.8138] [G: 1.4767] [FM: 3.7622] [GP: 1.9978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  11/200] [Batch   0/35] [D: 3.1799] [G: 2.3301] [FM: 4.5370] [GP: 1.9982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  12/200] [Batch   0/35] [D: 2.6692] [G: 2.6820] [FM: 4.0254] [GP: 1.9971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  13/200] [Batch   0/35] [D: 2.7259] [G: 2.4412] [FM: 3.8727] [GP: 1.9941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  14/200] [Batch   0/35] [D: 2.7350] [G: 2.3367] [FM: 3.7981] [GP: 1.9941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  15/200] [Batch   0/35] [D: 2.7530] [G: 1.9884] [FM: 2.9949] [GP: 1.9934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  16/200] [Batch   0/35] [D: 2.7451] [G: 2.0411] [FM: 2.5865] [GP: 1.9926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  17/200] [Batch   0/35] [D: 2.9100] [G: 2.4745] [FM: 2.8392] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  18/200] [Batch   0/35] [D: 2.7829] [G: 2.2714] [FM: 2.5161] [GP: 1.9908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  19/200] [Batch   0/35] [D: 2.7546] [G: 2.2839] [FM: 3.1424] [GP: 1.9949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  20/200] [Batch   0/35] [D: 2.7158] [G: 2.4093] [FM: 2.9032] [GP: 1.9892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  21/200] [Batch   0/35] [D: 2.8895] [G: 2.4305] [FM: 2.5675] [GP: 1.9980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 21 Summary ===============\n",
      "📈 Avg Generator Loss: 2.2263\n",
      "📉 Avg Discriminator Loss: 0.4264\n",
      "🎯 Avg Feature Matching Loss: 2.8749\n",
      "⚖️  Avg Gradient Penalty: 2.1094\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: -0.0319\n",
      "   • Std: 0.2687\n",
      "   • Range: 1.9998\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  22/200] [Batch   0/35] [D: 2.8493] [G: 2.1545] [FM: 2.5864] [GP: 1.9980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  23/200] [Batch   0/35] [D: 2.7004] [G: 2.5201] [FM: 2.2569] [GP: 1.9916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  24/200] [Batch   0/35] [D: 2.7451] [G: 2.4573] [FM: 2.4742] [GP: 1.9984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  25/200] [Batch   0/35] [D: 2.8378] [G: 2.1584] [FM: 2.5580] [GP: 1.9944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  26/200] [Batch   0/35] [D: 2.7080] [G: 2.3763] [FM: 3.0824] [GP: 1.9945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  27/200] [Batch   0/35] [D: 2.7163] [G: 2.2912] [FM: 3.2712] [GP: 1.9896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  28/200] [Batch   0/35] [D: 2.6900] [G: 2.2921] [FM: 2.9568] [GP: 1.9910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  29/200] [Batch   0/35] [D: 2.6098] [G: 2.3373] [FM: 2.8272] [GP: 1.9789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  30/200] [Batch   0/35] [D: 3.0364] [G: 2.1372] [FM: 3.3234] [GP: 1.9981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  31/200] [Batch   0/35] [D: 2.6969] [G: 2.5263] [FM: 3.7672] [GP: 1.9992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  32/200] [Batch   0/35] [D: 2.6857] [G: 2.3851] [FM: 3.0983] [GP: 1.9963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  33/200] [Batch   0/35] [D: 2.7180] [G: 2.3212] [FM: 2.9408] [GP: 1.9924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  34/200] [Batch   0/35] [D: 2.7795] [G: 2.2383] [FM: 2.7322] [GP: 1.9986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  35/200] [Batch   0/35] [D: 3.5558] [G: 2.2054] [FM: 3.0212] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  36/200] [Batch   0/35] [D: 2.7139] [G: 2.2159] [FM: 3.4460] [GP: 1.9946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  37/200] [Batch   0/35] [D: 2.6388] [G: 2.2536] [FM: 2.9584] [GP: 1.9929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  38/200] [Batch   0/35] [D: 3.0183] [G: 2.5885] [FM: 3.2406] [GP: 1.9902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  39/200] [Batch   0/35] [D: 2.7119] [G: 2.1084] [FM: 3.4980] [GP: 1.9916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  40/200] [Batch   0/35] [D: 2.6892] [G: 2.3770] [FM: 3.0883] [GP: 1.9899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  41/200] [Batch   0/35] [D: 2.7852] [G: 2.4382] [FM: 3.4470] [GP: 1.9926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 41 Summary ===============\n",
      "📈 Avg Generator Loss: 2.2304\n",
      "📉 Avg Discriminator Loss: 0.3856\n",
      "🎯 Avg Feature Matching Loss: 3.1753\n",
      "⚖️  Avg Gradient Penalty: 2.1111\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: 0.0122\n",
      "   • Std: 0.3298\n",
      "   • Range: 1.9994\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  42/200] [Batch   0/35] [D: 2.6247] [G: 1.9883] [FM: 3.4049] [GP: 1.9963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  43/200] [Batch   0/35] [D: 2.6634] [G: 2.4997] [FM: 3.2700] [GP: 1.9825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  44/200] [Batch   0/35] [D: 2.7139] [G: 2.3652] [FM: 4.2476] [GP: 1.9993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  45/200] [Batch   0/35] [D: 2.7129] [G: 2.2704] [FM: 3.4882] [GP: 1.9948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  46/200] [Batch   0/35] [D: 2.6840] [G: 2.3254] [FM: 3.4738] [GP: 1.9916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  47/200] [Batch   0/35] [D: 2.6957] [G: 2.3707] [FM: 3.0750] [GP: 1.9908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  48/200] [Batch   0/35] [D: 2.9654] [G: 2.0074] [FM: 3.0905] [GP: 1.9979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  49/200] [Batch   0/35] [D: 2.8136] [G: 2.4060] [FM: 2.4107] [GP: 1.9961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  50/200] [Batch   0/35] [D: 2.7422] [G: 2.0861] [FM: 2.7275] [GP: 1.9896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  51/200] [Batch   0/35] [D: 2.7852] [G: 2.2865] [FM: 3.5614] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  52/200] [Batch   0/35] [D: 2.6625] [G: 2.4203] [FM: 2.7609] [GP: 1.9918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  53/200] [Batch   0/35] [D: 2.7342] [G: 2.2110] [FM: 2.7889] [GP: 1.9919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  54/200] [Batch   0/35] [D: 2.7252] [G: 2.1682] [FM: 2.3870] [GP: 1.9975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  55/200] [Batch   0/35] [D: 2.6987] [G: 2.1857] [FM: 2.8774] [GP: 1.9975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  56/200] [Batch   0/35] [D: 2.6998] [G: 2.2683] [FM: 2.0927] [GP: 1.9973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  57/200] [Batch   0/35] [D: 2.9295] [G: 2.6946] [FM: 2.0798] [GP: 1.9975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  58/200] [Batch   0/35] [D: 2.6905] [G: 2.4014] [FM: 2.7537] [GP: 1.9938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  59/200] [Batch   0/35] [D: 2.7100] [G: 2.1043] [FM: 3.0827] [GP: 1.9961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  60/200] [Batch   0/35] [D: 2.7485] [G: 2.2140] [FM: 2.7886] [GP: 1.9947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  61/200] [Batch   0/35] [D: 2.7943] [G: 2.0643] [FM: 2.1287] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 61 Summary ===============\n",
      "📈 Avg Generator Loss: 2.2670\n",
      "📉 Avg Discriminator Loss: 0.3774\n",
      "🎯 Avg Feature Matching Loss: 2.9869\n",
      "⚖️  Avg Gradient Penalty: 2.1118\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: 0.0142\n",
      "   • Std: 0.3653\n",
      "   • Range: 1.9999\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  62/200] [Batch   0/35] [D: 2.6387] [G: 2.4038] [FM: 3.6838] [GP: 1.9962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  63/200] [Batch   0/35] [D: 2.7611] [G: 2.3676] [FM: 3.0348] [GP: 1.9982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  64/200] [Batch   0/35] [D: 2.6878] [G: 2.2958] [FM: 3.5725] [GP: 1.9908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  65/200] [Batch   0/35] [D: 2.6498] [G: 2.4375] [FM: 3.5736] [GP: 1.9977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  66/200] [Batch   0/35] [D: 2.7097] [G: 2.1760] [FM: 3.2291] [GP: 1.9928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  67/200] [Batch   0/35] [D: 2.7563] [G: 2.0851] [FM: 3.6008] [GP: 1.9909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  68/200] [Batch   0/35] [D: 2.8417] [G: 2.2182] [FM: 3.2947] [GP: 1.9917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  69/200] [Batch   0/35] [D: 2.7338] [G: 2.2791] [FM: 2.9606] [GP: 1.9917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  70/200] [Batch   0/35] [D: 2.7116] [G: 2.2901] [FM: 2.6315] [GP: 1.9923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  71/200] [Batch   0/35] [D: 2.8001] [G: 2.4410] [FM: 3.1814] [GP: 1.9901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  72/200] [Batch   0/35] [D: 2.6955] [G: 2.4043] [FM: 2.6522] [GP: 1.9973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  73/200] [Batch   0/35] [D: 2.7138] [G: 2.4166] [FM: 2.2350] [GP: 1.9889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  74/200] [Batch   0/35] [D: 2.7405] [G: 2.3580] [FM: 2.8986] [GP: 1.9947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  75/200] [Batch   0/35] [D: 2.7102] [G: 2.3008] [FM: 3.2666] [GP: 1.9944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  76/200] [Batch   0/35] [D: 2.7149] [G: 2.4562] [FM: 3.4864] [GP: 1.9873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  77/200] [Batch   0/35] [D: 2.7181] [G: 2.4777] [FM: 3.4231] [GP: 1.9952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  78/200] [Batch   0/35] [D: 2.6758] [G: 2.4087] [FM: 2.4607] [GP: 1.9921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  79/200] [Batch   0/35] [D: 2.7115] [G: 2.1484] [FM: 2.7364] [GP: 1.9924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  80/200] [Batch   0/35] [D: 2.7048] [G: 2.2926] [FM: 3.8026] [GP: 1.9965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  81/200] [Batch   0/35] [D: 2.7037] [G: 2.2665] [FM: 3.1665] [GP: 1.9900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 81 Summary ===============\n",
      "📈 Avg Generator Loss: 2.2221\n",
      "📉 Avg Discriminator Loss: 0.4032\n",
      "🎯 Avg Feature Matching Loss: 3.1223\n",
      "⚖️  Avg Gradient Penalty: 2.1101\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: -0.0274\n",
      "   • Std: 0.4009\n",
      "   • Range: 1.9999\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  82/200] [Batch   0/35] [D: 2.7750] [G: 2.2954] [FM: 2.8284] [GP: 1.9910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  83/200] [Batch   0/35] [D: 2.6501] [G: 2.2487] [FM: 3.6929] [GP: 1.9975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  84/200] [Batch   0/35] [D: 2.7497] [G: 2.2037] [FM: 3.3416] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  85/200] [Batch   0/35] [D: 2.8812] [G: 2.2442] [FM: 3.3152] [GP: 1.9983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  86/200] [Batch   0/35] [D: 2.6987] [G: 2.3668] [FM: 3.3302] [GP: 1.9925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  87/200] [Batch   0/35] [D: 3.1075] [G: 2.3133] [FM: 3.1692] [GP: 1.9978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  88/200] [Batch   0/35] [D: 2.6789] [G: 1.8960] [FM: 3.4609] [GP: 1.9910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  89/200] [Batch   0/35] [D: 2.7558] [G: 2.4183] [FM: 3.7058] [GP: 1.9986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  90/200] [Batch   0/35] [D: 2.6838] [G: 2.4895] [FM: 3.2768] [GP: 1.9881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  91/200] [Batch   0/35] [D: 2.7095] [G: 2.2192] [FM: 5.0208] [GP: 1.9970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  92/200] [Batch   0/35] [D: 2.7418] [G: 2.3924] [FM: 4.0850] [GP: 1.9974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  93/200] [Batch   0/35] [D: 2.7493] [G: 2.5443] [FM: 2.6801] [GP: 1.9942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  94/200] [Batch   0/35] [D: 2.6891] [G: 2.1939] [FM: 3.8593] [GP: 1.9937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  95/200] [Batch   0/35] [D: 2.6739] [G: 2.1819] [FM: 3.8537] [GP: 1.9974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  96/200] [Batch   0/35] [D: 2.6885] [G: 2.0400] [FM: 5.3249] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  97/200] [Batch   0/35] [D: 2.7612] [G: 2.5997] [FM: 3.8486] [GP: 1.9969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  98/200] [Batch   0/35] [D: 2.7052] [G: 2.3560] [FM: 4.2299] [GP: 1.9976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  99/200] [Batch   0/35] [D: 2.7265] [G: 2.2994] [FM: 3.8375] [GP: 1.9946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 100/200] [Batch   0/35] [D: 3.0070] [G: 2.1187] [FM: 5.1350] [GP: 1.9915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 101/200] [Batch   0/35] [D: 2.6992] [G: 2.3188] [FM: 3.7636] [GP: 1.9938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 101 Summary ===============\n",
      "📈 Avg Generator Loss: 2.2601\n",
      "📉 Avg Discriminator Loss: 0.3777\n",
      "🎯 Avg Feature Matching Loss: 3.8635\n",
      "⚖️  Avg Gradient Penalty: 2.1133\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: 0.0265\n",
      "   • Std: 0.3849\n",
      "   • Range: 1.9999\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 102/200] [Batch   0/35] [D: 2.6830] [G: 2.4186] [FM: 3.7713] [GP: 1.9981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 103/200] [Batch   0/35] [D: 3.0662] [G: 2.4537] [FM: 4.5586] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 104/200] [Batch   0/35] [D: 2.7364] [G: 2.1884] [FM: 4.2334] [GP: 1.9964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 105/200] [Batch   0/35] [D: 2.8051] [G: 2.6534] [FM: 4.3444] [GP: 1.9983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 106/200] [Batch   0/35] [D: 2.6824] [G: 2.6930] [FM: 2.2041] [GP: 1.9917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 107/200] [Batch   0/35] [D: 2.7847] [G: 2.0495] [FM: 3.4991] [GP: 1.9920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 108/200] [Batch   0/35] [D: 2.7016] [G: 2.0732] [FM: 3.6561] [GP: 1.9898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 109/200] [Batch   0/35] [D: 2.8115] [G: 2.1751] [FM: 3.8513] [GP: 1.9945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 110/200] [Batch   0/35] [D: 2.7201] [G: 2.2862] [FM: 4.2062] [GP: 1.9986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 111/200] [Batch   0/35] [D: 2.7150] [G: 2.4149] [FM: 3.0039] [GP: 1.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 112/200] [Batch   0/35] [D: 2.7312] [G: 2.3560] [FM: 3.9893] [GP: 1.9947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 113/200] [Batch   0/35] [D: 2.6798] [G: 2.2858] [FM: 3.3356] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 114/200] [Batch   0/35] [D: 2.7069] [G: 1.9548] [FM: 4.4964] [GP: 1.9920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 115/200] [Batch   0/35] [D: 2.8657] [G: 2.1801] [FM: 4.6582] [GP: 1.9924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 116/200] [Batch   0/35] [D: 2.7327] [G: 1.9576] [FM: 4.0840] [GP: 1.9981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 117/200] [Batch   0/35] [D: 2.8639] [G: 2.2405] [FM: 4.2763] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 118/200] [Batch   0/35] [D: 2.7234] [G: 2.3776] [FM: 4.1355] [GP: 1.9983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 119/200] [Batch   0/35] [D: 2.7326] [G: 2.1908] [FM: 2.8370] [GP: 1.9977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 120/200] [Batch   0/35] [D: 2.7131] [G: 2.4973] [FM: 3.3694] [GP: 1.9946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 121/200] [Batch   0/35] [D: 2.7214] [G: 2.2792] [FM: 3.1587] [GP: 1.9879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 121 Summary ===============\n",
      "📈 Avg Generator Loss: 2.3248\n",
      "📉 Avg Discriminator Loss: 0.4001\n",
      "🎯 Avg Feature Matching Loss: 3.3161\n",
      "⚖️  Avg Gradient Penalty: 2.1131\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: -0.0015\n",
      "   • Std: 0.3888\n",
      "   • Range: 2.0000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 122/200] [Batch   0/35] [D: 2.7208] [G: 2.2761] [FM: 3.2115] [GP: 1.9974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 123/200] [Batch   0/35] [D: 2.7372] [G: 2.2926] [FM: 3.8065] [GP: 1.9898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 124/200] [Batch   0/35] [D: 2.7825] [G: 2.1719] [FM: 3.0598] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 125/200] [Batch   0/35] [D: 2.6702] [G: 2.1271] [FM: 3.3433] [GP: 1.9906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 126/200] [Batch   0/35] [D: 2.7071] [G: 2.3735] [FM: 4.8888] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 127/200] [Batch   0/35] [D: 2.6889] [G: 2.3878] [FM: 4.6703] [GP: 1.9911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 128/200] [Batch   0/35] [D: 2.6922] [G: 2.3717] [FM: 3.9571] [GP: 1.9926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 129/200] [Batch   0/35] [D: 2.7736] [G: 2.0853] [FM: 2.7631] [GP: 1.9831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 130/200] [Batch   0/35] [D: 2.6964] [G: 2.0750] [FM: 4.1935] [GP: 1.9848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 131/200] [Batch   0/35] [D: 2.7400] [G: 2.1829] [FM: 3.7818] [GP: 1.9928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 132/200] [Batch   0/35] [D: 2.6258] [G: 2.3028] [FM: 3.6746] [GP: 1.9911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 133/200] [Batch   0/35] [D: 2.7314] [G: 2.1717] [FM: 4.4121] [GP: 1.9986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 134/200] [Batch   0/35] [D: 2.7009] [G: 2.1920] [FM: 2.8886] [GP: 1.9964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 135/200] [Batch   0/35] [D: 2.7762] [G: 2.4133] [FM: 3.1368] [GP: 1.9984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 136/200] [Batch   0/35] [D: 2.6721] [G: 2.2947] [FM: 5.2851] [GP: 1.9982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 137/200] [Batch   0/35] [D: 2.7723] [G: 2.2329] [FM: 4.2340] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 138/200] [Batch   0/35] [D: 2.6834] [G: 2.3135] [FM: 3.7136] [GP: 1.9919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 139/200] [Batch   0/35] [D: 2.7621] [G: 2.3538] [FM: 4.7616] [GP: 1.9988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 140/200] [Batch   0/35] [D: 2.9321] [G: 2.3197] [FM: 3.3324] [GP: 1.9940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 141/200] [Batch   0/35] [D: 2.7324] [G: 2.4023] [FM: 4.6630] [GP: 1.9995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 141 Summary ===============\n",
      "📈 Avg Generator Loss: 2.1904\n",
      "📉 Avg Discriminator Loss: 0.3898\n",
      "🎯 Avg Feature Matching Loss: 3.7627\n",
      "⚖️  Avg Gradient Penalty: 2.1128\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: 0.0011\n",
      "   • Std: 0.4144\n",
      "   • Range: 2.0000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 142/200] [Batch   0/35] [D: 2.7386] [G: 2.1745] [FM: 3.6050] [GP: 1.9957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 143/200] [Batch   0/35] [D: 2.6795] [G: 2.1941] [FM: 4.4074] [GP: 1.9932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 144/200] [Batch   0/35] [D: 2.7776] [G: 2.2714] [FM: 3.8751] [GP: 1.9978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 145/200] [Batch   0/35] [D: 2.7204] [G: 2.4014] [FM: 4.8228] [GP: 1.9851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 146/200] [Batch   0/35] [D: 2.7867] [G: 2.1695] [FM: 3.9131] [GP: 1.9922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 147/200] [Batch   0/35] [D: 2.7617] [G: 2.2981] [FM: 3.9737] [GP: 1.9961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 148/200] [Batch   0/35] [D: 3.0064] [G: 2.3855] [FM: 3.0736] [GP: 1.9988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 149/200] [Batch   0/35] [D: 2.6316] [G: 2.1570] [FM: 3.6829] [GP: 1.9925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 150/200] [Batch   0/35] [D: 2.7333] [G: 2.1827] [FM: 3.0430] [GP: 1.9898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 151/200] [Batch   0/35] [D: 2.7831] [G: 2.2289] [FM: 3.6311] [GP: 1.9940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 152/200] [Batch   0/35] [D: 2.6977] [G: 2.3358] [FM: 3.6176] [GP: 1.9885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 153/200] [Batch   0/35] [D: 2.7344] [G: 2.2497] [FM: 5.4837] [GP: 1.9989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 154/200] [Batch   0/35] [D: 2.7076] [G: 2.2989] [FM: 3.3865] [GP: 1.9870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 155/200] [Batch   0/35] [D: 2.7221] [G: 2.4640] [FM: 5.0106] [GP: 1.9959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 156/200] [Batch   0/35] [D: 2.7577] [G: 2.2039] [FM: 3.7936] [GP: 1.9936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 157/200] [Batch   0/35] [D: 2.7375] [G: 2.3522] [FM: 5.5713] [GP: 1.9891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 158/200] [Batch   0/35] [D: 2.7586] [G: 2.3968] [FM: 4.1489] [GP: 1.9869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 159/200] [Batch   0/35] [D: 2.7411] [G: 2.4650] [FM: 4.0833] [GP: 1.9972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 160/200] [Batch   0/35] [D: 2.7386] [G: 2.2423] [FM: 3.6460] [GP: 1.9944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 161/200] [Batch   0/35] [D: 2.6932] [G: 2.3217] [FM: 4.5475] [GP: 1.9963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 161 Summary ===============\n",
      "📈 Avg Generator Loss: 2.3176\n",
      "📉 Avg Discriminator Loss: 0.3779\n",
      "🎯 Avg Feature Matching Loss: 4.6619\n",
      "⚖️  Avg Gradient Penalty: 2.1136\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: -0.0120\n",
      "   • Std: 0.3898\n",
      "   • Range: 1.9995\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 162/200] [Batch   0/35] [D: 2.6986] [G: 2.2939] [FM: 5.3471] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 163/200] [Batch   0/35] [D: 2.7658] [G: 2.2095] [FM: 3.6444] [GP: 1.9991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 164/200] [Batch   0/35] [D: 2.8001] [G: 2.2237] [FM: 3.9249] [GP: 1.9984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 165/200] [Batch   0/35] [D: 2.7395] [G: 2.2309] [FM: 3.4608] [GP: 1.9954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 166/200] [Batch   0/35] [D: 2.7294] [G: 2.3213] [FM: 4.7094] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 167/200] [Batch   0/35] [D: 2.6990] [G: 2.2714] [FM: 3.1309] [GP: 1.9907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 168/200] [Batch   0/35] [D: 2.7731] [G: 2.2047] [FM: 4.1028] [GP: 1.9988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 169/200] [Batch   0/35] [D: 2.7346] [G: 2.1461] [FM: 3.5836] [GP: 1.9965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 170/200] [Batch   0/35] [D: 2.7140] [G: 2.2549] [FM: 4.4626] [GP: 1.9981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 171/200] [Batch   0/35] [D: 2.7305] [G: 2.3472] [FM: 5.6257] [GP: 1.9959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 172/200] [Batch   0/35] [D: 2.7225] [G: 2.3296] [FM: 5.2217] [GP: 1.9945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 173/200] [Batch   0/35] [D: 2.6590] [G: 2.3998] [FM: 3.8723] [GP: 1.9952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 174/200] [Batch   0/35] [D: 2.7120] [G: 2.3268] [FM: 4.4978] [GP: 1.9898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 175/200] [Batch   0/35] [D: 2.7017] [G: 2.1598] [FM: 3.4721] [GP: 1.9974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 176/200] [Batch   0/35] [D: 2.7504] [G: 2.2175] [FM: 4.2368] [GP: 1.9883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 177/200] [Batch   0/35] [D: 2.6843] [G: 2.2812] [FM: 4.8363] [GP: 1.9956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 178/200] [Batch   0/35] [D: 2.7144] [G: 2.2902] [FM: 4.0480] [GP: 1.9975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 179/200] [Batch   0/35] [D: 2.6565] [G: 2.2193] [FM: 4.6675] [GP: 1.9859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 180/200] [Batch   0/35] [D: 2.6974] [G: 2.3910] [FM: 4.4958] [GP: 1.9904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 181/200] [Batch   0/35] [D: 2.7382] [G: 2.2580] [FM: 5.3930] [GP: 1.9973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== Epoch 181 Summary ===============\n",
      "📈 Avg Generator Loss: 2.2863\n",
      "📉 Avg Discriminator Loss: 0.3902\n",
      "🎯 Avg Feature Matching Loss: 4.4508\n",
      "⚖️  Avg Gradient Penalty: 2.1129\n",
      "📊 Generated Sample Stats:\n",
      "   • Mean: 0.0035\n",
      "   • Std: 0.4118\n",
      "   • Range: 1.9999\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 182/200] [Batch   0/35] [D: 2.7441] [G: 2.2520] [FM: 3.8900] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 183/200] [Batch   0/35] [D: 2.7227] [G: 2.2235] [FM: 4.3666] [GP: 1.9959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 184/200] [Batch   0/35] [D: 2.7396] [G: 2.1903] [FM: 4.5416] [GP: 1.9977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 185/200] [Batch   0/35] [D: 2.6680] [G: 2.3767] [FM: 3.4445] [GP: 1.9900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 186/200] [Batch   0/35] [D: 2.7828] [G: 2.4320] [FM: 4.1014] [GP: 1.9969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 187/200] [Batch   0/35] [D: 2.6831] [G: 2.2673] [FM: 3.8106] [GP: 1.9898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 188/200] [Batch   0/35] [D: 2.6389] [G: 2.5620] [FM: 4.6901] [GP: 1.9969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 189/200] [Batch   0/35] [D: 2.7318] [G: 2.4190] [FM: 4.4285] [GP: 1.9976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 190/200] [Batch   0/35] [D: 2.6998] [G: 2.5630] [FM: 2.4715] [GP: 1.9985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 191/200] [Batch   0/35] [D: 2.7513] [G: 2.1595] [FM: 3.3282] [GP: 1.9965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 192/200] [Batch   0/35] [D: 2.7261] [G: 2.0831] [FM: 3.1040] [GP: 1.9951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 193/200] [Batch   0/35] [D: 2.7846] [G: 2.2151] [FM: 3.8590] [GP: 1.9992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 194/200] [Batch   0/35] [D: 3.0544] [G: 2.2634] [FM: 3.0034] [GP: 1.9990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 195/200] [Batch   0/35] [D: 2.6410] [G: 2.4290] [FM: 4.5837] [GP: 1.9994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 196/200] [Batch   0/35] [D: 2.7605] [G: 2.3257] [FM: 4.7828] [GP: 1.9957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 197/200] [Batch   0/35] [D: 2.6994] [G: 2.2838] [FM: 2.6765] [GP: 1.9872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 198/200] [Batch   0/35] [D: 2.7324] [G: 2.1999] [FM: 3.1639] [GP: 1.9899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 199/200] [Batch   0/35] [D: 2.7298] [G: 2.4099] [FM: 3.6510] [GP: 1.9914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 200/200] [Batch   0/35] [D: 2.7092] [G: 2.3408] [FM: 3.8412] [GP: 1.9955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated data shape: (552, 4500, 14)\n",
      "📊 Original data shape: (552, 4500, 14)\n"
     ]
    }
   ],
   "source": [
    "# Train with optimized parameters for better FID score\n",
    "generator, discriminator, g_loss, d_loss, feature_loss, gp_loss = train_improved_few_shot_gan(\n",
    "    X_train_normal, \n",
    "    epochs=200, \n",
    "    batch_size=16,      # Reduced from 32 for better stability\n",
    "    latent_dim=128,\n",
    "    lr_g=0.0002,        # Reduced from 0.0003\n",
    "    lr_d=0.0001,        # Keep same\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Generate samples with correct function\n",
    "generated_data = generate_samples(generator, len(X_train_normal), latent_dim=128)\n",
    "\n",
    "print(f\"✅ Generated data shape: {generated_data.shape}\")\n",
    "print(f\"📊 Original data shape: {X_train_normal.shape}\")\n",
    "\n",
    "normal_combine = np.concatenate((X_train_normal, generated_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d120bc3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:49:05.484946Z",
     "iopub.status.busy": "2025-07-13T07:49:05.484509Z",
     "iopub.status.idle": "2025-07-13T07:49:05.489764Z",
     "shell.execute_reply": "2025-07-13T07:49:05.488895Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# # Test the simplified FID calculation\n",
    "# print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# # Use smaller subsets for testing\n",
    "# test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "# test_generated = generated_data[:100]\n",
    "\n",
    "# print(f\"Test real data shape: {test_real.shape}\")\n",
    "# print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# # Calculate FID score\n",
    "# fid_score = calculate_fid_score(\n",
    "#     real_data=test_real,\n",
    "#     fake_data=test_generated,\n",
    "#     device=device,\n",
    "#     sample_rate=1000,\n",
    "# )\n",
    "\n",
    "# if fid_score is not None:\n",
    "#     print(f\"\\n🎉 SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "#     # Interpret the score\n",
    "#     if fid_score < 10:\n",
    "#         quality = \"Excellent\"\n",
    "#     elif fid_score < 25:\n",
    "#         quality = \"Good\"\n",
    "#     elif fid_score < 50:\n",
    "#         quality = \"Fair\"\n",
    "#     elif fid_score < 100:\n",
    "#         quality = \"Poor\"\n",
    "#     else:\n",
    "#         quality = \"Very Poor\"\n",
    "    \n",
    "#     print(f\"Quality Assessment: {quality}\")\n",
    "# else:\n",
    "#     print(\"❌ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a9ad17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:49:05.493226Z",
     "iopub.status.busy": "2025-07-13T07:49:05.492829Z",
     "iopub.status.idle": "2025-07-13T08:27:16.986459Z",
     "shell.execute_reply": "2025-07-13T08:27:16.985136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-fold cross-validation...\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.289003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.115891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.071878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.068603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.052292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9143, Prec=1.0000, Rec=0.5714, F1=0.7273\n",
      "Optimal threshold: 0.039887\n",
      "\n",
      "Fold 2/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.289330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.111455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.070788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.067106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.047838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8571, Prec=1.0000, Rec=0.2857, F1=0.4444\n",
      "Optimal threshold: 0.036746\n",
      "\n",
      "Fold 3/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.289345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.109353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.071064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.061023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.041490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9429, Prec=0.8571, Rec=0.8571, F1=0.8571\n",
      "Optimal threshold: 0.028564\n",
      "\n",
      "Fold 4/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.289061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.114926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.071152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.069969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.068915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9429, Prec=1.0000, Rec=0.7500, F1=0.8571\n",
      "Optimal threshold: 0.053738\n",
      "\n",
      "Fold 5/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.289209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.109296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.071024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.069820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.066777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8857, Prec=1.0000, Rec=0.5000, F1=0.6667\n",
      "Optimal threshold: 0.051545\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "FOLD-BY-FOLD RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Fold   Accuracy   Precision   Recall   F1-Score  Threshold   \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.9143     1.0000      0.5714   0.7273    0.039887    \n",
      "2      0.8571     1.0000      0.2857   0.4444    0.036746    \n",
      "3      0.9429     0.8571      0.8571   0.8571    0.028564    \n",
      "4      0.9429     1.0000      0.7500   0.8571    0.053738    \n",
      "5      0.8857     1.0000      0.5000   0.6667    0.051545    \n",
      "\n",
      "STATISTICAL SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "Metric       Mean     Std      Min      Max      Median  \n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy     0.9086   0.0333   0.8571   0.9429   0.9143  \n",
      "Precision    0.9714   0.0571   0.8571   1.0000   1.0000  \n",
      "Recall       0.5929   0.1990   0.2857   0.8571   0.5714  \n",
      "F1           0.7105   0.1523   0.4444   0.8571   0.7273  \n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "  Mean F1-Score: 0.7105 ± 0.1523\n",
      "  F1-Score Range: [0.4444, 0.8571]\n",
      "  Mean Threshold: 0.042096 ± 0.009396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fold_results': [{'fold': 1,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.05229172855615616,\n",
       "   'optimal_threshold': 0.0398870822456148,\n",
       "   'accuracy': 0.9142857142857143,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.5714285714285714,\n",
       "   'f1': 0.7272727272727273},\n",
       "  {'fold': 2,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.0478384343907237,\n",
       "   'optimal_threshold': 0.036745875057849015,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.2857142857142857,\n",
       "   'f1': 0.4444444444444445},\n",
       "  {'fold': 3,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.04148967862129212,\n",
       "   'optimal_threshold': 0.028564112505527456,\n",
       "   'accuracy': 0.9428571428571428,\n",
       "   'precision': 0.8571428571428571,\n",
       "   'recall': 0.8571428571428571,\n",
       "   'f1': 0.8571428571428571},\n",
       "  {'fold': 4,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.06891531608998776,\n",
       "   'optimal_threshold': 0.053738499681154885,\n",
       "   'accuracy': 0.9428571428571428,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.75,\n",
       "   'f1': 0.8571428571428571},\n",
       "  {'fold': 5,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.06677719037979842,\n",
       "   'optimal_threshold': 0.0515447197989984,\n",
       "   'accuracy': 0.8857142857142857,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.5,\n",
       "   'f1': 0.6666666666666666}],\n",
       " 'statistics': {'accuracy': {'mean': 0.9085714285714286,\n",
       "   'std': 0.03331972511340173,\n",
       "   'min': 0.8571428571428571,\n",
       "   'max': 0.9428571428571428,\n",
       "   'median': 0.9142857142857143,\n",
       "   'values': array([0.91428571, 0.85714286, 0.94285714, 0.94285714, 0.88571429])},\n",
       "  'precision': {'mean': 0.9714285714285715,\n",
       "   'std': 0.05714285714285716,\n",
       "   'min': 0.8571428571428571,\n",
       "   'max': 1.0,\n",
       "   'median': 1.0,\n",
       "   'values': array([1.        , 1.        , 0.85714286, 1.        , 1.        ])},\n",
       "  'recall': {'mean': 0.5928571428571429,\n",
       "   'std': 0.19897697538834455,\n",
       "   'min': 0.2857142857142857,\n",
       "   'max': 0.8571428571428571,\n",
       "   'median': 0.5714285714285714,\n",
       "   'values': array([0.57142857, 0.28571429, 0.85714286, 0.75      , 0.5       ])},\n",
       "  'f1': {'mean': 0.7105339105339106,\n",
       "   'std': 0.1523131599604387,\n",
       "   'min': 0.4444444444444445,\n",
       "   'max': 0.8571428571428571,\n",
       "   'median': 0.7272727272727273,\n",
       "   'values': array([0.72727273, 0.44444444, 0.85714286, 0.85714286, 0.66666667])},\n",
       "  'optimal_threshold': {'mean': 0.042096057857828904,\n",
       "   'std': 0.009396182611616963,\n",
       "   'min': 0.028564112505527456,\n",
       "   'max': 0.053738499681154885,\n",
       "   'median': 0.0398870822456148,\n",
       "   'values': array([0.03988708, 0.03674588, 0.02856411, 0.0537385 , 0.05154472])},\n",
       "  'train_loss': {'mean': 0.05546246960759162,\n",
       "   'std': 0.010699723214943824,\n",
       "   'min': 0.04148967862129212,\n",
       "   'max': 0.06891531608998776,\n",
       "   'median': 0.05229172855615616,\n",
       "   'values': array([0.05229173, 0.04783843, 0.04148968, 0.06891532, 0.06677719])}},\n",
       " 'all_metrics': {'accuracy': [0.9142857142857143,\n",
       "   0.8571428571428571,\n",
       "   0.9428571428571428,\n",
       "   0.9428571428571428,\n",
       "   0.8857142857142857],\n",
       "  'precision': [1.0, 1.0, 0.8571428571428571, 1.0, 1.0],\n",
       "  'recall': [0.5714285714285714,\n",
       "   0.2857142857142857,\n",
       "   0.8571428571428571,\n",
       "   0.75,\n",
       "   0.5],\n",
       "  'f1': [0.7272727272727273,\n",
       "   0.4444444444444445,\n",
       "   0.8571428571428571,\n",
       "   0.8571428571428571,\n",
       "   0.6666666666666666],\n",
       "  'optimal_threshold': [0.0398870822456148,\n",
       "   0.036745875057849015,\n",
       "   0.028564112505527456,\n",
       "   0.053738499681154885,\n",
       "   0.0515447197989984],\n",
       "  'train_loss': [0.05229172855615616,\n",
       "   0.0478384343907237,\n",
       "   0.04148967862129212,\n",
       "   0.06891531608998776,\n",
       "   0.06677719037979842]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline_with_cv(normal_combine, X_test_normal, X_test_faulty, \n",
    "                     device=device, batch_size=64, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c1261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
