{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA A100-PCIE-40GB\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from torch.autograd import grad\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "normal_indices = np.where(label == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Time GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f3564",
   "metadata": {},
   "source": [
    "## Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.rnn(x)\n",
    "        return self.linear(h)\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.block = RNNBlock(input_dim, hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.block = RNNBlock(hidden_dim, input_dim, num_layers)\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.block(h)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.block = RNNBlock(z_dim, hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.block(z)\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.block = RNNBlock(hidden_dim, hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.block(h)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.block = RNNBlock(hidden_dim, 1, num_layers)\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.block(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5460be",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de699e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def reconstruction_loss(x, x_tilde):\n",
    "    return F.mse_loss(x_tilde, x)\n",
    "\n",
    "def supervised_loss(h, h_hat_supervise):\n",
    "    return F.mse_loss(h_hat_supervise[:, :-1, :], h[:, 1:, :])\n",
    "\n",
    "def generator_loss(y_fake, h_hat_supervise, x, x_hat):\n",
    "    g_loss_u = torch.mean(torch.nn.functional.binary_cross_entropy_with_logits(y_fake, torch.ones_like(y_fake)))\n",
    "    g_loss_s = supervised_loss(h_hat_supervise, h_hat_supervise)\n",
    "    g_loss_v = reconstruction_loss(x, x_hat)\n",
    "    return g_loss_u + g_loss_s + 100 * torch.sqrt(g_loss_v)\n",
    "\n",
    "def discriminator_loss(y_real, y_fake):\n",
    "    d_loss_real = F.binary_cross_entropy_with_logits(y_real, torch.ones_like(y_real))\n",
    "    d_loss_fake = F.binary_cross_entropy_with_logits(y_fake, torch.zeros_like(y_fake))\n",
    "    return d_loss_real + d_loss_fake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327c49a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_timegan(data, model_dict, optimizer_dict, epochs):\n",
    "    embedder, recovery, generator, supervisor, discriminator = model_dict.values()\n",
    "    e_opt, g_opt, d_opt = optimizer_dict.values()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Phase 1: Embedder training\n",
    "        h = embedder(data)\n",
    "        x_tilde = recovery(h)\n",
    "        e_loss = reconstruction_loss(data, x_tilde)\n",
    "        e_opt.zero_grad()\n",
    "        e_loss.backward()\n",
    "        e_opt.step()\n",
    "\n",
    "        # Phase 2: Supervisor training\n",
    "        h = embedder(data)\n",
    "        h_hat_supervise = supervisor(h)\n",
    "        s_loss = supervised_loss(h, h_hat_supervise)\n",
    "        g_opt.zero_grad()\n",
    "        s_loss.backward()\n",
    "        g_opt.step()\n",
    "\n",
    "        # Phase 3: Joint training\n",
    "        z = torch.randn_like(data)\n",
    "        e_h = embedder(data)\n",
    "        h_hat = generator(z)\n",
    "        h_hat_supervise = supervisor(h_hat)\n",
    "        x_hat = recovery(h_hat_supervise)\n",
    "        y_fake = discriminator(h_hat_supervise)\n",
    "        y_real = discriminator(e_h)\n",
    "\n",
    "        g_loss = generator_loss(y_fake, h_hat_supervise, data, x_hat)\n",
    "        d_loss = discriminator_loss(y_real, y_fake)\n",
    "\n",
    "        g_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "\n",
    "        d_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "\n",
    "        print(f\"[{epoch}] E_loss: {e_loss.item():.4f}, G_loss: {g_loss.item():.4f}, D_loss: {d_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa034590",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 32, got 24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Dummy data for illustration\u001b[39;00m\n\u001b[1;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m24\u001b[39m, input_dim))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_timegan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtrain_timegan\u001b[0;34m(data, model_dict, optimizer_dict, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Phase 1: Embedder training\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     h \u001b[38;5;241m=\u001b[39m embedder(data)\n\u001b[0;32m----> 8\u001b[0m     x_tilde \u001b[38;5;241m=\u001b[39m \u001b[43mrecovery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     e_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss(data, x_tilde)\n\u001b[1;32m     10\u001b[0m     e_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mRecovery.forward\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, h):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mRNNBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(h)\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:1137\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m-> 1137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1140\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:280\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/GANs/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 32, got 24"
     ]
    }
   ],
   "source": [
    "input_dim = 24\n",
    "hidden_dim = 32\n",
    "num_layers = 3\n",
    "epochs = 1000\n",
    "\n",
    "embedder = Embedder(input_dim, hidden_dim, num_layers)\n",
    "recovery = Recovery(hidden_dim, input_dim, num_layers)\n",
    "generator = Generator(input_dim, hidden_dim, num_layers)\n",
    "supervisor = Supervisor(hidden_dim, num_layers)\n",
    "discriminator = Discriminator(hidden_dim, num_layers)\n",
    "\n",
    "model_dict = {\n",
    "    'embedder': embedder,\n",
    "    'recovery': recovery,\n",
    "    'generator': generator,\n",
    "    'supervisor': supervisor,\n",
    "    'discriminator': discriminator\n",
    "}\n",
    "\n",
    "optimizer_dict = {\n",
    "    'embedder': torch.optim.Adam(embedder.parameters(), lr=1e-3),\n",
    "    'generator': torch.optim.Adam(list(generator.parameters()) + list(supervisor.parameters()), lr=1e-3),\n",
    "    'discriminator': torch.optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "}\n",
    "\n",
    "# Dummy data for illustration\n",
    "data = torch.rand((64, 24, input_dim))\n",
    "print(data.shape)\n",
    "train_timegan(data, model_dict, optimizer_dict, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ee542",
   "metadata": {},
   "source": [
    "# Generate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_data = np.concatenate((generated_samples, data), axis=0)  # Combine real and generated data\n",
    "# combine_labels = np.concatenate((np.zeros(num_samples), label), axis=0)  # Labels: 0 for real, 0 for generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901a15",
   "metadata": {},
   "source": [
    "# Processing: Mel Spec > Resizing > Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d5682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01061dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and convert to 3-channel image\n",
    "def resize_spectrogram(spectrogram):\n",
    "    spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-6)\n",
    "    spectrogram = np.uint8(spectrogram.cpu().numpy() * 255)\n",
    "    spectrogram = np.stack([spectrogram] * 3, axis=-1)\n",
    "    image = Image.fromarray(spectrogram)\n",
    "    image = transforms.Resize((224, 224))(image)\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "# Process dataset\n",
    "def process_dataset(data):\n",
    "    num_samples, _, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, num_channels, 4096))\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=2500000, n_mels=128).to(device)\n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            img = resize_spectrogram(mel)\n",
    "            with torch.no_grad():\n",
    "                feat = model(img.unsqueeze(0).to(device))\n",
    "            features[i, j, :] = feat.squeeze().cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ecf3b",
   "metadata": {},
   "source": [
    "# Mel Scale comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19f39ba8",
   "metadata": {},
   "source": [
    "# AE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "# Train autoencoder\n",
    "def train_autoencoder(features, epochs=20, batch_size=128):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_eval(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))\n",
    "  print(\"F1 = {}\".format(f1_score(labels, predictions)))\n",
    "  print(confusion_matrix(labels, predictions))\n",
    "\n",
    "# Plot reconstruction error histogram\n",
    "def plot_reconstruction_error(model, features, percentile=95):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=64)\n",
    "    errors = []\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            batch_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            errors.extend(batch_errors.cpu().numpy())\n",
    "\n",
    "    threshold = np.percentile(errors, percentile)\n",
    "    anomalies = np.sum(np.array(errors) > threshold)\n",
    "\n",
    "    plt.hist(errors, bins=50, alpha=0.75)\n",
    "    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({percentile}%)')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reconstruction Error Histogram')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "    print(f\"Detected anomalies: {anomalies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae6f0a",
   "metadata": {},
   "source": [
    "# Cross Validation without Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "features = process_dataset(combine_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64ca03",
   "metadata": {},
   "source": [
    "# Cross Validation with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = StandardScaler().fit_transform(combine_data.reshape(-1, data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e602f",
   "metadata": {},
   "source": [
    "# Cross Validation with MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = MinMaxScaler().fit_transform(combine_data.reshape(-1, combine_data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25980ecb",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Comparing with and without normalizing data \n",
    "\n",
    "### MinMaxed scored\n",
    "\n",
    "Accuracy = 0.8461538461538461\n",
    "\n",
    "Precision = 0.3125\n",
    "\n",
    "Recall = 0.2777777777777778\n",
    "\n",
    "F1 = 0.29411764705882354\n",
    "\n",
    "[[254  22]\n",
    "\n",
    "[ 26  10]]\n",
    "\n",
    "---\n",
    "\n",
    "### StandardScaled scored\n",
    "\n",
    "\n",
    "Accuracy = 0.782051282051282\n",
    "\n",
    "Precision = 0.0\n",
    "\n",
    "Recall = 0.0\n",
    "\n",
    "F1 = 0.0\n",
    "\n",
    "[[244  32]\n",
    "\n",
    "[ 36   0]]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Without any normlaization scored\n",
    "\n",
    "Accuracy = 0.782051282051282\n",
    "\n",
    "Precision = 0.0\n",
    "\n",
    "Recall = 0.0\n",
    "\n",
    "F1 = 0.0\n",
    "\n",
    "[[244  32]\n",
    " \n",
    "[ 36   0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5a291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
