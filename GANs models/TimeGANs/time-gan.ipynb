{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:42:01.505056Z",
     "iopub.status.busy": "2025-07-01T04:42:01.504584Z",
     "iopub.status.idle": "2025-07-01T04:42:08.391775Z",
     "shell.execute_reply": "2025-07-01T04:42:08.391009Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Time GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f71ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:42:08.397394Z",
     "iopub.status.busy": "2025-07-01T04:42:08.396715Z",
     "iopub.status.idle": "2025-07-01T04:42:08.428118Z",
     "shell.execute_reply": "2025-07-01T04:42:08.427362Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"\"\"Enhanced Embedding network optimized for anomaly detection time series.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Multi-scale feature extraction for better temporal patterns\n",
    "        self.conv1d = nn.Conv1d(input_dim, hidden_dim // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Bidirectional LSTM for better temporal understanding\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim // 2, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for important feature selection\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2, \n",
    "            num_heads=4, \n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layer to desired hidden dimension\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass for embedding with attention mechanism.\"\"\"\n",
    "        batch_size, seq_len, input_dim = X.shape\n",
    "        \n",
    "        # Conv1D for local pattern extraction\n",
    "        X_conv = self.conv1d(X.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # LSTM for temporal dynamics\n",
    "        H_lstm, _ = self.lstm(X_conv)\n",
    "        \n",
    "        # Self-attention for important pattern focus\n",
    "        H_att, _ = self.attention(H_lstm, H_lstm, H_lstm)\n",
    "        \n",
    "        # Project to final embedding\n",
    "        H = self.projection(H_att)\n",
    "        \n",
    "        return H\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    \"\"\"Enhanced Recovery network with residual connections for better reconstruction.\"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM for reconstruction\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Multi-layer reconstruction with residual connections\n",
    "        self.recovery_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.LayerNorm(hidden_dim // 2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.1)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Final reconstruction layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Tanh()  # Bounded output for stability\n",
    "        )\n",
    "        \n",
    "    def forward(self, H):\n",
    "        \"\"\"Forward pass for recovery with residual connections.\"\"\"\n",
    "        # LSTM processing\n",
    "        H_lstm, _ = self.lstm(H)\n",
    "        \n",
    "        # Progressive reconstruction with residuals\n",
    "        x = H_lstm\n",
    "        for layer in self.recovery_layers:\n",
    "            residual = x\n",
    "            x = layer(x)\n",
    "            # Add residual connection where dimensions match\n",
    "            if x.shape[-1] == residual.shape[-1]:\n",
    "                x = x + residual * 0.1  # Scaled residual\n",
    "        \n",
    "        # Final output\n",
    "        X_tilde = self.output_layer(x)\n",
    "        return X_tilde\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Enhanced Generator with noise injection and temporal consistency.\"\"\"\n",
    "    def __init__(self, z_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initial noise transformation\n",
    "        self.noise_transform = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM for better generation\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Temporal consistency layers\n",
    "        self.temporal_conv = nn.Conv1d(\n",
    "            hidden_dim * 2, hidden_dim, \n",
    "            kernel_size=3, padding=1\n",
    "        )\n",
    "        \n",
    "        # Final generation layer with progressive refinement\n",
    "        self.generation_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, Z):\n",
    "        \"\"\"Forward pass for generator with temporal consistency.\"\"\"\n",
    "        # Transform noise\n",
    "        Z_transformed = self.noise_transform(Z)\n",
    "        \n",
    "        # LSTM generation\n",
    "        H_lstm, _ = self.lstm(Z_transformed)\n",
    "        \n",
    "        # Temporal consistency via convolution\n",
    "        H_conv = self.temporal_conv(H_lstm.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Final generation\n",
    "        H_hat = self.generation_layers(H_conv)\n",
    "        \n",
    "        return H_hat\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    \"\"\"Enhanced Supervisor with temporal prediction for anomaly detection.\"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Multi-step prediction LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Temporal prediction with attention\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Prediction refinement\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, H):\n",
    "        \"\"\"Forward pass for supervisor with temporal prediction.\"\"\"\n",
    "        # LSTM for sequence modeling\n",
    "        H_lstm, _ = self.lstm(H)\n",
    "        \n",
    "        # Attention for temporal dependencies\n",
    "        H_att, _ = self.temporal_attention(H_lstm, H_lstm, H_lstm)\n",
    "        \n",
    "        # Final prediction\n",
    "        H_hat_supervise = self.prediction_head(H_att)\n",
    "        \n",
    "        return H_hat_supervise\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Enhanced Discriminator with multi-scale analysis for anomaly detection.\"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Multi-scale temporal analysis\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=k, padding=k//2)\n",
    "            for k in [3, 5, 7]  # Different temporal scales\n",
    "        ])\n",
    "        \n",
    "        # Main LSTM discriminator\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim * 3,  # Concatenated multi-scale features\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers-1 if num_layers > 1 else 1,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Feature attention for important pattern focus\n",
    "        self.feature_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=2,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Progressive discrimination\n",
    "        self.discriminator_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, H):\n",
    "        \"\"\"Forward pass for discriminator with multi-scale analysis.\"\"\"\n",
    "        # Multi-scale convolution analysis\n",
    "        H_transpose = H.transpose(1, 2)  # For conv1d\n",
    "        multi_scale_features = []\n",
    "        \n",
    "        for conv in self.conv_layers:\n",
    "            conv_out = torch.relu(conv(H_transpose))\n",
    "            multi_scale_features.append(conv_out)\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        H_multi = torch.cat(multi_scale_features, dim=1).transpose(1, 2)\n",
    "        \n",
    "        # LSTM processing\n",
    "        H_lstm, _ = self.lstm(H_multi)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        H_att, _ = self.feature_attention(H_lstm, H_lstm, H_lstm)\n",
    "        \n",
    "        # Final discrimination\n",
    "        Y = self.discriminator_head(H_att)\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd7a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:42:08.432014Z",
     "iopub.status.busy": "2025-07-01T04:42:08.431711Z",
     "iopub.status.idle": "2025-07-01T04:42:08.452688Z",
     "shell.execute_reply": "2025-07-01T04:42:08.451872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add this chunking function after your imports\n",
    "def chunk_sequences(data, chunk_size=100, overlap=10):\n",
    "    \"\"\"\n",
    "    Split long sequences into smaller chunks\n",
    "    \n",
    "    Args:\n",
    "        data: shape [n_samples, seq_len, features] = (690, 4500, 14)\n",
    "        chunk_size: size of each chunk\n",
    "        overlap: overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        chunked_data: shape [n_chunks, chunk_size, features]\n",
    "    \"\"\"\n",
    "    n_samples, seq_len, n_features = data.shape\n",
    "    chunks = []\n",
    "    \n",
    "    for sample in data:\n",
    "        # Create chunks with overlap\n",
    "        for start in range(0, seq_len - chunk_size + 1, chunk_size - overlap):\n",
    "            end = start + chunk_size\n",
    "            if end <= seq_len:\n",
    "                chunks.append(sample[start:end])\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Enhanced chunking for better anomaly detection\n",
    "def chunk_sequences_enhanced(data, chunk_size=150, overlap=20):\n",
    "    \"\"\"\n",
    "    Enhanced chunking with better overlap for anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        data: shape [n_samples, seq_len, features]\n",
    "        chunk_size: larger chunks for better context\n",
    "        overlap: more overlap for continuity\n",
    "    \n",
    "    Returns:\n",
    "        chunked_data: shape [n_chunks, chunk_size, features]\n",
    "    \"\"\"\n",
    "    n_samples, seq_len, n_features = data.shape\n",
    "    chunks = []\n",
    "    \n",
    "    for sample in data:\n",
    "        # Create chunks with strategic overlap\n",
    "        for start in range(0, seq_len - chunk_size + 1, chunk_size - overlap):\n",
    "            end = start + chunk_size\n",
    "            if end <= seq_len:\n",
    "                chunks.append(sample[start:end])\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Update your loss functions to be more stable\n",
    "def embedding_loss(X, X_tilde):\n",
    "    \"\"\"\n",
    "    Robust reconstruction loss using relative error\n",
    "    \"\"\"\n",
    "    # Use relative L1 loss to handle large values\n",
    "    return torch.mean(torch.abs(X - X_tilde) / (torch.abs(X) + 1e-6))\n",
    "\n",
    "\n",
    "def supervised_loss(H, H_hat_supervise):\n",
    "    \"\"\"\n",
    "    Supervised loss for the supervisor network - with safety check\n",
    "    \"\"\"\n",
    "    if H.size(1) > 1:\n",
    "        return torch.mean(torch.abs(H[:, 1:, :] - H_hat_supervise[:, :-1, :]))\n",
    "    return torch.tensor(0.0, device=H.device)\n",
    "\n",
    "def discriminator_loss(Y_real, Y_fake):\n",
    "    \"\"\"\n",
    "    Discriminator loss using BCE with logits for stability\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    real_loss = criterion(Y_real, torch.ones_like(Y_real))\n",
    "    fake_loss = criterion(Y_fake, torch.zeros_like(Y_fake))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(Y_fake, H, H_hat_supervise, X, X_hat, lambda_sup=1.0, lambda_recon=0.01):\n",
    "    \"\"\"\n",
    "    Generator loss with MUCH lower reconstruction weight for raw data\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Adversarial loss\n",
    "    loss_adv = criterion(Y_fake, torch.ones_like(Y_fake))\n",
    "    \n",
    "    # Supervised loss\n",
    "    loss_sup = supervised_loss(H, H_hat_supervise)\n",
    "    \n",
    "    # Relative reconstruction loss (VERY low weight for raw data)\n",
    "    loss_recon = torch.mean(torch.abs(X - X_hat) / (torch.abs(X) + 1e-6))\n",
    "    \n",
    "    # CRITICAL: Much lower reconstruction weight for raw data\n",
    "    total_loss = loss_adv + lambda_sup * loss_sup + lambda_recon * loss_recon\n",
    "    return total_loss\n",
    "\n",
    "# Enhanced loss functions for anomaly detection\n",
    "def embedding_loss_enhanced(X, X_tilde):\n",
    "    \"\"\"\n",
    "    Multi-objective embedding loss for anomaly detection\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (L1 + L2 combination)\n",
    "    l1_loss = torch.mean(torch.abs(X - X_tilde))\n",
    "    l2_loss = torch.mean((X - X_tilde) ** 2)\n",
    "    \n",
    "    # Frequency domain loss for temporal patterns\n",
    "    X_fft = torch.fft.fft(X, dim=1)\n",
    "    X_tilde_fft = torch.fft.fft(X_tilde, dim=1)\n",
    "    freq_loss = torch.mean(torch.abs(X_fft - X_tilde_fft))\n",
    "    \n",
    "    # Feature correlation preservation\n",
    "    X_corr = torch.corrcoef(X.reshape(-1, X.shape[-1]).T)\n",
    "    X_tilde_corr = torch.corrcoef(X_tilde.reshape(-1, X_tilde.shape[-1]).T)\n",
    "    corr_loss = torch.mean((X_corr - X_tilde_corr) ** 2)\n",
    "    \n",
    "    # Combined loss with weights optimized for anomaly detection\n",
    "    total_loss = 0.4 * l1_loss + 0.3 * l2_loss + 0.2 * freq_loss + 0.1 * corr_loss\n",
    "    return total_loss\n",
    "\n",
    "def supervised_loss_enhanced(H, H_hat_supervise):\n",
    "    \"\"\"\n",
    "    Enhanced supervised loss with temporal consistency\n",
    "    \"\"\"\n",
    "    if H.size(1) <= 1:\n",
    "        return torch.tensor(0.0, device=H.device)\n",
    "    \n",
    "    # Standard supervised loss\n",
    "    base_loss = torch.mean(torch.abs(H[:, 1:, :] - H_hat_supervise[:, :-1, :]))\n",
    "    \n",
    "    # Temporal smoothness constraint\n",
    "    H_diff = H[:, 1:, :] - H[:, :-1, :]\n",
    "    H_hat_diff = H_hat_supervise[:, 1:, :] - H_hat_supervise[:, :-1, :]\n",
    "    smooth_loss = torch.mean(torch.abs(H_diff - H_hat_diff))\n",
    "    \n",
    "    return base_loss + 0.1 * smooth_loss\n",
    "\n",
    "def discriminator_loss_enhanced(Y_real, Y_fake):\n",
    "    \"\"\"\n",
    "    Enhanced discriminator loss with gradient penalty\n",
    "    \"\"\"\n",
    "    # Least squares loss for more stable training\n",
    "    real_loss = torch.mean((Y_real - 1) ** 2)\n",
    "    fake_loss = torch.mean(Y_fake ** 2)\n",
    "    \n",
    "    return (real_loss + fake_loss) / 2\n",
    "\n",
    "def generator_loss_enhanced(Y_fake, H, H_hat_supervise, X, X_hat, \n",
    "                          lambda_sup=2.0, lambda_recon=0.1, lambda_diversity=0.05):\n",
    "    \"\"\"\n",
    "    Enhanced generator loss optimized for anomaly detection\n",
    "    \"\"\"\n",
    "    # Adversarial loss (least squares)\n",
    "    loss_adv = torch.mean((Y_fake - 1) ** 2)\n",
    "    \n",
    "    # Enhanced supervised loss\n",
    "    loss_sup = supervised_loss_enhanced(H, H_hat_supervise)\n",
    "    \n",
    "    # Enhanced reconstruction loss\n",
    "    loss_recon = embedding_loss_enhanced(X, X_hat)\n",
    "    \n",
    "    # Diversity loss to prevent mode collapse\n",
    "    batch_size = Y_fake.shape[0]\n",
    "    if batch_size > 1:\n",
    "        # Encourage diversity in generated samples\n",
    "        H_hat_flat = H_hat_supervise.reshape(batch_size, -1)\n",
    "        pairwise_dist = torch.pdist(H_hat_flat, p=2)\n",
    "        diversity_loss = torch.exp(-pairwise_dist.mean())\n",
    "    else:\n",
    "        diversity_loss = torch.tensor(0.0, device=Y_fake.device)\n",
    "    \n",
    "    # Combined loss with optimized weights for anomaly detection\n",
    "    total_loss = (loss_adv + \n",
    "                 lambda_sup * loss_sup + \n",
    "                 lambda_recon * loss_recon + \n",
    "                 lambda_diversity * diversity_loss)\n",
    "    \n",
    "    return total_loss, {\n",
    "        'adv': loss_adv.item(),\n",
    "        'sup': loss_sup.item(),\n",
    "        'recon': loss_recon.item(),\n",
    "        'div': diversity_loss.item()\n",
    "    }\n",
    "\n",
    "# Quality assessment for generated samples\n",
    "def assess_generation_quality(real_data, synthetic_data):\n",
    "    \"\"\"\n",
    "    Assess quality of generated samples for anomaly detection\n",
    "    \"\"\"\n",
    "    real_mean = np.mean(real_data, axis=(0, 1))\n",
    "    synth_mean = np.mean(synthetic_data, axis=(0, 1))\n",
    "    \n",
    "    real_std = np.std(real_data, axis=(0, 1))\n",
    "    synth_std = np.std(synthetic_data, axis=(0, 1))\n",
    "    \n",
    "    # Statistical similarity\n",
    "    mean_diff = np.mean(np.abs(real_mean - synth_mean))\n",
    "    std_diff = np.mean(np.abs(real_std - synth_std))\n",
    "    \n",
    "    # Temporal correlation preservation\n",
    "    real_corr = np.corrcoef(real_data.reshape(-1, real_data.shape[-1]).T)\n",
    "    synth_corr = np.corrcoef(synthetic_data.reshape(-1, synthetic_data.shape[-1]).T)\n",
    "    corr_diff = np.mean(np.abs(real_corr - synth_corr))\n",
    "    \n",
    "    return {\n",
    "        'mean_difference': mean_diff,\n",
    "        'std_difference': std_diff,\n",
    "        'correlation_difference': corr_diff,\n",
    "        'quality_score': 1.0 / (1.0 + mean_diff + std_diff + corr_diff)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d31132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:42:08.456795Z",
     "iopub.status.busy": "2025-07-01T04:42:08.456480Z",
     "iopub.status.idle": "2025-07-01T04:42:08.490940Z",
     "shell.execute_reply": "2025-07-01T04:42:08.490005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced training function optimized for anomaly detection\n",
    "def train_timegan_enhanced(data, seq_len, batch_size, model_params, train_params):\n",
    "    \"\"\"\n",
    "    Enhanced TimeGAN training specifically optimized for anomaly detection\n",
    "    \"\"\"\n",
    "    # Enhanced chunking with better parameters\n",
    "    chunk_size = seq_len\n",
    "    print(f\"Enhanced chunking sequences into size {chunk_size}...\")\n",
    "    chunked_data = chunk_sequences_enhanced(data, chunk_size=chunk_size, overlap=30)\n",
    "    print(f\"Created {len(chunked_data)} enhanced chunks from {len(data)} original sequences\")\n",
    "    \n",
    "    # Model parameters\n",
    "    input_dim = model_params['input_dim']\n",
    "    hidden_dim = model_params['hidden_dim']\n",
    "    num_layers = model_params['num_layers']\n",
    "    z_dim = model_params['z_dim']\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = train_params['epochs']\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    data_tensor = torch.tensor(chunked_data, dtype=torch.float32)\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Initialize enhanced models\n",
    "    embedder = Embedder(input_dim, hidden_dim, num_layers).to(device)\n",
    "    recovery = Recovery(hidden_dim, input_dim, num_layers).to(device)\n",
    "    generator = Generator(z_dim, hidden_dim, num_layers).to(device)\n",
    "    supervisor = Supervisor(hidden_dim, num_layers).to(device)\n",
    "    discriminator = Discriminator(hidden_dim, num_layers).to(device)\n",
    "    \n",
    "    # Enhanced weight initialization\n",
    "    def enhanced_weights_init(m):\n",
    "        if isinstance(m, nn.LSTM):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(param.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(param.data)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param.data, 0)\n",
    "                    # Set forget gate bias to 1\n",
    "                    n = param.size(0)\n",
    "                    param.data[n//4:n//2].fill_(1.)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "    \n",
    "    # Apply enhanced initialization\n",
    "    for model in [embedder, recovery, generator, supervisor, discriminator]:\n",
    "        model.apply(enhanced_weights_init)\n",
    "    \n",
    "    # Enhanced optimizers with different learning rates\n",
    "    e_optimizer = optim.AdamW(\n",
    "        list(embedder.parameters()) + list(recovery.parameters()), \n",
    "        lr=learning_rate, betas=(0.5, 0.999), weight_decay=1e-5\n",
    "    )\n",
    "    g_optimizer = optim.AdamW(\n",
    "        list(generator.parameters()) + list(supervisor.parameters()), \n",
    "        lr=learning_rate, betas=(0.5, 0.999), weight_decay=1e-5\n",
    "    )\n",
    "    d_optimizer = optim.AdamW(\n",
    "        discriminator.parameters(), \n",
    "        lr=learning_rate * 0.1, betas=(0.5, 0.999), weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    e_scheduler = optim.lr_scheduler.CosineAnnealingLR(e_optimizer, T_max=epochs)\n",
    "    g_scheduler = optim.lr_scheduler.CosineAnnealingLR(g_optimizer, T_max=epochs)\n",
    "    d_scheduler = optim.lr_scheduler.CosineAnnealingLR(d_optimizer, T_max=epochs)\n",
    "    \n",
    "    print('Starting Enhanced TimeGAN Training for Anomaly Detection...')\n",
    "    print(f'Model Parameters: Hidden={hidden_dim}, Layers={num_layers}, Z_dim={z_dim}')\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'embedding_loss': [],\n",
    "        'generator_loss': [],\n",
    "        'discriminator_loss': [],\n",
    "        'quality_scores': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_e_loss = 0\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        epoch_losses_detail = {'adv': 0, 'sup': 0, 'recon': 0, 'div': 0}\n",
    "        \n",
    "        for batch_idx, (X_mb,) in enumerate(dataloader):\n",
    "            X_mb = X_mb.to(device)\n",
    "            batch_size_actual = X_mb.shape[0]\n",
    "            \n",
    "            # Phase 1: Enhanced Embedding Training (every iteration)\n",
    "            embedder.train()\n",
    "            recovery.train()\n",
    "            \n",
    "            H = embedder(X_mb)\n",
    "            X_tilde = recovery(H)\n",
    "            \n",
    "            e_loss = embedding_loss_enhanced(X_mb, X_tilde)\n",
    "            \n",
    "            e_optimizer.zero_grad()\n",
    "            e_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(embedder.parameters()) + list(recovery.parameters()), \n",
    "                max_norm=1.0\n",
    "            )\n",
    "            e_optimizer.step()\n",
    "            \n",
    "            epoch_e_loss += e_loss.item()\n",
    "            \n",
    "            # Phase 2: Enhanced Generator and Discriminator Training\n",
    "            if batch_idx % 3 == 0:  # Train G and D every 3 iterations for stability\n",
    "                \n",
    "                # Generator training with enhanced loss\n",
    "                generator.train()\n",
    "                supervisor.train()\n",
    "                \n",
    "                Z_mb = torch.randn(batch_size_actual, seq_len, z_dim).to(device)\n",
    "                H_hat = generator(Z_mb)\n",
    "                H_hat_supervise = supervisor(H_hat)\n",
    "                X_hat = recovery(H_hat)\n",
    "                \n",
    "                # Get embeddings from real data\n",
    "                with torch.no_grad():\n",
    "                    H_real = embedder(X_mb)\n",
    "                \n",
    "                # Discriminator outputs\n",
    "                Y_fake = discriminator(H_hat)\n",
    "                \n",
    "                # Enhanced generator loss\n",
    "                g_loss, loss_details = generator_loss_enhanced(\n",
    "                    Y_fake, H_real, H_hat_supervise, X_mb, X_hat\n",
    "                )\n",
    "                \n",
    "                g_optimizer.zero_grad()\n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(generator.parameters()) + list(supervisor.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                epoch_g_loss += g_loss.item()\n",
    "                for key in loss_details:\n",
    "                    epoch_losses_detail[key] += loss_details[key]\n",
    "                \n",
    "                # Enhanced Discriminator training\n",
    "                discriminator.train()\n",
    "                \n",
    "                # Generate fresh samples for discriminator\n",
    "                Z_mb_d = torch.randn(batch_size_actual, seq_len, z_dim).to(device)\n",
    "                with torch.no_grad():\n",
    "                    H_hat_d = generator(Z_mb_d)\n",
    "                    H_real_d = embedder(X_mb)\n",
    "                \n",
    "                Y_fake_d = discriminator(H_hat_d.detach())\n",
    "                Y_real_d = discriminator(H_real_d)\n",
    "                \n",
    "                d_loss = discriminator_loss_enhanced(Y_real_d, Y_fake_d)\n",
    "                \n",
    "                d_optimizer.zero_grad()\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                epoch_d_loss += d_loss.item()\n",
    "        \n",
    "        # Update learning rates\n",
    "        e_scheduler.step()\n",
    "        g_scheduler.step()\n",
    "        d_scheduler.step()\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        num_batches = len(dataloader)\n",
    "        g_d_batches = num_batches // 3 if num_batches > 3 else max(1, num_batches)\n",
    "        \n",
    "        avg_e_loss = epoch_e_loss / num_batches\n",
    "        avg_g_loss = epoch_g_loss / g_d_batches\n",
    "        avg_d_loss = epoch_d_loss / g_d_batches\n",
    "        \n",
    "        # Store history\n",
    "        history['embedding_loss'].append(avg_e_loss)\n",
    "        history['generator_loss'].append(avg_g_loss)\n",
    "        history['discriminator_loss'].append(avg_d_loss)\n",
    "        \n",
    "        # Quality assessment every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Generate sample for quality assessment\n",
    "                Z_sample = torch.randn(min(100, len(chunked_data)), seq_len, z_dim).to(device)\n",
    "                generator.eval()\n",
    "                supervisor.eval()\n",
    "                recovery.eval()\n",
    "                \n",
    "                H_sample = generator(Z_sample)\n",
    "                H_sample = supervisor(H_sample)\n",
    "                X_sample = recovery(H_sample)\n",
    "                \n",
    "                sample_real = chunked_data[:min(100, len(chunked_data))]\n",
    "                sample_synth = X_sample.cpu().numpy()\n",
    "                \n",
    "                quality = assess_generation_quality(sample_real, sample_synth)\n",
    "                history['quality_scores'].append(quality['quality_score'])\n",
    "        \n",
    "        # Enhanced progress reporting\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'  Embedding Loss: {avg_e_loss:.4f}')\n",
    "            print(f'  Generator Loss: {avg_g_loss:.4f} (Adv: {epoch_losses_detail[\"adv\"]/g_d_batches:.3f}, '\n",
    "                  f'Sup: {epoch_losses_detail[\"sup\"]/g_d_batches:.3f}, '\n",
    "                  f'Recon: {epoch_losses_detail[\"recon\"]/g_d_batches:.3f})')\n",
    "            print(f'  Discriminator Loss: {avg_d_loss:.4f}')\n",
    "            \n",
    "            # Training stability indicators\n",
    "            if len(history['generator_loss']) > 10:\n",
    "                g_stability = np.std(history['generator_loss'][-10:])\n",
    "                d_stability = np.std(history['discriminator_loss'][-10:])\n",
    "                \n",
    "                if g_stability < 0.1 and d_stability < 0.1:\n",
    "                    print(f'  âœ… Training highly stable')\n",
    "                elif g_stability < 0.5 and d_stability < 0.5:\n",
    "                    print(f'  ðŸ”„ Training moderately stable')\n",
    "                else:\n",
    "                    print(f'  âš ï¸  Training showing variation')\n",
    "            \n",
    "            if len(history['quality_scores']) > 0:\n",
    "                print(f'  Quality Score: {history[\"quality_scores\"][-1]:.4f}')\n",
    "    \n",
    "    print('Enhanced TimeGAN training completed!')\n",
    "    \n",
    "    return {\n",
    "        'embedder': embedder,\n",
    "        'recovery': recovery,\n",
    "        'generator': generator,\n",
    "        'supervisor': supervisor,\n",
    "        'discriminator': discriminator,\n",
    "        'chunk_size': chunk_size,\n",
    "        'original_seq_len': data.shape[1],\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "# Enhanced generation function\n",
    "def generate_timegan_samples_enhanced(model, n_samples, seq_len, z_dim):\n",
    "    \"\"\"\n",
    "    Generate high-quality synthetic samples for anomaly detection\n",
    "    \"\"\"\n",
    "    generator = model['generator']\n",
    "    supervisor = model['supervisor']\n",
    "    recovery = model['recovery']\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    generator.eval()\n",
    "    supervisor.eval()\n",
    "    recovery.eval()\n",
    "    \n",
    "    generated_samples = []\n",
    "    batch_size = 64  # Generate in batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            current_batch_size = min(batch_size, n_samples - i)\n",
    "            \n",
    "            # Generate diverse noise\n",
    "            Z = torch.randn(current_batch_size, seq_len, z_dim).to(device)\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            H_hat = generator(Z)\n",
    "            H_hat = supervisor(H_hat)\n",
    "            X_hat = recovery(H_hat)\n",
    "            \n",
    "            generated_samples.append(X_hat.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(generated_samples, axis=0)\n",
    "\n",
    "def reconstruct_full_sequences_enhanced(chunks, original_length=4500, chunk_size=150, overlap=30):\n",
    "    \"\"\"\n",
    "    Enhanced sequence reconstruction with better overlap handling\n",
    "    \"\"\"\n",
    "    step_size = chunk_size - overlap\n",
    "    chunks_needed = (original_length - overlap) // step_size + 1\n",
    "    \n",
    "    n_full_sequences = len(chunks) // chunks_needed\n",
    "    full_sequences = []\n",
    "    \n",
    "    for i in range(n_full_sequences):\n",
    "        start_idx = i * chunks_needed\n",
    "        end_idx = start_idx + min(chunks_needed, len(chunks) - start_idx)\n",
    "        sequence_chunks = chunks[start_idx:end_idx]\n",
    "        \n",
    "        # Enhanced reconstruction with smooth transitions\n",
    "        reconstructed = np.zeros((original_length, sequence_chunks.shape[2]))\n",
    "        weights = np.zeros(original_length)\n",
    "        \n",
    "        for j, chunk in enumerate(sequence_chunks):\n",
    "            pos = j * step_size\n",
    "            end_pos = min(pos + chunk_size, original_length)\n",
    "            chunk_len = end_pos - pos\n",
    "            \n",
    "            if chunk_len > 0:\n",
    "                # Weighted averaging for smooth transitions\n",
    "                weight = np.ones(chunk_len)\n",
    "                if j > 0:  # Not the first chunk\n",
    "                    weight[:overlap] = np.linspace(0.5, 1.0, overlap)\n",
    "                if j < len(sequence_chunks) - 1:  # Not the last chunk\n",
    "                    weight[-overlap:] = np.linspace(1.0, 0.5, overlap)\n",
    "                \n",
    "                reconstructed[pos:end_pos] += chunk[:chunk_len] * weight[:, np.newaxis]\n",
    "                weights[pos:end_pos] += weight\n",
    "        \n",
    "        # Normalize by weights\n",
    "        weights[weights == 0] = 1  # Avoid division by zero\n",
    "        reconstructed = reconstructed / weights[:, np.newaxis]\n",
    "        \n",
    "        full_sequences.append(reconstructed)\n",
    "    \n",
    "    return np.array(full_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf86b46",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbc319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T04:42:08.495580Z",
     "iopub.status.busy": "2025-07-01T04:42:08.495182Z",
     "iopub.status.idle": "2025-07-01T05:31:00.832687Z",
     "shell.execute_reply": "2025-07-01T05:31:00.831267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced model parameters optimized for anomaly detection\n",
    "chunk_size = 150  # Larger chunks for better context\n",
    "input_dim = data.shape[2]  # 14 features\n",
    "hidden_dim = 64  # Increased for better representation\n",
    "num_layers = 3   # More layers for complex patterns\n",
    "z_dim = input_dim * 2  # Larger latent space\n",
    "seq_len = chunk_size\n",
    "batch_size = 32  # Optimized batch size\n",
    "\n",
    "model_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'z_dim': z_dim\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'epochs': 100,  # More epochs for convergence\n",
    "    'learning_rate': 0.0005  # Optimized learning rate\n",
    "}\n",
    "\n",
    "print(\"Enhanced TimeGAN Configuration for Anomaly Detection:\")\n",
    "print(f\"Chunk Size: {chunk_size} (better temporal context)\")\n",
    "print(f\"Hidden Dimension: {hidden_dim} (enhanced representation)\")\n",
    "print(f\"Number of Layers: {num_layers} (deeper networks)\")\n",
    "print(f\"Latent Dimension: {z_dim} (richer noise space)\")\n",
    "print(f\"Batch Size: {batch_size} (optimized for stability)\")\n",
    "print(f\"Epochs: {train_params['epochs']} (sufficient convergence)\")\n",
    "\n",
    "# Train the Enhanced TimeGAN model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING ENHANCED TIMEGAN TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trained_model = train_timegan_enhanced(X_train_normal, seq_len, batch_size, model_params, train_params)\n",
    "\n",
    "# Generate enhanced synthetic data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING ENHANCED SYNTHETIC DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_samples = len(X_train_normal)\n",
    "n_full_sequences_desired = num_samples\n",
    "\n",
    "# Calculate chunks needed per sequence with enhanced parameters\n",
    "step_size = chunk_size - 30  # overlap = 30\n",
    "chunks_per_sequence = (4500 - 30) // step_size + 1\n",
    "n_synthetic_chunks = n_full_sequences_desired * chunks_per_sequence\n",
    "\n",
    "print(f\"Generating {n_full_sequences_desired} full sequences:\")\n",
    "print(f\"Chunks per sequence: {chunks_per_sequence}\")\n",
    "print(f\"Total chunks needed: {n_synthetic_chunks}\")\n",
    "\n",
    "# Generate enhanced synthetic chunks\n",
    "synthetic_chunks = generate_timegan_samples_enhanced(\n",
    "    trained_model, n_synthetic_chunks, seq_len, z_dim\n",
    ")\n",
    "print(f\"Generated {synthetic_chunks.shape} enhanced synthetic chunks\")\n",
    "\n",
    "# Reconstruct full sequences with enhanced method\n",
    "synthetic_full = reconstruct_full_sequences_enhanced(\n",
    "    synthetic_chunks,\n",
    "    original_length=4500,\n",
    "    chunk_size=chunk_size,\n",
    "    overlap=30\n",
    ")\n",
    "\n",
    "print(f\"Reconstructed {synthetic_full.shape} full enhanced synthetic sequences\")\n",
    "\n",
    "# Quality Assessment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quality_metrics = assess_generation_quality(X_train_normal, synthetic_full)\n",
    "print(\"Enhanced TimeGAN Quality Metrics:\")\n",
    "print(f\"âœ“ Mean Difference: {quality_metrics['mean_difference']:.6f}\")\n",
    "print(f\"âœ“ Std Difference: {quality_metrics['std_difference']:.6f}\")\n",
    "print(f\"âœ“ Correlation Difference: {quality_metrics['correlation_difference']:.6f}\")\n",
    "print(f\"âœ“ Overall Quality Score: {quality_metrics['quality_score']:.6f}\")\n",
    "\n",
    "# Plot training history\n",
    "if 'history' in trained_model:\n",
    "    history = trained_model['history']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Training Losses\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history['embedding_loss'], label='Embedding Loss', color='blue', alpha=0.7)\n",
    "    plt.title('Embedding Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history['generator_loss'], label='Generator Loss', color='red', alpha=0.7)\n",
    "    plt.title('Generator Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history['discriminator_loss'], label='Discriminator Loss', color='green', alpha=0.7)\n",
    "    plt.title('Discriminator Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Quality Scores\n",
    "    if len(history['quality_scores']) > 0:\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.plot(range(0, len(history['embedding_loss']), 10)[:len(history['quality_scores'])], \n",
    "                history['quality_scores'], label='Quality Score', color='purple', alpha=0.7, marker='o')\n",
    "        plt.title('Generation Quality Over Time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Quality Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Data Comparison\n",
    "    plt.subplot(2, 3, 5)\n",
    "    # Compare first feature across time for first sample\n",
    "    sample_idx = 0\n",
    "    feature_idx = 0\n",
    "    time_points = range(min(500, X_train_normal.shape[1]))  # First 500 time points\n",
    "    \n",
    "    plt.plot(time_points, X_train_normal[sample_idx, time_points, feature_idx], \n",
    "            label='Real Data', alpha=0.7, linewidth=2)\n",
    "    plt.plot(time_points, synthetic_full[sample_idx, time_points, feature_idx], \n",
    "            label='Synthetic Data', alpha=0.7, linewidth=2)\n",
    "    plt.title(f'Real vs Synthetic (Feature {feature_idx})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Statistical Comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    real_means = np.mean(X_train_normal, axis=(0,1))\n",
    "    synth_means = np.mean(synthetic_full, axis=(0,1))\n",
    "    \n",
    "    x_pos = np.arange(len(real_means))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x_pos - width/2, real_means, width, label='Real Data', alpha=0.7)\n",
    "    plt.bar(x_pos + width/2, synth_means, width, label='Synthetic Data', alpha=0.7)\n",
    "    plt.title('Feature Means Comparison')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Mean Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nâœ… Enhanced TimeGAN training and generation completed successfully!\")\n",
    "print(f\"ðŸ“Š Generated {len(synthetic_full)} high-quality synthetic sequences\")\n",
    "print(f\"ðŸŽ¯ Quality Score: {quality_metrics['quality_score']:.4f}\")\n",
    "print(\"ðŸš€ Ready for enhanced anomaly detection pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c793d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:31:00.838547Z",
     "iopub.status.busy": "2025-07-01T05:31:00.837911Z",
     "iopub.status.idle": "2025-07-01T05:31:06.429472Z",
     "shell.execute_reply": "2025-07-01T05:31:06.428473Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = synthetic_full[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"âŒ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83678d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T05:31:06.433539Z",
     "iopub.status.busy": "2025-07-01T05:31:06.433064Z",
     "iopub.status.idle": "2025-07-01T05:34:40.467541Z",
     "shell.execute_reply": "2025-07-01T05:34:40.466663Z"
    }
   },
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, synthetic_full, epochs=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
