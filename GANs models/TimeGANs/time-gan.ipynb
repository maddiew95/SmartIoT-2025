{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from torch.autograd import grad\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda0\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "normal_indices = np.where(label == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Time GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f3564",
   "metadata": {},
   "source": [
    "## Model Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061f71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    \"\"\"Embedding network between original feature space and latent space.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Architecture\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass for embedding.\n",
    "        \n",
    "        Args:\n",
    "            X: input time series features, shape [batch_size, seq_len, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            H: latent representation, shape [batch_size, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        H, _ = self.rnn(X)\n",
    "        return H\n",
    "\n",
    "class Recovery(nn.Module):\n",
    "    \"\"\"Recovery network from latent space to original space.\"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Architecture\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, H):\n",
    "        \"\"\"Forward pass for recovery.\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation, shape [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            X_tilde: recovered data, shape [batch_size, seq_len, output_dim]\n",
    "        \"\"\"\n",
    "        H_tilde, _ = self.rnn(H)\n",
    "        X_tilde = self.output_layer(H_tilde)\n",
    "        return X_tilde\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network for generating synthetic data.\"\"\"\n",
    "    def __init__(self, z_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Architecture\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=z_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        \"\"\"Forward pass for generator.\n",
    "        \n",
    "        Args:\n",
    "            Z: random noise, shape [batch_size, seq_len, z_dim]\n",
    "            \n",
    "        Returns:\n",
    "            H_hat: generated latent data, shape [batch_size, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        H_hat, _ = self.rnn(Z)\n",
    "        return H_hat\n",
    "\n",
    "class Supervisor(nn.Module):\n",
    "    \"\"\"Supervisor network for predicting next sequence step.\"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Architecture\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, H):\n",
    "        \"\"\"Forward pass for supervisor.\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation, shape [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            H_hat_supervise: predicted next step, shape [batch_size, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        H_hat_supervise, _ = self.rnn(H)\n",
    "        return H_hat_supervise\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network between real and synthetic data.\"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Architecture\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers-1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, H):\n",
    "        \"\"\"Forward pass for discriminator.\n",
    "        \n",
    "        Args:\n",
    "            H: latent representation, shape [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Y: discrimination result, shape [batch_size, seq_len, 1]\n",
    "        \"\"\"\n",
    "        features, _ = self.rnn(H)\n",
    "        Y = self.linear(features)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bd7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this chunking function after your imports\n",
    "def chunk_sequences(data, chunk_size=100, overlap=10):\n",
    "    \"\"\"\n",
    "    Split long sequences into smaller chunks\n",
    "    \n",
    "    Args:\n",
    "        data: shape [n_samples, seq_len, features] = (690, 4500, 14)\n",
    "        chunk_size: size of each chunk\n",
    "        overlap: overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        chunked_data: shape [n_chunks, chunk_size, features]\n",
    "    \"\"\"\n",
    "    n_samples, seq_len, n_features = data.shape\n",
    "    chunks = []\n",
    "    \n",
    "    for sample in data:\n",
    "        # Create chunks with overlap\n",
    "        for start in range(0, seq_len - chunk_size + 1, chunk_size - overlap):\n",
    "            end = start + chunk_size\n",
    "            if end <= seq_len:\n",
    "                chunks.append(sample[start:end])\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Update your loss functions to be more stable\n",
    "def embedding_loss(X, X_tilde):\n",
    "    \"\"\"\n",
    "    Robust reconstruction loss using relative error\n",
    "    \"\"\"\n",
    "    # Use relative L1 loss to handle large values\n",
    "    return torch.mean(torch.abs(X - X_tilde) / (torch.abs(X) + 1e-6))\n",
    "\n",
    "\n",
    "def supervised_loss(H, H_hat_supervise):\n",
    "    \"\"\"\n",
    "    Supervised loss for the supervisor network - with safety check\n",
    "    \"\"\"\n",
    "    if H.size(1) > 1:\n",
    "        return torch.mean(torch.abs(H[:, 1:, :] - H_hat_supervise[:, :-1, :]))\n",
    "    return torch.tensor(0.0, device=H.device)\n",
    "\n",
    "def discriminator_loss(Y_real, Y_fake):\n",
    "    \"\"\"\n",
    "    Discriminator loss using BCE with logits for stability\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    real_loss = criterion(Y_real, torch.ones_like(Y_real))\n",
    "    fake_loss = criterion(Y_fake, torch.zeros_like(Y_fake))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(Y_fake, H, H_hat_supervise, X, X_hat, lambda_sup=1.0, lambda_recon=0.01):\n",
    "    \"\"\"\n",
    "    Generator loss with MUCH lower reconstruction weight for raw data\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Adversarial loss\n",
    "    loss_adv = criterion(Y_fake, torch.ones_like(Y_fake))\n",
    "    \n",
    "    # Supervised loss\n",
    "    loss_sup = supervised_loss(H, H_hat_supervise)\n",
    "    \n",
    "    # Relative reconstruction loss (VERY low weight for raw data)\n",
    "    loss_recon = torch.mean(torch.abs(X - X_hat) / (torch.abs(X) + 1e-6))\n",
    "    \n",
    "    # CRITICAL: Much lower reconstruction weight for raw data\n",
    "    total_loss = loss_adv + lambda_sup * loss_sup + lambda_recon * loss_recon\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d31132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated training function with chunking support\n",
    "# Updated training function WITHOUT normalization\n",
    "def train_timegan(data, seq_len, batch_size, model_params, train_params):\n",
    "    \"\"\"\n",
    "    Train TimeGAN model with chunked sequences (no normalization)\n",
    "    \"\"\"\n",
    "    # Chunk the sequences directly without normalization\n",
    "    chunk_size = seq_len  # Use the provided seq_len as chunk size\n",
    "    print(f\"Chunking sequences into size {chunk_size}...\")\n",
    "    chunked_data = chunk_sequences(data, chunk_size=chunk_size, overlap=10)\n",
    "    print(f\"Created {len(chunked_data)} chunks from {len(data)} original sequences\")\n",
    "    \n",
    "    # Model parameters\n",
    "    input_dim = model_params['input_dim']\n",
    "    hidden_dim = model_params['hidden_dim']\n",
    "    num_layers = model_params['num_layers']\n",
    "    z_dim = model_params['z_dim']\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = train_params['epochs']\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    data_tensor = torch.tensor(chunked_data, dtype=torch.float32)\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    embedder = Embedder(input_dim, hidden_dim, num_layers).to(device)\n",
    "    recovery = Recovery(hidden_dim, input_dim, num_layers).to(device)\n",
    "    generator = Generator(z_dim, hidden_dim, num_layers).to(device)\n",
    "    supervisor = Supervisor(hidden_dim, num_layers).to(device)\n",
    "    discriminator = Discriminator(hidden_dim, num_layers).to(device)\n",
    "    \n",
    "    # Initialize optimizers with different learning rates\n",
    "    e_optimizer = optim.Adam(list(embedder.parameters()) + list(recovery.parameters()), lr=learning_rate)\n",
    "    g_optimizer = optim.Adam(list(generator.parameters()) + list(supervisor.parameters()), lr=learning_rate)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate * 0.1)  # Slower discriminator\n",
    "    \n",
    "    print('Start training TimeGAN...')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        step_e_loss = 0\n",
    "        step_g_loss = 0\n",
    "        step_d_loss = 0\n",
    "        \n",
    "        for batch_idx, (X_mb,) in enumerate(dataloader):\n",
    "            X_mb = X_mb.to(device)\n",
    "            batch_size_actual = X_mb.shape[0]\n",
    "            \n",
    "            # Phase 1: Embedding network training (every iteration)\n",
    "            embedder.train()\n",
    "            recovery.train()\n",
    "            \n",
    "            H = embedder(X_mb)\n",
    "            X_tilde = recovery(H)\n",
    "            \n",
    "            e_loss = embedding_loss(X_mb, X_tilde)\n",
    "            \n",
    "            e_optimizer.zero_grad()\n",
    "            e_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(embedder.parameters()) + list(recovery.parameters()), 1.0)\n",
    "            e_optimizer.step()\n",
    "            \n",
    "            step_e_loss += e_loss.item()\n",
    "            \n",
    "            # Phase 2: Train generator and discriminator (every few iterations)\n",
    "            if batch_idx % 2 == 0:  # Train G and D every 2 iterations\n",
    "                # Generator training\n",
    "                generator.train()\n",
    "                supervisor.train()\n",
    "                \n",
    "                Z_mb = torch.randn(batch_size_actual, seq_len, z_dim).to(device)\n",
    "                H_hat = generator(Z_mb)\n",
    "                H_hat_supervise = supervisor(H_hat)\n",
    "                X_hat = recovery(H_hat)\n",
    "                \n",
    "                # Get embeddings from real data\n",
    "                with torch.no_grad():\n",
    "                    H_real = embedder(X_mb)\n",
    "                \n",
    "                # Discriminator outputs\n",
    "                Y_fake = discriminator(H_hat)\n",
    "                \n",
    "                # Generator loss\n",
    "                g_loss = generator_loss(Y_fake, H_real, H_hat_supervise, X_mb, X_hat)\n",
    "                \n",
    "                g_optimizer.zero_grad()\n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(generator.parameters()) + list(supervisor.parameters()), 1.0)\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                step_g_loss += g_loss.item()\n",
    "                \n",
    "                # Discriminator training\n",
    "                discriminator.train()\n",
    "                \n",
    "                # Generate new samples for discriminator\n",
    "                Z_mb_d = torch.randn(batch_size_actual, seq_len, z_dim).to(device)\n",
    "                with torch.no_grad():\n",
    "                    H_hat_d = generator(Z_mb_d)\n",
    "                    H_real_d = embedder(X_mb)\n",
    "                \n",
    "                Y_fake_d = discriminator(H_hat_d)\n",
    "                Y_real_d = discriminator(H_real_d)\n",
    "                \n",
    "                d_loss = discriminator_loss(Y_real_d, Y_fake_d)\n",
    "                \n",
    "                d_optimizer.zero_grad()\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                step_d_loss += d_loss.item()\n",
    "        \n",
    "        # Print progress with reasonable scaling\n",
    "        num_batches = len(dataloader)\n",
    "        avg_e_loss = step_e_loss / num_batches\n",
    "        avg_g_loss = step_g_loss / (num_batches // 2) if num_batches > 2 else step_g_loss / max(1, num_batches)\n",
    "        avg_d_loss = step_d_loss / (num_batches // 2) if num_batches > 2 else step_d_loss / max(1, num_batches)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Embedding loss: {avg_e_loss:.4f}')\n",
    "        print(f'  Generator loss: {avg_g_loss:.4f}')\n",
    "        print(f'  Discriminator loss: {avg_d_loss:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'embedder': embedder,\n",
    "        'recovery': recovery,\n",
    "        'generator': generator,\n",
    "        'supervisor': supervisor,\n",
    "        'discriminator': discriminator,\n",
    "        'chunk_size': chunk_size,\n",
    "        'original_seq_len': data.shape[1]\n",
    "    }\n",
    "\n",
    "# Updated generation function for chunks\n",
    "def generate_timegan_samples(model, n_samples, seq_len, z_dim):\n",
    "    \"\"\"\n",
    "    Generate synthetic samples and denormalize them\n",
    "    \"\"\"\n",
    "    generator = model['generator']\n",
    "    supervisor = model['supervisor']\n",
    "    recovery = model['recovery']\n",
    "    \n",
    "    # Get normalization parameters\n",
    "    data_min = model['data_min']\n",
    "    data_max = model['data_max']\n",
    "    data_range = model['data_range']\n",
    "    \n",
    "    # Generate random noise\n",
    "    Z = torch.randn(n_samples, seq_len, z_dim).to(device)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        supervisor.eval()\n",
    "        recovery.eval()\n",
    "        \n",
    "        H_hat = generator(Z)\n",
    "        H_hat = supervisor(H_hat)\n",
    "        X_hat = recovery(H_hat)\n",
    "    \n",
    "    # Denormalize generated data\n",
    "    X_hat_denorm = X_hat.cpu().numpy() * data_range + data_min\n",
    "    \n",
    "    return X_hat_denorm\n",
    "\n",
    "def reconstruct_full_sequences(chunks, original_length=4500, chunk_size=100, overlap=10):\n",
    "    \"\"\"\n",
    "    Reconstruct full sequences from generated chunks\n",
    "    \"\"\"\n",
    "    step_size = chunk_size - overlap\n",
    "    chunks_needed = (original_length - overlap) // step_size\n",
    "    \n",
    "    n_full_sequences = len(chunks) // chunks_needed\n",
    "    full_sequences = []\n",
    "    \n",
    "    for i in range(n_full_sequences):\n",
    "        start_idx = i * chunks_needed\n",
    "        end_idx = start_idx + chunks_needed\n",
    "        sequence_chunks = chunks[start_idx:end_idx]\n",
    "        \n",
    "        # Reconstruct by overlapping chunks\n",
    "        reconstructed = np.zeros((original_length, sequence_chunks.shape[2]))\n",
    "        pos = 0\n",
    "        \n",
    "        for j, chunk in enumerate(sequence_chunks):\n",
    "            if j == 0:\n",
    "                reconstructed[pos:pos + chunk_size] = chunk\n",
    "                pos += step_size\n",
    "            else:\n",
    "                # Average overlapping regions\n",
    "                overlap_start = pos\n",
    "                overlap_end = pos + overlap\n",
    "                if overlap_end <= original_length:\n",
    "                    reconstructed[overlap_start:overlap_end] = (\n",
    "                        reconstructed[overlap_start:overlap_end] + chunk[:overlap]\n",
    "                    ) / 2\n",
    "                    reconstructed[pos + overlap:pos + chunk_size] = chunk[overlap:]\n",
    "                    pos += step_size\n",
    "        \n",
    "        full_sequences.append(reconstructed)\n",
    "    \n",
    "    return np.array(full_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707da765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (690, 4500, 14)\n",
      "Training on 690 normal samples\n",
      "Chunking sequences into size 100...\n",
      "Created 33810 chunks from 690 original sequences\n",
      "Start training TimeGAN...\n",
      "Epoch 1/50:\n",
      "  Embedding loss: 1.0047\n",
      "  Generator loss: 0.8224\n",
      "  Discriminator loss: 1.5493\n",
      "Epoch 2/50:\n",
      "  Embedding loss: 0.9926\n",
      "  Generator loss: 0.4573\n",
      "  Discriminator loss: 1.8277\n",
      "Epoch 3/50:\n",
      "  Embedding loss: 0.9903\n",
      "  Generator loss: 0.4860\n",
      "  Discriminator loss: 1.7540\n",
      "Epoch 4/50:\n",
      "  Embedding loss: 0.9896\n",
      "  Generator loss: 0.5283\n",
      "  Discriminator loss: 1.6700\n",
      "Epoch 5/50:\n",
      "  Embedding loss: 0.9888\n",
      "  Generator loss: 0.5580\n",
      "  Discriminator loss: 1.6215\n",
      "Epoch 6/50:\n",
      "  Embedding loss: 0.9877\n",
      "  Generator loss: 0.6145\n",
      "  Discriminator loss: 1.5375\n",
      "Epoch 7/50:\n",
      "  Embedding loss: 0.9865\n",
      "  Generator loss: 0.6712\n",
      "  Discriminator loss: 1.4679\n",
      "Epoch 8/50:\n",
      "  Embedding loss: 0.9862\n",
      "  Generator loss: 0.7349\n",
      "  Discriminator loss: 1.3973\n",
      "Epoch 9/50:\n",
      "  Embedding loss: 0.9816\n",
      "  Generator loss: 0.7552\n",
      "  Discriminator loss: 1.3692\n",
      "Epoch 10/50:\n",
      "  Embedding loss: 0.9800\n",
      "  Generator loss: 0.7547\n",
      "  Discriminator loss: 1.3580\n",
      "Epoch 11/50:\n",
      "  Embedding loss: 0.9854\n",
      "  Generator loss: 0.7816\n",
      "  Discriminator loss: 1.3240\n",
      "Epoch 12/50:\n",
      "  Embedding loss: 0.9761\n",
      "  Generator loss: 0.6768\n",
      "  Discriminator loss: 1.4498\n",
      "Epoch 13/50:\n",
      "  Embedding loss: 0.9748\n",
      "  Generator loss: 0.6413\n",
      "  Discriminator loss: 1.4989\n",
      "Epoch 14/50:\n",
      "  Embedding loss: 0.9718\n",
      "  Generator loss: 0.6949\n",
      "  Discriminator loss: 1.4451\n",
      "Epoch 15/50:\n",
      "  Embedding loss: 0.9743\n",
      "  Generator loss: 0.7271\n",
      "  Discriminator loss: 1.4159\n",
      "Epoch 16/50:\n",
      "  Embedding loss: 0.9723\n",
      "  Generator loss: 0.7743\n",
      "  Discriminator loss: 1.3736\n",
      "Epoch 17/50:\n",
      "  Embedding loss: 0.9687\n",
      "  Generator loss: 0.7710\n",
      "  Discriminator loss: 1.3930\n",
      "Epoch 18/50:\n",
      "  Embedding loss: 0.9706\n",
      "  Generator loss: 0.7543\n",
      "  Discriminator loss: 1.4159\n",
      "Epoch 19/50:\n",
      "  Embedding loss: 0.9684\n",
      "  Generator loss: 0.7487\n",
      "  Discriminator loss: 1.4227\n",
      "Epoch 20/50:\n",
      "  Embedding loss: 0.9703\n",
      "  Generator loss: 0.7598\n",
      "  Discriminator loss: 1.4081\n",
      "Epoch 21/50:\n",
      "  Embedding loss: 0.9739\n",
      "  Generator loss: 0.7643\n",
      "  Discriminator loss: 1.3997\n",
      "Epoch 22/50:\n",
      "  Embedding loss: 0.9702\n",
      "  Generator loss: 0.7693\n",
      "  Discriminator loss: 1.3969\n",
      "Epoch 23/50:\n",
      "  Embedding loss: 0.9707\n",
      "  Generator loss: 0.7575\n",
      "  Discriminator loss: 1.4219\n",
      "Epoch 24/50:\n",
      "  Embedding loss: 0.9707\n",
      "  Generator loss: 0.7802\n",
      "  Discriminator loss: 1.3987\n",
      "Epoch 25/50:\n",
      "  Embedding loss: 0.9676\n",
      "  Generator loss: 0.7933\n",
      "  Discriminator loss: 1.3878\n",
      "Epoch 26/50:\n",
      "  Embedding loss: 0.9729\n",
      "  Generator loss: 0.7831\n",
      "  Discriminator loss: 1.3988\n",
      "Epoch 27/50:\n",
      "  Embedding loss: 0.9655\n",
      "  Generator loss: 0.7363\n",
      "  Discriminator loss: 1.4352\n",
      "Epoch 28/50:\n",
      "  Embedding loss: 0.9680\n",
      "  Generator loss: 0.7844\n",
      "  Discriminator loss: 1.3790\n",
      "Epoch 29/50:\n",
      "  Embedding loss: 0.9673\n",
      "  Generator loss: 0.7908\n",
      "  Discriminator loss: 1.3562\n",
      "Epoch 30/50:\n",
      "  Embedding loss: 0.9643\n",
      "  Generator loss: 0.7448\n",
      "  Discriminator loss: 1.3993\n",
      "Epoch 31/50:\n",
      "  Embedding loss: 0.9644\n",
      "  Generator loss: 0.7113\n",
      "  Discriminator loss: 1.4451\n",
      "Epoch 32/50:\n",
      "  Embedding loss: 0.9737\n",
      "  Generator loss: 0.7310\n",
      "  Discriminator loss: 1.4212\n",
      "Epoch 33/50:\n",
      "  Embedding loss: 0.9636\n",
      "  Generator loss: 0.7619\n",
      "  Discriminator loss: 1.4089\n",
      "Epoch 34/50:\n",
      "  Embedding loss: 0.9758\n",
      "  Generator loss: 0.7730\n",
      "  Discriminator loss: 1.3897\n",
      "Epoch 35/50:\n",
      "  Embedding loss: 0.9640\n",
      "  Generator loss: 0.7690\n",
      "  Discriminator loss: 1.4004\n",
      "Epoch 36/50:\n",
      "  Embedding loss: 0.9632\n",
      "  Generator loss: 0.7629\n",
      "  Discriminator loss: 1.4147\n",
      "Epoch 37/50:\n",
      "  Embedding loss: 0.9632\n",
      "  Generator loss: 0.7897\n",
      "  Discriminator loss: 1.3915\n",
      "Epoch 38/50:\n",
      "  Embedding loss: 0.9641\n",
      "  Generator loss: 0.8026\n",
      "  Discriminator loss: 1.3716\n",
      "Epoch 39/50:\n",
      "  Embedding loss: 0.9629\n",
      "  Generator loss: 0.7736\n",
      "  Discriminator loss: 1.4051\n",
      "Epoch 40/50:\n",
      "  Embedding loss: 0.9636\n",
      "  Generator loss: 0.7900\n",
      "  Discriminator loss: 1.3856\n",
      "Epoch 41/50:\n",
      "  Embedding loss: 0.9639\n",
      "  Generator loss: 0.8241\n",
      "  Discriminator loss: 1.3580\n",
      "Epoch 42/50:\n",
      "  Embedding loss: 0.9687\n",
      "  Generator loss: 0.7551\n",
      "  Discriminator loss: 1.4268\n",
      "Epoch 43/50:\n",
      "  Embedding loss: 0.9714\n",
      "  Generator loss: 0.7872\n",
      "  Discriminator loss: 1.4074\n",
      "Epoch 44/50:\n",
      "  Embedding loss: 0.9648\n",
      "  Generator loss: 0.7837\n",
      "  Discriminator loss: 1.3942\n",
      "Epoch 45/50:\n",
      "  Embedding loss: 0.9701\n",
      "  Generator loss: 0.8045\n",
      "  Discriminator loss: 1.3813\n",
      "Epoch 46/50:\n",
      "  Embedding loss: 0.9606\n",
      "  Generator loss: 0.7939\n",
      "  Discriminator loss: 1.3981\n",
      "Epoch 47/50:\n",
      "  Embedding loss: 0.9599\n",
      "  Generator loss: 0.7869\n",
      "  Discriminator loss: 1.3986\n",
      "Epoch 48/50:\n",
      "  Embedding loss: 0.9618\n",
      "  Generator loss: 0.7902\n",
      "  Discriminator loss: 1.3786\n",
      "Epoch 49/50:\n",
      "  Embedding loss: 0.9619\n",
      "  Generator loss: 0.7703\n",
      "  Discriminator loss: 1.3785\n",
      "Epoch 50/50:\n",
      "  Embedding loss: 0.9608\n",
      "  Generator loss: 0.7438\n",
      "  Discriminator loss: 1.4142\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Generate synthetic chunks (RAW data)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m n_synthetic_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 33\u001b[0m synthetic_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_timegan_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_synthetic_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msynthetic_chunks\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m synthetic chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Reconstruct some full sequences (optional)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 147\u001b[0m, in \u001b[0;36mgenerate_timegan_samples\u001b[0;34m(model, n_samples, seq_len, z_dim)\u001b[0m\n\u001b[1;32m    144\u001b[0m recovery \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecovery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Get normalization parameters\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m data_min \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_min\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    148\u001b[0m data_max \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_max\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    149\u001b[0m data_range \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_range\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data_min'"
     ]
    }
   ],
   "source": [
    "# Update your model instantiation code - NO NORMALIZATION\n",
    "# Model parameters - updated for chunked approach\n",
    "chunk_size = 100  # Much more manageable than 4500\n",
    "input_dim = data.shape[2]  # 14 features\n",
    "hidden_dim = 32  # Reduced for stability\n",
    "num_layers = 2   # Fewer layers\n",
    "z_dim = input_dim\n",
    "seq_len = chunk_size  # Use chunk size as sequence length\n",
    "batch_size = 64  # Larger batch size for chunks\n",
    "\n",
    "model_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'z_dim': z_dim\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'epochs': 50,  # Fewer epochs initially\n",
    "    'learning_rate': 0.0001  # Higher learning rate\n",
    "}\n",
    "\n",
    "# Convert data to tensor and create a subset for training (NO NORMALIZATION)\n",
    "normal_data = data[normal_indices]\n",
    "print(f\"Original data shape: {normal_data.shape}\")\n",
    "print(f\"Training on {len(normal_data)} normal samples\")\n",
    "\n",
    "# Train the TimeGAN model on RAW data\n",
    "trained_model = train_timegan(normal_data, seq_len, batch_size, model_params, train_params)\n",
    "\n",
    "# Generate synthetic chunks (RAW data)\n",
    "n_synthetic_chunks = 1000\n",
    "synthetic_chunks = generate_timegan_samples(trained_model, n_synthetic_chunks, seq_len, z_dim)\n",
    "print(f\"Generated {synthetic_chunks.shape} synthetic chunks\")\n",
    "\n",
    "# Reconstruct some full sequences (optional)\n",
    "n_full_sequences = 100\n",
    "synthetic_full = reconstruct_full_sequences(\n",
    "    synthetic_chunks[:n_full_sequences * 45],  # 45 chunks per full sequence (4500/100)\n",
    "    original_length=4500,\n",
    "    chunk_size=chunk_size\n",
    ")\n",
    "print(f\"Reconstructed {synthetic_full.shape} full synthetic sequences\")\n",
    "\n",
    "# For downstream tasks, you can use either chunks or reconstructed sequences\n",
    "# Option 1: Use chunks directly\n",
    "combine_data_chunks = np.concatenate((synthetic_chunks, chunk_sequences(data, chunk_size)), axis=0)\n",
    "combine_labels_chunks = np.concatenate((\n",
    "    np.zeros(len(synthetic_chunks)), \n",
    "    np.repeat(label, (data.shape[1] - 10) // (chunk_size - 10))  # Repeat labels for chunks\n",
    "), axis=0)\n",
    "\n",
    "# Option 2: Use reconstructed full sequences\n",
    "combine_data = np.concatenate((synthetic_full, data), axis=0)\n",
    "combine_labels = np.concatenate((np.zeros(len(synthetic_full)), label), axis=0)\n",
    "\n",
    "print(f\"Combined data shape: {combine_data.shape}\")\n",
    "print(f\"Combined labels shape: {combine_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ca5f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Key Changes Made:\n",
    "\n",
    "1. **Added chunking functionality** that splits your 4500-length sequences into manageable 100-length chunks\n",
    "2. **Updated loss functions** for better stability (L1 loss, BCE with logits)\n",
    "3. **Added data normalization** per channel\n",
    "4. **Reduced model complexity** (32 hidden units, 2 layers)\n",
    "5. **Added gradient clipping** to prevent exploding gradients\n",
    "6. **Different training frequency** for G/D vs embedding networks\n",
    "7. **Sequence reconstruction** capability to get back full-length sequences\n",
    "\n",
    "## Benefits:\n",
    "\n",
    "- **Memory efficient**: Processes 100-timestep chunks instead of 4500\n",
    "- **Stable training**: Should see losses in range 0.1-10.0 instead of billions\n",
    "- **Faster convergence**: Smaller sequences train faster\n",
    "- **Preserves patterns**: Overlapping chunks maintain temporal structure\n",
    "\n",
    "This should resolve your training instability while keeping your existing TimeGAN architecture intact!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ee542",
   "metadata": {},
   "source": [
    "# Generate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_data = np.concatenate((generated_samples, data), axis=0)  # Combine real and generated data\n",
    "# combine_labels = np.concatenate((np.zeros(num_samples), label), axis=0)  # Labels: 0 for real, 0 for generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901a15",
   "metadata": {},
   "source": [
    "# Processing: Mel Spec > Resizing > Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01061dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and convert to 3-channel image\n",
    "def resize_spectrogram(spectrogram):\n",
    "    spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-6)\n",
    "    spectrogram = np.uint8(spectrogram.cpu().numpy() * 255)\n",
    "    spectrogram = np.stack([spectrogram] * 3, axis=-1)\n",
    "    image = Image.fromarray(spectrogram)\n",
    "    image = transforms.Resize((224, 224))(image)\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "# Process dataset\n",
    "def process_dataset(data):\n",
    "    num_samples, _, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, num_channels, 4096))\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=2500000, n_mels=128).to(device)\n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            img = resize_spectrogram(mel)\n",
    "            with torch.no_grad():\n",
    "                feat = model(img.unsqueeze(0).to(device))\n",
    "            features[i, j, :] = feat.squeeze().cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ecf3b",
   "metadata": {},
   "source": [
    "# Mel Scale comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19f39ba8",
   "metadata": {},
   "source": [
    "# AE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "# Train autoencoder\n",
    "def train_autoencoder(features, epochs=20, batch_size=128):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_eval(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))\n",
    "  print(\"F1 = {}\".format(f1_score(labels, predictions)))\n",
    "  print(confusion_matrix(labels, predictions))\n",
    "\n",
    "# Plot reconstruction error histogram\n",
    "def plot_reconstruction_error(model, features, percentile=95):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=64)\n",
    "    errors = []\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            batch_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            errors.extend(batch_errors.cpu().numpy())\n",
    "\n",
    "    threshold = np.percentile(errors, percentile)\n",
    "    anomalies = np.sum(np.array(errors) > threshold)\n",
    "\n",
    "    plt.hist(errors, bins=50, alpha=0.75)\n",
    "    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({percentile}%)')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reconstruction Error Histogram')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "    print(f\"Detected anomalies: {anomalies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae6f0a",
   "metadata": {},
   "source": [
    "# Cross Validation without Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "features = process_dataset(combine_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64ca03",
   "metadata": {},
   "source": [
    "# Cross Validation with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = StandardScaler().fit_transform(combine_data.reshape(-1, data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e602f",
   "metadata": {},
   "source": [
    "# Cross Validation with MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = MinMaxScaler().fit_transform(combine_data.reshape(-1, combine_data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "normal_indices = np.where(combine_labels == 0)[0]\n",
    "print(\"Features shape:\", features.shape)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_labels)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_fold_data, val_fold_data = features[train_idx], features[val_idx]\n",
    "    train_fold_labels, val_fold_labels = combine_labels[train_idx], combine_labels[val_idx]\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(features[normal_indices], epochs=15, batch_size=64)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    x_val = torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader_val = DataLoader(TensorDataset(x_val), batch_size=64)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    x = model(torch.tensor(val_fold_data.reshape(-1, 4096), dtype=torch.float32).to(device)).cpu().detach().numpy()\n",
    "    errors = np.mean((x - val_fold_data.reshape(-1, 4096)) ** 2, axis=1)\n",
    "\n",
    "    # Reshape to (175, 14)\n",
    "    errors = errors.reshape(val_fold_data.shape[0], val_fold_data.shape[1])\n",
    "\n",
    "    # Aggregate per sample (e.g., mean across channels)\n",
    "    sample_errors = np.mean(errors, axis=1)\n",
    "\n",
    "    percentile = 90\n",
    "    # Thresholding\n",
    "    threshold = np.percentile(sample_errors, percentile)\n",
    "    predictions = (sample_errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_reconstruction_error(model, val_fold_data, percentile=percentile)\n",
    "    print_eval(predictions, val_fold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25980ecb",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "Comparing with and without normalizing data \n",
    "\n",
    "### MinMaxed scored\n",
    "\n",
    "Accuracy = 0.8461538461538461\n",
    "\n",
    "Precision = 0.3125\n",
    "\n",
    "Recall = 0.2777777777777778\n",
    "\n",
    "F1 = 0.29411764705882354\n",
    "\n",
    "[[254  22]\n",
    "\n",
    "[ 26  10]]\n",
    "\n",
    "---\n",
    "\n",
    "### StandardScaled scored\n",
    "\n",
    "\n",
    "Accuracy = 0.782051282051282\n",
    "\n",
    "Precision = 0.0\n",
    "\n",
    "Recall = 0.0\n",
    "\n",
    "F1 = 0.0\n",
    "\n",
    "[[244  32]\n",
    "\n",
    "[ 36   0]]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Without any normlaization scored\n",
    "\n",
    "Accuracy = 0.782051282051282\n",
    "\n",
    "Precision = 0.0\n",
    "\n",
    "Recall = 0.0\n",
    "\n",
    "F1 = 0.0\n",
    "\n",
    "[[244  32]\n",
    " \n",
    "[ 36   0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5a291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
