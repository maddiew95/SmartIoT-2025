{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca19d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:24:55.661695Z",
     "iopub.status.busy": "2025-07-13T07:24:55.661034Z",
     "iopub.status.idle": "2025-07-13T07:25:01.755393Z",
     "shell.execute_reply": "2025-07-13T07:25:01.754612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A30\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aec1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:25:01.762547Z",
     "iopub.status.busy": "2025-07-13T07:25:01.761989Z",
     "iopub.status.idle": "2025-07-13T07:25:01.797593Z",
     "shell.execute_reply": "2025-07-13T07:25:01.797033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced Conv1D Generator for Time Series\n",
    "class Conv1DConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=2, num_features=14, seq_len=4500):\n",
    "        super(Conv1DConditionalGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Optimized label embedding for sensor data\n",
    "        self.label_emb = nn.Embedding(num_classes, 64)\n",
    "        \n",
    "        # Optimized initial size for 4500 sequence length\n",
    "        self.init_size = 71  # 71 * 64 = 4544 ‚âà 4500\n",
    "        input_dim = latent_dim + 64  # latent + label embedding\n",
    "        \n",
    "        # Enhanced initial projection for temporal data\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512 * self.init_size),\n",
    "            nn.BatchNorm1d(512 * self.init_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Optimized Conv1D upsampling for sensor patterns\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Block 1: 512 -> 256 channels (71 -> 142)\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 2: 256 -> 128 channels (142 -> 284)\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 3: 128 -> 64 channels (284 -> 568)\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 4: 64 -> 32 channels (568 -> 1136)\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Block 5: 32 -> 16 channels (1136 -> 2272)\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final block: 16 -> 14 channels (2272 -> 4544)\n",
    "            nn.ConvTranspose1d(16, num_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        # Embed labels\n",
    "        label_emb = self.label_emb(labels)\n",
    "        \n",
    "        # Concatenate noise and label embedding\n",
    "        gen_input = torch.cat((z, label_emb), dim=1)\n",
    "        \n",
    "        # Project to initial size\n",
    "        out = self.fc(gen_input)\n",
    "        out = out.view(out.shape[0], 512, self.init_size)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        out = self.conv_blocks(out)\n",
    "        \n",
    "        # Precise adjustment to 4500 length\n",
    "        if out.shape[2] != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Transpose to (batch_size, seq_len, num_features)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "# Enhanced Conv1D Discriminator for Time Series\n",
    "class Conv1DConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes=2, num_features=14, seq_len=4500):\n",
    "        super(Conv1DConditionalDiscriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Optimized label embedding for sensor discrimination\n",
    "        self.label_emb = nn.Embedding(num_classes, 64)\n",
    "        self.label_proj = nn.Linear(64, seq_len)\n",
    "        \n",
    "        # Optimized Conv1D blocks for sensor feature extraction\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Input: (14 + 1) channels, 4500 length\n",
    "            nn.utils.spectral_norm(nn.Conv1d(num_features + 1, 32, kernel_size=8, stride=2, padding=3)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            # 2250 length\n",
    "            nn.utils.spectral_norm(nn.Conv1d(32, 64, kernel_size=8, stride=2, padding=3)),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            # 1125 length\n",
    "            nn.utils.spectral_norm(nn.Conv1d(64, 128, kernel_size=8, stride=2, padding=3)),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 562 length\n",
    "            nn.utils.spectral_norm(nn.Conv1d(128, 256, kernel_size=8, stride=2, padding=3)),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 281 length\n",
    "            nn.utils.spectral_norm(nn.Conv1d(256, 512, kernel_size=8, stride=2, padding=3)),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Optimized classifier for sensor data\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.utils.spectral_norm(nn.Linear(512, 256)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.utils.spectral_norm(nn.Linear(256, 1))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embed and project labels\n",
    "        label_emb = self.label_emb(labels)\n",
    "        label_seq = self.label_proj(label_emb).unsqueeze(1)\n",
    "        \n",
    "        # Transpose x to (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Concatenate data and label\n",
    "        x_labeled = torch.cat([x, label_seq], dim=1)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        features = self.conv_blocks(x_labeled)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Enhanced training function with improved stability\n",
    "def train_conditional_gan_conv1d(normal_data, normal_labels, device, epochs=50, batch_size=64, lr_g=0.0002, lr_d=0.0001):\n",
    "    \"\"\"\n",
    "    Train Conditional GAN optimized for sensor time series data\n",
    "    \"\"\"\n",
    "    print(f\"Training data shape: {normal_data.shape}\")\n",
    "    print(f\"Labels shape: {normal_labels.shape}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    latent_dim = 128  # Increased for complex sensor patterns\n",
    "    num_classes = 2\n",
    "    num_features = normal_data.shape[-1]\n",
    "    seq_len = normal_data.shape[1]\n",
    "    \n",
    "    print(f\"Model parameters: latent_dim={latent_dim}, num_classes={num_classes}, num_features={num_features}, seq_len={seq_len}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Conv1DConditionalGenerator(latent_dim, num_classes, num_features, seq_len).to(device)\n",
    "    discriminator = Conv1DConditionalDiscriminator(num_classes, num_features, seq_len).to(device)\n",
    "    \n",
    "    # Optimized weight initialization for sensor data\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            nn.init.xavier_normal_(m.weight, gain=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight, gain=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    \n",
    "    # Balanced optimizers for sensor data - FIXED: More balanced learning rates\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d * 0.5, betas=(0.5, 0.999))  # Reduce D learning rate\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Create dataloader with optimized batch size\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(normal_data, dtype=torch.float32),\n",
    "        torch.tensor(normal_labels, dtype=torch.long)\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(\"Starting optimized Conditional GAN training for sensor data...\")\n",
    "    \n",
    "    # Training history\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_d_losses = []\n",
    "        epoch_g_losses = []\n",
    "        \n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device)\n",
    "            current_batch_size = real_data.size(0)\n",
    "            \n",
    "            # FIXED: Stronger label smoothing for sensor data\n",
    "            valid = torch.ones(current_batch_size, 1, device=device) * (0.8 + 0.1 * torch.rand(current_batch_size, 1, device=device))\n",
    "            fake = torch.zeros(current_batch_size, 1, device=device) + (0.2 * torch.rand(current_batch_size, 1, device=device))\n",
    "            \n",
    "            # FIXED: Train Discriminator less frequently (every 3 iterations for balance)\n",
    "            if i % 3 == 0:\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Add noise to real data for robustness\n",
    "                noise_factor = 0.05\n",
    "                noisy_real_data = real_data + noise_factor * torch.randn_like(real_data)\n",
    "                \n",
    "                # Real data loss\n",
    "                real_pred = discriminator(noisy_real_data, real_labels)\n",
    "                d_real_loss = criterion(real_pred, valid)\n",
    "                \n",
    "                # Fake data loss\n",
    "                z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                gen_labels = torch.randint(0, num_classes, (current_batch_size,), device=device)\n",
    "                fake_data = generator(z, gen_labels)\n",
    "                fake_pred = discriminator(fake_data.detach(), gen_labels)\n",
    "                d_fake_loss = criterion(fake_pred, fake)\n",
    "                \n",
    "                # Total discriminator loss\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # FIXED: Stronger gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 0.5)\n",
    "                \n",
    "                optimizer_D.step()\n",
    "                epoch_d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Train Generator (every iteration)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake data\n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            gen_labels = torch.randint(0, num_classes, (current_batch_size,), device=device)\n",
    "            fake_data = generator(z, gen_labels)\n",
    "            \n",
    "            # Generator loss\n",
    "            fake_pred = discriminator(fake_data, gen_labels)\n",
    "            g_loss = criterion(fake_pred, valid)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "            \n",
    "            optimizer_G.step()\n",
    "            epoch_g_losses.append(g_loss.item())\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_d_loss = np.mean(epoch_d_losses) if epoch_d_losses else 0\n",
    "        avg_g_loss = np.mean(epoch_g_losses) if epoch_g_losses else 0\n",
    "        \n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "            \n",
    "            # FIXED: Monitor balance ratio\n",
    "            if avg_d_loss > 0:\n",
    "                balance_ratio = avg_g_loss / avg_d_loss\n",
    "                print(f\"  Balance Ratio (G/D): {balance_ratio:.2f} | Target: 1.5-3.0\")\n",
    "                \n",
    "                if balance_ratio > 8:\n",
    "                    print(\"  ‚ö†Ô∏è  Discriminator too strong - consider reducing D learning rate\")\n",
    "                elif balance_ratio < 0.5:\n",
    "                    print(\"  ‚ö†Ô∏è  Generator too strong - consider reducing G learning rate\")\n",
    "                else:\n",
    "                    print(\"  ‚úÖ Training appears balanced\")\n",
    "    \n",
    "    return generator, discriminator, d_losses, g_losses, (normal_data.min(), normal_data.max())\n",
    "\n",
    "# Enhanced sample generation\n",
    "def generate_conditional_samples(generator, num_samples, target_class, seq_len, latent_dim, device, data_range):\n",
    "    \"\"\"\n",
    "    Generate conditional samples optimized for sensor data\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    generated_batches = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            labels = torch.full((current_batch_size,), target_class, dtype=torch.long, device=device)\n",
    "            \n",
    "            batch_generated = generator(z, labels)\n",
    "            generated_batches.append(batch_generated.cpu())\n",
    "    \n",
    "    return torch.cat(generated_batches, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082f6da",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2478943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:25:01.802589Z",
     "iopub.status.busy": "2025-07-13T07:25:01.802221Z",
     "iopub.status.idle": "2025-07-13T07:27:18.428227Z",
     "shell.execute_reply": "2025-07-13T07:27:18.427103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (552, 4500, 14)\n",
      "Labels shape: (552,)\n",
      "Model parameters: latent_dim=128, num_classes=2, num_features=14, seq_len=4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized Conditional GAN training for sensor data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | D Loss: 0.6937 | G Loss: 0.7037\n",
      "  Balance Ratio (G/D): 1.01 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 | D Loss: 0.6577 | G Loss: 0.7486\n",
      "  Balance Ratio (G/D): 1.14 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 | D Loss: 0.6687 | G Loss: 0.7591\n",
      "  Balance Ratio (G/D): 1.14 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 | D Loss: 0.5837 | G Loss: 0.8714\n",
      "  Balance Ratio (G/D): 1.49 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 | D Loss: 0.4613 | G Loss: 1.0671\n",
      "  Balance Ratio (G/D): 2.31 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 | D Loss: 0.4618 | G Loss: 1.1771\n",
      "  Balance Ratio (G/D): 2.55 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200 | D Loss: 0.4084 | G Loss: 1.4693\n",
      "  Balance Ratio (G/D): 3.60 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200 | D Loss: 0.3957 | G Loss: 1.5989\n",
      "  Balance Ratio (G/D): 4.04 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200 | D Loss: 0.3865 | G Loss: 1.6628\n",
      "  Balance Ratio (G/D): 4.30 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200 | D Loss: 0.3702 | G Loss: 1.7674\n",
      "  Balance Ratio (G/D): 4.77 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200 | D Loss: 0.3863 | G Loss: 1.7449\n",
      "  Balance Ratio (G/D): 4.52 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200 | D Loss: 0.3812 | G Loss: 1.8578\n",
      "  Balance Ratio (G/D): 4.87 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200 | D Loss: 0.3769 | G Loss: 1.8670\n",
      "  Balance Ratio (G/D): 4.95 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/200 | D Loss: 0.3688 | G Loss: 1.8804\n",
      "  Balance Ratio (G/D): 5.10 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200 | D Loss: 0.3702 | G Loss: 1.9312\n",
      "  Balance Ratio (G/D): 5.22 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200 | D Loss: 0.3735 | G Loss: 1.9453\n",
      "  Balance Ratio (G/D): 5.21 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200 | D Loss: 0.3829 | G Loss: 1.9053\n",
      "  Balance Ratio (G/D): 4.98 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200 | D Loss: 0.3833 | G Loss: 1.9235\n",
      "  Balance Ratio (G/D): 5.02 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200 | D Loss: 0.3764 | G Loss: 1.9389\n",
      "  Balance Ratio (G/D): 5.15 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/200 | D Loss: 0.3656 | G Loss: 1.9625\n",
      "  Balance Ratio (G/D): 5.37 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200 | D Loss: 0.3687 | G Loss: 1.9959\n",
      "  Balance Ratio (G/D): 5.41 | Target: 1.5-3.0\n",
      "  ‚úÖ Training appears balanced\n"
     ]
    }
   ],
   "source": [
    "# Optimized training for sensor data\n",
    "generator, discriminator, d_history, g_history, data_range = train_conditional_gan_conv1d(\n",
    "    X_train_normal, \n",
    "    y_train_normal,\n",
    "    device, \n",
    "    epochs=200,         # Reduced for faster convergence\n",
    "    batch_size=32,      # Optimized for 690 samples\n",
    "    lr_g=0.001,        # INCREASED Generator learning rate\n",
    "    lr_d=0.0001         # Keep discriminator learning rate same (will be halved in function)\n",
    ")\n",
    "\n",
    "num_samples = len(X_train_normal)\n",
    "# Generate samples for class 0 (normal) with optimized latent_dim\n",
    "generated_data = generate_conditional_samples(\n",
    "    generator, \n",
    "    num_samples=num_samples, \n",
    "    target_class=0, \n",
    "    seq_len=X_train_normal.shape[1],  # FIXED: Use X_train_normal instead of normal_data\n",
    "    latent_dim=128,     # Updated to match generator\n",
    "    device=device, \n",
    "    data_range=data_range\n",
    ")\n",
    "normal_combine = np.concatenate((generated_data, X_train_normal), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27919b42",
   "metadata": {},
   "source": [
    "# FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7cd479e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:27:18.434071Z",
     "iopub.status.busy": "2025-07-13T07:27:18.433736Z",
     "iopub.status.idle": "2025-07-13T07:27:18.438971Z",
     "shell.execute_reply": "2025-07-13T07:27:18.438169Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# # Test the simplified FID calculation\n",
    "# print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# # Use smaller subsets for testing\n",
    "# test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "# test_generated = generated_data[:100]\n",
    "\n",
    "# print(f\"Test real data shape: {test_real.shape}\")\n",
    "# print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# # Calculate FID score\n",
    "# fid_score = calculate_fid_score(\n",
    "#     real_data=test_real,\n",
    "#     fake_data=test_generated,\n",
    "#     device=device,\n",
    "#     sample_rate=1000,\n",
    "# )\n",
    "\n",
    "# if fid_score is not None:\n",
    "#     print(f\"\\nüéâ SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "#     # Interpret the score\n",
    "#     if fid_score < 10:\n",
    "#         quality = \"Excellent\"\n",
    "#     elif fid_score < 25:\n",
    "#         quality = \"Good\"\n",
    "#     elif fid_score < 50:\n",
    "#         quality = \"Fair\"\n",
    "#     elif fid_score < 100:\n",
    "#         quality = \"Poor\"\n",
    "#     else:\n",
    "#         quality = \"Very Poor\"\n",
    "    \n",
    "#     print(f\"Quality Assessment: {quality}\")\n",
    "# else:\n",
    "#     print(\"‚ùå FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea11e12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T07:27:18.443126Z",
     "iopub.status.busy": "2025-07-13T07:27:18.442801Z",
     "iopub.status.idle": "2025-07-13T08:05:22.683411Z",
     "shell.execute_reply": "2025-07-13T08:05:22.681883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-fold cross-validation...\n",
      "============================================================\n",
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.237204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.088416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.047359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.040570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.019507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9143, Prec=0.7500, Rec=0.8571, F1=0.8000\n",
      "Optimal threshold: 0.025406\n",
      "\n",
      "Fold 2/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.237496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.087908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.047168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.042961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.025621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8571, Prec=1.0000, Rec=0.2857, F1=0.4444\n",
      "Optimal threshold: 0.028869\n",
      "\n",
      "Fold 3/5\n",
      "------------------------------\n",
      "Train normal samples: 110\n",
      "Test samples: 35 (28 normal, 7 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.237239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.090178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.047234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.044679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.031828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9429, Prec=1.0000, Rec=0.7143, F1=0.8333\n",
      "Optimal threshold: 0.030643\n",
      "\n",
      "Fold 4/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.237480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.086961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.047364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.039375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.020140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.9143, Prec=1.0000, Rec=0.6250, F1=0.7692\n",
      "Optimal threshold: 0.025462\n",
      "\n",
      "Fold 5/5\n",
      "------------------------------\n",
      "Train normal samples: 111\n",
      "Test samples: 35 (27 normal, 8 anomaly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/20, Loss: 0.237530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6/20, Loss: 0.094994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11/20, Loss: 0.047317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16/20, Loss: 0.037483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20, Loss: 0.018497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Acc=0.8571, Prec=1.0000, Rec=0.3750, F1=0.5455\n",
      "Optimal threshold: 0.029577\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "FOLD-BY-FOLD RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Fold   Accuracy   Precision   Recall   F1-Score  Threshold   \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.9143     0.7500      0.8571   0.8000    0.025406    \n",
      "2      0.8571     1.0000      0.2857   0.4444    0.028869    \n",
      "3      0.9429     1.0000      0.7143   0.8333    0.030643    \n",
      "4      0.9143     1.0000      0.6250   0.7692    0.025462    \n",
      "5      0.8571     1.0000      0.3750   0.5455    0.029577    \n",
      "\n",
      "STATISTICAL SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "Metric       Mean     Std      Min      Max      Median  \n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy     0.8971   0.0343   0.8571   0.9429   0.9143  \n",
      "Precision    0.9500   0.1000   0.7500   1.0000   1.0000  \n",
      "Recall       0.5714   0.2122   0.2857   0.8571   0.6250  \n",
      "F1           0.6785   0.1546   0.4444   0.8333   0.7692  \n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "  Mean F1-Score: 0.6785 ¬± 0.1546\n",
      "  F1-Score Range: [0.4444, 0.8333]\n",
      "  Mean Threshold: 0.027992 ¬± 0.002163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fold_results': [{'fold': 1,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.019506846275180578,\n",
       "   'optimal_threshold': 0.025405999160174168,\n",
       "   'accuracy': 0.9142857142857143,\n",
       "   'precision': 0.75,\n",
       "   'recall': 0.8571428571428571,\n",
       "   'f1': 0.7999999999999999},\n",
       "  {'fold': 2,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.025620919279754163,\n",
       "   'optimal_threshold': 0.02886944489948677,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.2857142857142857,\n",
       "   'f1': 0.4444444444444445},\n",
       "  {'fold': 3,\n",
       "   'train_samples': 110,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.03182781198993325,\n",
       "   'optimal_threshold': 0.03064283041866741,\n",
       "   'accuracy': 0.9428571428571428,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.7142857142857143,\n",
       "   'f1': 0.8333333333333333},\n",
       "  {'fold': 4,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.020139819476753473,\n",
       "   'optimal_threshold': 0.025462265436848007,\n",
       "   'accuracy': 0.9142857142857143,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.625,\n",
       "   'f1': 0.7692307692307693},\n",
       "  {'fold': 5,\n",
       "   'train_samples': 111,\n",
       "   'test_samples': 35,\n",
       "   'final_train_loss': 0.01849718261510134,\n",
       "   'optimal_threshold': 0.02957720930377642,\n",
       "   'accuracy': 0.8571428571428571,\n",
       "   'precision': 1.0,\n",
       "   'recall': 0.375,\n",
       "   'f1': 0.5454545454545454}],\n",
       " 'statistics': {'accuracy': {'mean': 0.8971428571428571,\n",
       "   'std': 0.0342857142857143,\n",
       "   'min': 0.8571428571428571,\n",
       "   'max': 0.9428571428571428,\n",
       "   'median': 0.9142857142857143,\n",
       "   'values': array([0.91428571, 0.85714286, 0.94285714, 0.91428571, 0.85714286])},\n",
       "  'precision': {'mean': 0.95,\n",
       "   'std': 0.09999999999999999,\n",
       "   'min': 0.75,\n",
       "   'max': 1.0,\n",
       "   'median': 1.0,\n",
       "   'values': array([0.75, 1.  , 1.  , 1.  , 1.  ])},\n",
       "  'recall': {'mean': 0.5714285714285714,\n",
       "   'std': 0.2121921539644707,\n",
       "   'min': 0.2857142857142857,\n",
       "   'max': 0.8571428571428571,\n",
       "   'median': 0.625,\n",
       "   'values': array([0.85714286, 0.28571429, 0.71428571, 0.625     , 0.375     ])},\n",
       "  'f1': {'mean': 0.6784926184926184,\n",
       "   'std': 0.15456440688949788,\n",
       "   'min': 0.4444444444444445,\n",
       "   'max': 0.8333333333333333,\n",
       "   'median': 0.7692307692307693,\n",
       "   'values': array([0.8       , 0.44444444, 0.83333333, 0.76923077, 0.54545455])},\n",
       "  'optimal_threshold': {'mean': 0.027991549843790552,\n",
       "   'std': 0.002163176196583614,\n",
       "   'min': 0.025405999160174168,\n",
       "   'max': 0.03064283041866741,\n",
       "   'median': 0.02886944489948677,\n",
       "   'values': array([0.025406  , 0.02886944, 0.03064283, 0.02546227, 0.02957721])},\n",
       "  'train_loss': {'mean': 0.023118515927344558,\n",
       "   'std': 0.005007741451951146,\n",
       "   'min': 0.01849718261510134,\n",
       "   'max': 0.03182781198993325,\n",
       "   'median': 0.020139819476753473,\n",
       "   'values': array([0.01950685, 0.02562092, 0.03182781, 0.02013982, 0.01849718])}},\n",
       " 'all_metrics': {'accuracy': [0.9142857142857143,\n",
       "   0.8571428571428571,\n",
       "   0.9428571428571428,\n",
       "   0.9142857142857143,\n",
       "   0.8571428571428571],\n",
       "  'precision': [0.75, 1.0, 1.0, 1.0, 1.0],\n",
       "  'recall': [0.8571428571428571,\n",
       "   0.2857142857142857,\n",
       "   0.7142857142857143,\n",
       "   0.625,\n",
       "   0.375],\n",
       "  'f1': [0.7999999999999999,\n",
       "   0.4444444444444445,\n",
       "   0.8333333333333333,\n",
       "   0.7692307692307693,\n",
       "   0.5454545454545454],\n",
       "  'optimal_threshold': [0.025405999160174168,\n",
       "   0.02886944489948677,\n",
       "   0.03064283041866741,\n",
       "   0.025462265436848007,\n",
       "   0.02957720930377642],\n",
       "  'train_loss': [0.019506846275180578,\n",
       "   0.025620919279754163,\n",
       "   0.03182781198993325,\n",
       "   0.020139819476753473,\n",
       "   0.01849718261510134]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline_with_cv(normal_combine, X_test_normal, X_test_faulty, \n",
    "                    device=device, batch_size=64, num_epochs=20, cv_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
