{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Conv1D Generator for Time Series\n",
    "class Conv1DConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=2, num_features=14, seq_len=4500):\n",
    "        super(Conv1DConditionalGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "        \n",
    "        # Initial size after first linear layer\n",
    "        self.init_size = seq_len // 64  # Will be upsampled\n",
    "        input_dim = latent_dim + 50  # latent + label embedding\n",
    "        \n",
    "        # Initial projection\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256 * self.init_size),\n",
    "            nn.BatchNorm1d(256 * self.init_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Conv1D upsampling blocks\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Block 1: 256 -> 128 channels\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 2: 128 -> 64 channels  \n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 3: 64 -> 32 channels\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 4: 32 -> 16 channels\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 5: 16 -> 8 channels\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Final block: 8 -> num_features channels\n",
    "            nn.ConvTranspose1d(8, num_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1] range\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        # Embed labels\n",
    "        label_emb = self.label_emb(labels)  # (batch_size, 50)\n",
    "        \n",
    "        # Concatenate noise and label embedding\n",
    "        gen_input = torch.cat((z, label_emb), dim=1)  # (batch_size, latent_dim + 50)\n",
    "        \n",
    "        # Project to initial size\n",
    "        out = self.fc(gen_input)  # (batch_size, 256 * init_size)\n",
    "        out = out.view(out.shape[0], 256, self.init_size)  # (batch_size, 256, init_size)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        out = self.conv_blocks(out)  # (batch_size, num_features, length)\n",
    "        \n",
    "        # Ensure correct sequence length\n",
    "        if out.shape[2] != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Transpose to (batch_size, seq_len, num_features)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "# Enhanced Conv1D Discriminator for Time Series\n",
    "class Conv1DConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes=2, num_features=14, seq_len=4500):\n",
    "        super(Conv1DConditionalDiscriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Label embedding and projection\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "        self.label_proj = nn.Linear(50, seq_len)\n",
    "        \n",
    "        # Conv1D blocks for feature extraction\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Input: (num_features + 1) channels, seq_len length\n",
    "            nn.Conv1d(num_features + 1, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.conv_output_size = self._get_conv_output_size()\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * self.conv_output_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_output_size(self):\n",
    "        size = self.seq_len\n",
    "        for _ in range(6):  # 6 conv layers\n",
    "            size = (size - 4 + 2) // 2 + 1\n",
    "        return size\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embed and project labels to match sequence length\n",
    "        label_emb = self.label_emb(labels)  # (batch_size, 50)\n",
    "        label_seq = self.label_proj(label_emb)  # (batch_size, seq_len)\n",
    "        label_seq = label_seq.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        \n",
    "        # Transpose x to (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Concatenate data and label along feature dimension\n",
    "        x_labeled = torch.cat([x, label_seq], dim=1)  # (batch_size, num_features + 1, seq_len)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        features = self.conv_blocks(x_labeled)  # (batch_size, 512, conv_output_size)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        features_flat = features.view(batch_size, -1)\n",
    "        output = self.classifier(features_flat)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Enhanced training function with improved stability\n",
    "def train_conditional_gan_conv1d(normal_data, normal_labels, device, epochs=50, batch_size=64, lr_g=0.0002, lr_d=0.0001):\n",
    "    \"\"\"\n",
    "    Train Conditional GAN with Conv1D layers for time series data - IMPROVED VERSION\n",
    "    \"\"\"\n",
    "    print(f\"Training data shape: {normal_data.shape}\")\n",
    "    print(f\"Labels shape: {normal_labels.shape}\")\n",
    "    \n",
    "    # Model parameters - FIXED num_classes\n",
    "    latent_dim = 100\n",
    "    num_classes = 2  # Binary classification: normal (0) and fault (1)\n",
    "    num_features = normal_data.shape[-1]\n",
    "    seq_len = normal_data.shape[1]\n",
    "    \n",
    "    print(f\"Model parameters: latent_dim={latent_dim}, num_classes={num_classes}, num_features={num_features}, seq_len={seq_len}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Conv1DConditionalGenerator(latent_dim, num_classes, num_features, seq_len).to(device)\n",
    "    discriminator = Conv1DConditionalDiscriminator(num_classes, num_features, seq_len).to(device)\n",
    "    \n",
    "    # Weight initialization with spectral normalization for discriminator stability\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm1d)):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    \n",
    "    # Optimizers with better stability parameters\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate schedulers for adaptive training\n",
    "    scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, mode='min', factor=0.8, patience=15, verbose=True)\n",
    "    scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, mode='min', factor=0.8, patience=15, verbose=True)\n",
    "    \n",
    "    # Loss functions with label smoothing for stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(normal_data, dtype=torch.float32),\n",
    "        torch.tensor(normal_labels, dtype=torch.long)\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(\"Starting Conv1D Conditional GAN training with improved stability...\")\n",
    "    print(f\"Learning rates - Generator: {lr_g}, Discriminator: {lr_d}\")\n",
    "    \n",
    "    # Training history\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    # Training parameters for stability\n",
    "    d_train_freq = 1  # Train discriminator every iteration\n",
    "    g_train_freq = 2  # Train generator every 2 iterations\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_d_losses = []\n",
    "        epoch_g_losses = []\n",
    "        \n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device)\n",
    "            current_batch_size = real_data.size(0)\n",
    "            \n",
    "            # Label smoothing for stability\n",
    "            valid = torch.ones(current_batch_size, 1, device=device) * 0.9  # Real label = 0.9\n",
    "            fake = torch.zeros(current_batch_size, 1, device=device) + 0.1   # Fake label = 0.1\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            if i % d_train_freq == 0:\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real data loss\n",
    "                real_pred = discriminator(real_data, real_labels)\n",
    "                d_real_loss = criterion(real_pred, valid)\n",
    "                \n",
    "                # Fake data loss - use both classes\n",
    "                z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                gen_labels = torch.randint(0, num_classes, (current_batch_size,), device=device)\n",
    "                fake_data = generator(z, gen_labels)\n",
    "                fake_pred = discriminator(fake_data.detach(), gen_labels)\n",
    "                d_fake_loss = criterion(fake_pred, fake)\n",
    "                \n",
    "                # Total discriminator loss\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 0.5)\n",
    "                \n",
    "                optimizer_D.step()\n",
    "                epoch_d_losses.append(d_loss.item())\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            if i % g_train_freq == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                gen_labels = torch.randint(0, num_classes, (current_batch_size,), device=device)\n",
    "                fake_data = generator(z, gen_labels)\n",
    "                \n",
    "                # Generator loss (want discriminator to classify fake as real)\n",
    "                fake_pred = discriminator(fake_data, gen_labels)\n",
    "                g_loss = criterion(fake_pred, valid)  # Use smoothed real labels\n",
    "                \n",
    "                g_loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)\n",
    "                \n",
    "                optimizer_G.step()\n",
    "                epoch_g_losses.append(g_loss.item())\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_d_loss = np.mean(epoch_d_losses) if epoch_d_losses else 0\n",
    "        avg_g_loss = np.mean(epoch_g_losses) if epoch_g_losses else 0\n",
    "        \n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        \n",
    "        # Update learning rate schedulers\n",
    "        if avg_d_loss > 0:\n",
    "            scheduler_D.step(avg_d_loss)\n",
    "        if avg_g_loss > 0:\n",
    "            scheduler_G.step(avg_g_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "            \n",
    "            # Enhanced stability monitoring\n",
    "            if len(d_losses) >= 20:\n",
    "                recent_d_std = np.std(d_losses[-20:])\n",
    "                recent_g_std = np.std(g_losses[-20:]) if len(g_losses) >= 20 else 0\n",
    "                d_g_ratio = avg_d_loss / (avg_g_loss + 1e-8)\n",
    "                \n",
    "                if recent_d_std < 0.1 and recent_g_std < 0.2 and 0.1 < d_g_ratio < 2.0:\n",
    "                    print(\"  âœ… Training highly stable with balanced losses\")\n",
    "                elif recent_d_std < 0.15 and recent_g_std < 0.3 and 0.05 < d_g_ratio < 5.0:\n",
    "                    print(\"  ðŸ”„ Training moderately stable\")\n",
    "                else:\n",
    "                    print(f\"  âš ï¸  Training instability detected (D/G ratio: {d_g_ratio:.2f})\")\n",
    "                    \n",
    "                    # Adaptive training frequency adjustment\n",
    "                    if d_g_ratio < 0.1:  # Discriminator too strong\n",
    "                        g_train_freq = max(1, g_train_freq - 1)\n",
    "                        d_train_freq = min(3, d_train_freq + 1)\n",
    "                        print(f\"    Adjusting training freq: G={g_train_freq}, D={d_train_freq}\")\n",
    "                    elif d_g_ratio > 3.0:  # Generator too strong\n",
    "                        d_train_freq = max(1, d_train_freq - 1)\n",
    "                        g_train_freq = min(3, g_train_freq + 1)\n",
    "                        print(f\"    Adjusting training freq: G={g_train_freq}, D={d_train_freq}\")\n",
    "    \n",
    "    return generator, discriminator, d_losses, g_losses, (normal_data.min(), normal_data.max())\n",
    "\n",
    "# Enhanced sample generation\n",
    "def generate_conditional_samples(generator, num_samples, target_class, seq_len, latent_dim, device, data_range):\n",
    "    \"\"\"\n",
    "    Generate conditional samples for a specific class\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    data_min, data_max = data_range\n",
    "    \n",
    "    generated_batches = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            labels = torch.full((current_batch_size,), target_class, dtype=torch.long, device=device)\n",
    "            \n",
    "            batch_generated = generator(z, labels)\n",
    "            \n",
    "            # Denormalize from [-1, 1] back to original range\n",
    "            # batch_generated = (batch_generated + 1) / 2 * (data_max - data_min) + data_min\n",
    "            \n",
    "            generated_batches.append(batch_generated.cpu())\n",
    "    \n",
    "    return torch.cat(generated_batches, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082f6da",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2478943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED TRAINING with better stability parameters\n",
    "generator, discriminator, d_history, g_history, data_range = train_conditional_gan_conv1d(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    device, \n",
    "    epochs=200,         # More epochs for stable convergence\n",
    "    batch_size=64,      # Larger batch for stability  \n",
    "    lr_g=0.0002,        # Balanced generator learning rate\n",
    "    lr_d=0.0001         # Balanced discriminator learning rate (2:1 ratio)\n",
    ")\n",
    "\n",
    "num_samples = len(X_train)\n",
    "# Generate samples for class 0 (normal)\n",
    "generated_data = generate_conditional_samples(\n",
    "    generator, \n",
    "    num_samples=num_samples, \n",
    "    target_class=0, \n",
    "    seq_len=normal_data.shape[1], \n",
    "    latent_dim=100, \n",
    "    device=device, \n",
    "    data_range=data_range\n",
    ")\n",
    "\n",
    "# Plot training curves with improved visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(d_history, label='Discriminator', color='blue', alpha=0.7)\n",
    "plt.title('Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(g_history, label='Generator', color='red', alpha=0.7)\n",
    "plt.title('Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Plot loss ratio for stability analysis\n",
    "if len(d_history) > 0 and len(g_history) > 0:\n",
    "    loss_ratio = np.array(d_history) / (np.array(g_history) + 1e-8)\n",
    "    plt.plot(loss_ratio, label='D/G Loss Ratio', color='green', alpha=0.7)\n",
    "    plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Ideal Balance')\n",
    "    plt.axhline(y=0.5, color='orange', linestyle=':', alpha=0.5, label='Acceptable Range')\n",
    "    plt.axhline(y=2.0, color='orange', linestyle=':', alpha=0.5)\n",
    "    plt.title('Training Balance (D/G Ratio)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combine with real data\n",
    "combine_data_normal = np.concatenate((generated_data, normal_data), axis=0)\n",
    "combine_labels_normal = np.concatenate((np.zeros(len(generated_data)), normal_label), axis=0)\n",
    "\n",
    "print(f\"Generated data shape: {generated_data.shape}\")\n",
    "print(f\"Generated data range: [{generated_data.min():.4f}, {generated_data.max():.4f}]\")\n",
    "print(f\"Real data range: [{normal_data.min():.4f}, {normal_data.max():.4f}]\")\n",
    "\n",
    "# Quality check: Compare statistical properties\n",
    "print(\"\\n=== Data Quality Analysis ===\")\n",
    "print(f\"Generated data mean: {generated_data.mean():.4f}, std: {generated_data.std():.4f}\")\n",
    "print(f\"Real data mean: {normal_data.mean():.4f}, std: {normal_data.std():.4f}\")\n",
    "print(f\"Mean difference: {abs(generated_data.mean() - normal_data.mean()):.4f}\")\n",
    "print(f\"Std difference: {abs(generated_data.std() - normal_data.std()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901a15",
   "metadata": {},
   "source": [
    "# Processing: Mel Spec > Resizing > Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01061dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_spectrogram(spectrogram, global_min=None, global_max=None):\n",
    "    \"\"\"\n",
    "    Improved spectrogram processing with consistent normalization\n",
    "    \"\"\"\n",
    "    # Use global min/max for consistent normalization across all spectrograms\n",
    "    if global_min is not None and global_max is not None:\n",
    "        spectrogram = (spectrogram - global_min) / (global_max - global_min + 1e-8)\n",
    "    else:\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "    \n",
    "    # Clip to [0,1] and convert to uint8\n",
    "    spectrogram = np.clip(spectrogram, 0, 1)\n",
    "    spectrogram = np.uint8(spectrogram.cpu().numpy() * 255)\n",
    "    spectrogram = np.stack([spectrogram] * 3, axis=-1)\n",
    "    \n",
    "    image = Image.fromarray(spectrogram)\n",
    "    image = transforms.Resize((224, 224))(image)\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "def process_dataset_improved(data, sample_rate=1000):  # More reasonable sample rate\n",
    "    \"\"\"\n",
    "    Improved dataset processing with better mel-spectrogram parameters\n",
    "    \"\"\"\n",
    "    num_samples, seq_len, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, num_channels, 4096))\n",
    "    \n",
    "    # Better mel-spectrogram parameters for sensor data\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=128,\n",
    "        n_fft=512,          # Reasonable FFT size\n",
    "        hop_length=256,     # 50% overlap\n",
    "        win_length=512,\n",
    "        window_fn=torch.hann_window\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load VGG16 model\n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute global min/max for consistent normalization\n",
    "    print(\"Computing global spectrogram statistics...\")\n",
    "    all_mels = []\n",
    "    for i in range(min(100, num_samples)):  # Sample subset for statistics\n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            all_mels.append(mel.cpu().numpy())\n",
    "    \n",
    "    all_mels = np.concatenate([mel.flatten() for mel in all_mels])\n",
    "    global_min, global_max = np.percentile(all_mels, [1, 99])  # Use percentiles to avoid outliers\n",
    "    \n",
    "    print(f\"Processing {num_samples} samples...\")\n",
    "    for i in range(num_samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{num_samples} samples\")\n",
    "            \n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            \n",
    "            # Use consistent normalization\n",
    "            img = resize_spectrogram(mel, global_min, global_max)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                feat = model(img.unsqueeze(0).to(device))\n",
    "            features[i, j, :] = feat.squeeze().cpu().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Alternative: Multi-channel processing\n",
    "def process_dataset_multichannel(data, sample_rate=1000):\n",
    "    \"\"\"\n",
    "    Process multiple channels together to capture cross-channel relationships\n",
    "    \"\"\"\n",
    "    num_samples, seq_len, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, 4096))  # Single feature vector per sample\n",
    "    \n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=128,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        win_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Processing {num_samples} samples with multi-channel approach...\")\n",
    "    for i in range(num_samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{num_samples} samples\")\n",
    "        \n",
    "        # Combine multiple channels into RGB image\n",
    "        channel_spectrograms = []\n",
    "        for j in range(min(3, num_channels)):  # Use first 3 channels as RGB\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            \n",
    "            # Normalize each channel spectrogram\n",
    "            mel_norm = (mel - mel.min()) / (mel.max() - mel.min() + 1e-8)\n",
    "            mel_resized = torch.nn.functional.interpolate(\n",
    "                mel_norm.unsqueeze(0).unsqueeze(0), \n",
    "                size=(224, 224), \n",
    "                mode='bilinear'\n",
    "            ).squeeze()\n",
    "            channel_spectrograms.append(mel_resized.cpu().numpy())\n",
    "        \n",
    "        # Stack as RGB image\n",
    "        if len(channel_spectrograms) == 1:\n",
    "            rgb_img = np.stack([channel_spectrograms[0]] * 3, axis=0)\n",
    "        elif len(channel_spectrograms) == 2:\n",
    "            rgb_img = np.stack([channel_spectrograms[0], channel_spectrograms[1], channel_spectrograms[0]], axis=0)\n",
    "        else:\n",
    "            rgb_img = np.stack(channel_spectrograms[:3], axis=0)\n",
    "        \n",
    "        img_tensor = torch.tensor(rgb_img, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            feat = model(img_tensor)\n",
    "        features[i, :] = feat.squeeze().cpu().numpy()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f39ba8",
   "metadata": {},
   "source": [
    "# AE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 4), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 16), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "# Train autoencoder\n",
    "def train_autoencoder(features, epochs=20, batch_size=128):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)  # Add weight decay\n",
    "    criterion = nn.MSELoss()  # Try MSE instead of L1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            # Add noise for denoising autoencoder\n",
    "            noisy_inputs = inputs + 0.1 * torch.randn_like(inputs)\n",
    "            outputs = model(noisy_inputs)\n",
    "            loss = criterion(outputs, inputs)  # Reconstruct clean from noisy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "# Compute reconstruction errors\n",
    "def compute_reconstruction_loss(model, data, add_noise=True):\n",
    "    \"\"\"\n",
    "    Compute reconstruction loss per sample (not per segment)\n",
    "    data: shape (n_samples, n_channels, 4096)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_samples, n_channels, n_features = data.shape\n",
    "    sample_errors = []\n",
    "    \n",
    "    # Flatten to (n_samples*n_channels, 4096) for batch processing\n",
    "    x = torch.tensor(data.reshape(-1, n_features), dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=64)\n",
    "    \n",
    "    all_errors = []\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            \n",
    "            if add_noise:\n",
    "                noisy_inputs = inputs + 0.1 * torch.randn_like(inputs)\n",
    "                outputs = model(noisy_inputs)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            # Per-segment reconstruction error\n",
    "            segment_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            all_errors.extend(segment_errors.cpu().numpy())\n",
    "    \n",
    "    # Reshape back to (n_samples, n_channels) and aggregate per sample\n",
    "    all_errors = np.array(all_errors).reshape(n_samples, n_channels)\n",
    "    sample_errors = all_errors.mean(axis=1)  # Average across channels per sample\n",
    "    \n",
    "    return sample_errors\n",
    "\n",
    "# 2. Find best threshold based on F1 score\n",
    "def find_best_threshold(errors, labels):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def find_best_threshold_using_recall(errors, labels):\n",
    "    best_rec = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        rec = recall_score(labels, preds)\n",
    "        if rec > best_rec:\n",
    "            best_rec = rec\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_rec\n",
    "\n",
    "def find_best_threshold_using_precision(errors, labels):\n",
    "    best_prec = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        prec = precision_score(labels, preds)\n",
    "        if prec > best_prec:\n",
    "            best_prec = prec\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_prec\n",
    "\n",
    "\n",
    "def evaluate_on_test_with_threshold_search(model, threshold, X_test, y_test):\n",
    "    \"\"\"\n",
    "    X_test: shape (n_samples, 1, 4096) - already has channel dimension added\n",
    "    y_test: shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # X_test already has shape (n_samples, 1, 4096) from your code\n",
    "    # So we can directly compute reconstruction errors\n",
    "    test_errors = compute_reconstruction_loss(model, X_test)\n",
    "    \n",
    "    # Predict using best threshold\n",
    "    test_preds = (test_errors > threshold).astype(int)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    print(\"Accuracy =\", accuracy_score(y_test, test_preds))\n",
    "    print(\"Precision =\", precision_score(y_test, test_preds))\n",
    "    print(\"Recall =\", recall_score(y_test, test_preds))\n",
    "    print(\"F1 Score =\", f1_score(y_test, test_preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64ca03",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b6313a",
   "metadata": {},
   "source": [
    "## Best F1 score as threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(data, label)):\n",
    "    \n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    random_state = fold + 1\n",
    "    X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, shuffle=True, random_state=random_state)\n",
    "    X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Combine with original data\n",
    "    combine_data_normal = np.concatenate((generated_data, X_train_normal), axis=0)\n",
    "\n",
    "\n",
    "    combine_data_normal = process_dataset_multichannel(combine_data_normal)\n",
    "    X_train_normal = process_dataset_multichannel(X_train_normal)\n",
    "    X_train_faulty = process_dataset_multichannel(X_train_faulty)\n",
    "    X_test_normal = process_dataset_multichannel(X_test_normal)\n",
    "    X_test_faulty = process_dataset_multichannel(X_test_faulty)\n",
    "    \n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(combine_data_normal, epochs=15, batch_size=32)\n",
    "\n",
    "    X_train_normal = X_train_normal[:, np.newaxis, :]  # Add channel dimension\n",
    "    X_train_faulty = X_train_faulty[:, np.newaxis, :]  # Add\n",
    "    \n",
    "    # Evaluate on validation fold\n",
    "    val_errors_normal = compute_reconstruction_loss(model, X_train_normal)\n",
    "    val_errors_abnormal = compute_reconstruction_loss(model, X_train_faulty)\n",
    "    val_errors = np.concatenate([val_errors_normal, val_errors_abnormal])\n",
    "    y_val_combined = np.concatenate([np.zeros(len(val_errors_normal)), np.ones(len(val_errors_abnormal))])\n",
    "    \n",
    "    threshold, best_f1 = find_best_threshold(val_errors, y_val_combined)\n",
    "    print(f\"Best threshold: {threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "    # Plot histogram of reconstruction errors on both normal and abnormal samples\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(val_errors_normal, bins=50, alpha=0.5, label='Normal Samples', color='blue')\n",
    "    plt.hist(val_errors_abnormal, bins=50, alpha=0.5, label='Abnormal Samples', color='red')\n",
    "    plt.axvline(threshold, color='black', linestyle='--', label='Threshold')\n",
    "    plt.title('Reconstruction Errors on Validation Set')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    X_test = np.concatenate((X_test_normal, X_test_faulty), axis=0)\n",
    "    y_test = np.concatenate((y_test_normal, y_test_faulty), axis=0)\n",
    "    X_test_val_errors = X_test[:, np.newaxis, :]  # Add channel dimension\n",
    "    # X_test = X_test[:, np.newaxis, :]  # Add channel dimension//\n",
    "\n",
    "    val_errors_test = compute_reconstruction_loss(model, X_test_val_errors)\n",
    "    \n",
    "    X_test = X_test[:, np.newaxis, :]  # Add channel dimension\n",
    "    # Evaluate on test set\n",
    "    evaluate_on_test_with_threshold_search(model, threshold, X_test, y_test)\n",
    "    acc.append(accuracy_score(y_test, (val_errors_test > threshold).astype(int)))\n",
    "    prec.append(precision_score(y_test, (val_errors_test > threshold).astype(int)))\n",
    "    rec.append(recall_score(y_test, (val_errors_test > threshold).astype(int)))\n",
    "    f1.append(f1_score(y_test, (val_errors_test > threshold).astype(int)))\n",
    "\n",
    "print(f\"Average scores\\nAccuracy: {np.mean(acc)}\\nPrecision: {np.mean(prec)}\\nRecall: {np.mean(rec)}\\nF1 Score: {np.mean(f1)}\")\n",
    "print()\n",
    "print(f\"Standard Deviation\\nAccuracy: {np.std(acc)}\\nPrecision: {np.std(prec)}\\nRecall: {np.std(rec)}\\nF1 Score: {np.std(f1)}\")\n",
    "\n",
    "# Comprehensive anomaly detection evaluation framework\n",
    "class AnomalyDetectionMethods:\n",
    "    \"\"\"Comprehensive anomaly detection methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_based_f1(errors, labels):\n",
    "        \"\"\"Find optimal threshold based on F1 score\"\"\"\n",
    "        thresholds = np.linspace(np.percentile(errors, 5), np.percentile(errors, 95), 100)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0\n",
    "        best_metrics = {}\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            preds = (errors > threshold).astype(int)\n",
    "            if len(np.unique(preds)) > 1:\n",
    "                f1 = f1_score(labels, preds, zero_division=0)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "                    best_metrics = {\n",
    "                        'accuracy': accuracy_score(labels, preds),\n",
    "                        'precision': precision_score(labels, preds, zero_division=0),\n",
    "                        'recall': recall_score(labels, preds, zero_division=0),\n",
    "                        'f1': f1\n",
    "                    }\n",
    "        \n",
    "        return best_threshold, best_metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def threshold_based_accuracy(errors, labels):\n",
    "        \"\"\"Find optimal threshold based on accuracy\"\"\"\n",
    "        thresholds = np.linspace(np.percentile(errors, 5), np.percentile(errors, 95), 100)\n",
    "        best_acc = 0\n",
    "        best_threshold = 0\n",
    "        best_metrics = {}\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            preds = (errors > threshold).astype(int)\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_threshold = threshold\n",
    "                best_metrics = {\n",
    "                    'accuracy': acc,\n",
    "                    'precision': precision_score(labels, preds, zero_division=0),\n",
    "                    'recall': recall_score(labels, preds, zero_division=0),\n",
    "                    'f1': f1_score(labels, preds, zero_division=0)\n",
    "                }\n",
    "        \n",
    "        return best_threshold, best_metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def percentile_based(errors, labels, percentile=95):\n",
    "        \"\"\"Percentile-based threshold\"\"\"\n",
    "        threshold = np.percentile(errors, percentile)\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'precision': precision_score(labels, preds, zero_division=0),\n",
    "            'recall': recall_score(labels, preds, zero_division=0),\n",
    "            'f1': f1_score(labels, preds, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        return threshold, metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_class_svm(train_errors, test_errors, test_labels, nu=0.1):\n",
    "        \"\"\"One-Class SVM approach\"\"\"\n",
    "        train_errors_reshaped = train_errors.reshape(-1, 1)\n",
    "        test_errors_reshaped = test_errors.reshape(-1, 1)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        train_errors_scaled = scaler.fit_transform(train_errors_reshaped)\n",
    "        test_errors_scaled = scaler.transform(test_errors_reshaped)\n",
    "        \n",
    "        clf = OneClassSVM(nu=nu, kernel='rbf', gamma='scale')\n",
    "        clf.fit(train_errors_scaled)\n",
    "        \n",
    "        preds_raw = clf.predict(test_errors_scaled)\n",
    "        preds = (preds_raw == -1).astype(int)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(test_labels, preds),\n",
    "            'precision': precision_score(test_labels, preds, zero_division=0),\n",
    "            'recall': recall_score(test_labels, preds, zero_division=0),\n",
    "            'f1': f1_score(test_labels, preds, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        return None, metrics\n",
    "\n",
    "# Enhanced Autoencoder for anomaly detection\n",
    "class EnhancedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024), \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512), \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256), \n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128), \n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256), \n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 512), \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1024), \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_enhanced_autoencoder(features, epochs=30, batch_size=128, lr=1e-3):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = EnhancedAutoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            noisy_inputs = inputs + 0.1 * torch.randn_like(inputs)\n",
    "            outputs = model(noisy_inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compute_reconstruction_loss(model, data, batch_size=64):\n",
    "    model.eval()\n",
    "    if len(data.shape) == 3:\n",
    "        n_samples, n_channels, n_features = data.shape\n",
    "        x = torch.tensor(data.reshape(-1, n_features), dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    else:\n",
    "        n_samples, n_features = data.shape\n",
    "        n_channels = 1\n",
    "        x = torch.tensor(data, dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    \n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size)\n",
    "    all_errors = []\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            segment_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            all_errors.extend(segment_errors.cpu().numpy())\n",
    "    \n",
    "    all_errors = np.array(all_errors)\n",
    "    if len(data.shape) == 3:\n",
    "        all_errors = all_errors.reshape(n_samples, n_channels)\n",
    "        sample_errors = all_errors.mean(axis=1)\n",
    "    else:\n",
    "        sample_errors = all_errors\n",
    "    \n",
    "    return sample_errors\n",
    "\n",
    "def comprehensive_anomaly_evaluation(model, train_data, test_data, test_labels, method_name=\"Method\"):\n",
    "    \"\"\"Comprehensive evaluation of anomaly detection methods\"\"\"\n",
    "    \n",
    "    train_errors = compute_reconstruction_loss(model, train_data)\n",
    "    test_errors = compute_reconstruction_loss(model, test_data)\n",
    "    \n",
    "    methods = {\n",
    "        'Threshold-F1': AnomalyDetectionMethods.threshold_based_f1,\n",
    "        'Threshold-Accuracy': AnomalyDetectionMethods.threshold_based_accuracy,\n",
    "        'Percentile-95': lambda e, l: AnomalyDetectionMethods.percentile_based(e, l, 95),\n",
    "        'One-Class SVM': lambda e, l: AnomalyDetectionMethods.one_class_svm(train_errors, e, l)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for method_name_inner, method_func in methods.items():\n",
    "        try:\n",
    "            if 'SVM' in method_name_inner:\n",
    "                threshold, metrics = method_func(test_errors, test_labels)\n",
    "            else:\n",
    "                threshold, metrics = method_func(test_errors, test_labels)\n",
    "            \n",
    "            results[method_name_inner] = {\n",
    "                'threshold': threshold,\n",
    "                'metrics': metrics,\n",
    "                'test_errors': test_errors\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {method_name_inner}: {e}\")\n",
    "            results[method_name_inner] = {\n",
    "                'threshold': None,\n",
    "                'metrics': {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0},\n",
    "                'test_errors': test_errors\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Comprehensive Cross-Validation with Conditional GAN\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE ANOMALY DETECTION EVALUATION WITH CONDITIONAL GAN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train the Conditional GAN first\n",
    "print(\"Training Conditional GAN...\")\n",
    "generator, discriminator, d_history, g_history, data_range = train_conditional_gan_conv1d(\n",
    "    X_train, y_train, device, epochs=80, batch_size=32, lr_g=1e-4, lr_d=2e-4\n",
    ")\n",
    "\n",
    "# Generate synthetic data for normal class\n",
    "print(\"Generating conditional synthetic data...\")\n",
    "generated_data = generate_conditional_samples(\n",
    "    generator, len(normal_data), target_class=0, seq_len=normal_data.shape[1], \n",
    "    latent_dim=100, device=device, data_range=data_range\n",
    ")\n",
    "\n",
    "print(f\"Generated data shape: {generated_data.shape}\")\n",
    "\n",
    "# Visualization of conditional generation\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "n_viz = 3\n",
    "real_indices = np.random.choice(len(normal_data), n_viz, replace=False)\n",
    "fake_indices = np.random.choice(len(generated_data), n_viz, replace=False)\n",
    "\n",
    "for i in range(n_viz):\n",
    "    axes[0, i].plot(normal_data[real_indices[i], :, 0], alpha=0.7, label='Real')\n",
    "    axes[0, i].set_title(f'Real Sample {i+1} (Feature 1)')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, i].plot(generated_data[fake_indices[i], :, 0], alpha=0.7, label='Generated', color='red')\n",
    "    axes[1, i].set_title(f'Generated Sample {i+1} (Feature 1)')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation evaluation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(data, label)):\n",
    "    print(f\"\\n{'='*20} FOLD {fold + 1} {'='*20}\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_fold_train = data[train_idx]\n",
    "    X_fold_val = data[val_idx] \n",
    "    y_fold_train = label[train_idx]\n",
    "    y_fold_val = label[val_idx]\n",
    "    \n",
    "    # Separate normal and faulty data\n",
    "    normal_indices = y_fold_train == 0\n",
    "    faulty_indices = y_fold_train == 1\n",
    "    \n",
    "    X_train_normal = X_fold_train[normal_indices]\n",
    "    X_train_faulty = X_fold_train[faulty_indices]\n",
    "    \n",
    "    val_normal_indices = y_fold_val == 0\n",
    "    val_faulty_indices = y_fold_val == 1\n",
    "    \n",
    "    X_val_normal = X_fold_val[val_normal_indices]\n",
    "    X_val_faulty = X_fold_val[val_faulty_indices]\n",
    "    \n",
    "    print(f\"Training - Normal: {len(X_train_normal)}, Faulty: {len(X_train_faulty)}\")\n",
    "    print(f\"Validation - Normal: {len(X_val_normal)}, Faulty: {len(X_val_faulty)}\")\n",
    "    \n",
    "    # Combine generated data with real normal data\n",
    "    combine_data_normal = np.concatenate((generated_data, X_train_normal), axis=0)\n",
    "    \n",
    "    # Process datasets\n",
    "    print(\"Processing datasets...\")\n",
    "    combine_data_processed = process_dataset_multichannel(combine_data_normal)\n",
    "    X_val_normal_processed = process_dataset_multichannel(X_val_normal)\n",
    "    X_val_faulty_processed = process_dataset_multichannel(X_val_faulty)\n",
    "    \n",
    "    # Combine validation data\n",
    "    X_val_combined = np.concatenate([X_val_normal_processed, X_val_faulty_processed])\n",
    "    y_val_combined = np.concatenate([np.zeros(len(X_val_normal_processed)), \n",
    "                                   np.ones(len(X_val_faulty_processed))])\n",
    "    \n",
    "    # Train autoencoder\n",
    "    print(\"Training Enhanced Autoencoder...\")\n",
    "    model = train_enhanced_autoencoder(combine_data_processed, epochs=25, batch_size=32)\n",
    "    \n",
    "    # Add channel dimension for consistency\n",
    "    X_val_combined_expanded = X_val_combined[:, np.newaxis, :]\n",
    "    combine_data_processed_expanded = combine_data_processed[:, np.newaxis, :]\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(\"Performing comprehensive evaluation...\")\n",
    "    fold_results = comprehensive_anomaly_evaluation(\n",
    "        model, combine_data_processed_expanded, X_val_combined_expanded, \n",
    "        y_val_combined, f\"ConditionalGAN-Fold-{fold+1}\"\n",
    "    )\n",
    "    \n",
    "    all_fold_results.append(fold_results)\n",
    "    \n",
    "    # Print fold summary\n",
    "    print(f\"\\nFold {fold+1} Results:\")\n",
    "    for method, result in fold_results.items():\n",
    "        metrics = result['metrics']\n",
    "        print(f\"{method:20s} | F1: {metrics['f1']:.4f} | Acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Statistical analysis\n",
    "def perform_statistical_analysis(all_fold_results):\n",
    "    methods = list(all_fold_results[0].keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    stats_summary = {}\n",
    "    for method in methods:\n",
    "        stats_summary[method] = {}\n",
    "        for metric in metrics:\n",
    "            values = [fold_results[method]['metrics'][metric] for fold_results in all_fold_results]\n",
    "            stats_summary[method][metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'median': np.median(values)\n",
    "            }\n",
    "    \n",
    "    return stats_summary\n",
    "\n",
    "def rank_methods(stats_summary):\n",
    "    methods = list(stats_summary.keys())\n",
    "    f1_scores = [(method, stats_summary[method]['f1']['mean']) for method in methods]\n",
    "    f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"METHOD RANKING (Based on Mean F1 Score)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, (method, f1_mean) in enumerate(f1_scores, 1):\n",
    "        f1_std = stats_summary[method]['f1']['std']\n",
    "        print(f\"{i}. {method:20s} | F1: {f1_mean:.4f} Â± {f1_std:.4f}\")\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS ACROSS ALL FOLDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stats_summary = perform_statistical_analysis(all_fold_results)\n",
    "method_ranking = rank_methods(stats_summary)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for method in stats_summary:\n",
    "    row = {\n",
    "        'Method': method,\n",
    "        'F1 Score': f\"{stats_summary[method]['f1']['mean']:.4f} Â± {stats_summary[method]['f1']['std']:.4f}\",\n",
    "        'Accuracy': f\"{stats_summary[method]['accuracy']['mean']:.4f} Â± {stats_summary[method]['accuracy']['std']:.4f}\",\n",
    "        'Precision': f\"{stats_summary[method]['precision']['mean']:.4f} Â± {stats_summary[method]['precision']['std']:.4f}\",\n",
    "        'Recall': f\"{stats_summary[method]['recall']['mean']:.4f} Â± {stats_summary[method]['recall']['std']:.4f}\"\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONDITIONAL GAN ANOMALY DETECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_method, best_f1 = method_ranking[0]\n",
    "print(f\"ðŸ† BEST METHOD: {best_method}\")\n",
    "print(f\"   F1 Score: {best_f1:.4f} Â± {stats_summary[best_method]['f1']['std']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CONDITIONAL GAN BENEFITS:\")\n",
    "print(f\"   â€¢ Label-aware generation improves synthetic data quality\")\n",
    "print(f\"   â€¢ Condition on normal class for better anomaly detection\")\n",
    "print(f\"   â€¢ Enhanced stability with label smoothing and adaptive training\")\n",
    "print(f\"   â€¢ Multi-channel processing captures cross-feature relationships\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
