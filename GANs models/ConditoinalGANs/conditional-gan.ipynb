{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Conv1D Generator for Time Series\n",
    "class Conv1DConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=2, num_features=14, seq_len=4500):\n",
    "        super(Conv1DConditionalGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "        \n",
    "        # Initial size after first linear layer\n",
    "        self.init_size = seq_len // 64  # Will be upsampled\n",
    "        input_dim = latent_dim + 50  # latent + label embedding\n",
    "        \n",
    "        # Initial projection\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256 * self.init_size),\n",
    "            nn.BatchNorm1d(256 * self.init_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Conv1D upsampling blocks\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Block 1: 256 -> 128 channels\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 2: 128 -> 64 channels  \n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 3: 64 -> 32 channels\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 4: 32 -> 16 channels\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 5: 16 -> 8 channels\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Final block: 8 -> num_features channels\n",
    "            nn.ConvTranspose1d(8, num_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1] range\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        # Embed labels\n",
    "        label_emb = self.label_emb(labels)  # (batch_size, 50)\n",
    "        \n",
    "        # Concatenate noise and label embedding\n",
    "        gen_input = torch.cat((z, label_emb), dim=1)  # (batch_size, latent_dim + 50)\n",
    "        \n",
    "        # Project to initial size\n",
    "        out = self.fc(gen_input)  # (batch_size, 256 * init_size)\n",
    "        out = out.view(out.shape[0], 256, self.init_size)  # (batch_size, 256, init_size)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        out = self.conv_blocks(out)  # (batch_size, num_features, length)\n",
    "        \n",
    "        # Ensure correct sequence length\n",
    "        if out.shape[2] != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Transpose to (batch_size, seq_len, num_features)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "# Enhanced Conv1D Discriminator for Time Series\n",
    "class Conv1DConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes=2, num_features=14, seq_len=4500):\n",
    "        super(Conv1DConditionalDiscriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Label embedding and projection\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "        self.label_proj = nn.Linear(50, seq_len)\n",
    "        \n",
    "        # Conv1D blocks for feature extraction\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Input: (num_features + 1) channels, seq_len length\n",
    "            nn.Conv1d(num_features + 1, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.conv_output_size = self._get_conv_output_size()\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * self.conv_output_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_output_size(self):\n",
    "        size = self.seq_len\n",
    "        for _ in range(6):  # 6 conv layers\n",
    "            size = (size - 4 + 2) // 2 + 1\n",
    "        return size\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embed and project labels to match sequence length\n",
    "        label_emb = self.label_emb(labels)  # (batch_size, 50)\n",
    "        label_seq = self.label_proj(label_emb)  # (batch_size, seq_len)\n",
    "        label_seq = label_seq.unsqueeze(1)  # (batch_size, 1, seq_len)\n",
    "        \n",
    "        # Transpose x to (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Concatenate data and label along feature dimension\n",
    "        x_labeled = torch.cat([x, label_seq], dim=1)  # (batch_size, num_features + 1, seq_len)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        features = self.conv_blocks(x_labeled)  # (batch_size, 512, conv_output_size)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        features_flat = features.view(batch_size, -1)\n",
    "        output = self.classifier(features_flat)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Enhanced training function with improved stability\n",
    "def train_conditional_gan_conv1d(normal_data, normal_labels, device, epochs=50, batch_size=64, lr_g=0.0002, lr_d=0.0001):\n",
    "    \"\"\"\n",
    "    Train Conditional GAN with Conv1D layers for time series data - IMPROVED VERSION\n",
    "    \"\"\"\n",
    "    print(f\"Training data shape: {normal_data.shape}\")\n",
    "    print(f\"Labels shape: {normal_labels.shape}\")\n",
    "    \n",
    "    # Model parameters - FIXED num_classes\n",
    "    latent_dim = 100\n",
    "    num_classes = 2  # Binary classification: normal (0) and fault (1)\n",
    "    num_features = normal_data.shape[-1]\n",
    "    seq_len = normal_data.shape[1]\n",
    "    \n",
    "    print(f\"Model parameters: latent_dim={latent_dim}, num_classes={num_classes}, num_features={num_features}, seq_len={seq_len}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Conv1DConditionalGenerator(latent_dim, num_classes, num_features, seq_len).to(device)\n",
    "    discriminator = Conv1DConditionalDiscriminator(num_classes, num_features, seq_len).to(device)\n",
    "    \n",
    "    # Weight initialization with spectral normalization for discriminator stability\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm1d)):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    \n",
    "    # Optimizers with better stability parameters\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate schedulers for adaptive training\n",
    "    scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, mode='min', factor=0.8, patience=15, verbose=True)\n",
    "    scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, mode='min', factor=0.8, patience=15, verbose=True)\n",
    "    \n",
    "    # Loss functions with label smoothing for stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(normal_data, dtype=torch.float32),\n",
    "        torch.tensor(normal_labels, dtype=torch.long)\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(\"Starting Conv1D Conditional GAN training with improved stability...\")\n",
    "    print(f\"Learning rates - Generator: {lr_g}, Discriminator: {lr_d}\")\n",
    "    \n",
    "    # Training history\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    # Training parameters for stability\n",
    "    d_train_freq = 1  # Train discriminator every iteration\n",
    "    g_train_freq = 2  # Train generator every 2 iterations\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_d_losses = []\n",
    "        epoch_g_losses = []\n",
    "        \n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device)\n",
    "            current_batch_size = real_data.size(0)\n",
    "            \n",
    "            # Label smoothing for stability\n",
    "            valid = torch.ones(current_batch_size, 1, device=device) * 0.9  # Real label = 0.9\n",
    "            fake = torch.zeros(current_batch_size, 1, device=device) + 0.1   # Fake label = 0.1\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            if i % d_train_freq == 0:\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real data loss\n",
    "                real_pred = discriminator(real_data, real_labels)\n",
    "                d_real_loss = criterion(real_pred, valid)\n",
    "                \n",
    "                # Fake data loss - use both classes\n",
    "                z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                gen_labels = torch.randint(0, num_classes, (current_batch_size,), device=device)\n",
    "                fake_data = generator(z, gen_labels)\n",
    "                fake_pred = discriminator(fake_data.detach(), gen_labels)\n",
    "                d_fake_loss = criterion(fake_pred, fake)\n",
    "                \n",
    "                # Total discriminator loss\n",
    "                d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 0.5)\n",
    "                \n",
    "                optimizer_D.step()\n",
    "                epoch_d_losses.append(d_loss.item())\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            if i % g_train_freq == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                # Generate fake data\n",
    "                z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "                gen_labels = torch.randint(0, num_classes, (current_batch_size,), device=device)\n",
    "                fake_data = generator(z, gen_labels)\n",
    "                \n",
    "                # Generator loss (want discriminator to classify fake as real)\n",
    "                fake_pred = discriminator(fake_data, gen_labels)\n",
    "                g_loss = criterion(fake_pred, valid)  # Use smoothed real labels\n",
    "                \n",
    "                g_loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)\n",
    "                \n",
    "                optimizer_G.step()\n",
    "                epoch_g_losses.append(g_loss.item())\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_d_loss = np.mean(epoch_d_losses) if epoch_d_losses else 0\n",
    "        avg_g_loss = np.mean(epoch_g_losses) if epoch_g_losses else 0\n",
    "        \n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        \n",
    "        # Update learning rate schedulers\n",
    "        if avg_d_loss > 0:\n",
    "            scheduler_D.step(avg_d_loss)\n",
    "        if avg_g_loss > 0:\n",
    "            scheduler_G.step(avg_g_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "            \n",
    "            # Enhanced stability monitoring\n",
    "            if len(d_losses) >= 20:\n",
    "                recent_d_std = np.std(d_losses[-20:])\n",
    "                recent_g_std = np.std(g_losses[-20:]) if len(g_losses) >= 20 else 0\n",
    "                d_g_ratio = avg_d_loss / (avg_g_loss + 1e-8)\n",
    "                \n",
    "                if recent_d_std < 0.1 and recent_g_std < 0.2 and 0.1 < d_g_ratio < 2.0:\n",
    "                    print(\"  ✅ Training highly stable with balanced losses\")\n",
    "                elif recent_d_std < 0.15 and recent_g_std < 0.3 and 0.05 < d_g_ratio < 5.0:\n",
    "                    print(\"  🔄 Training moderately stable\")\n",
    "                else:\n",
    "                    print(f\"  ⚠️  Training instability detected (D/G ratio: {d_g_ratio:.2f})\")\n",
    "                    \n",
    "                    # Adaptive training frequency adjustment\n",
    "                    if d_g_ratio < 0.1:  # Discriminator too strong\n",
    "                        g_train_freq = max(1, g_train_freq - 1)\n",
    "                        d_train_freq = min(3, d_train_freq + 1)\n",
    "                        print(f\"    Adjusting training freq: G={g_train_freq}, D={d_train_freq}\")\n",
    "                    elif d_g_ratio > 3.0:  # Generator too strong\n",
    "                        d_train_freq = max(1, d_train_freq - 1)\n",
    "                        g_train_freq = min(3, g_train_freq + 1)\n",
    "                        print(f\"    Adjusting training freq: G={g_train_freq}, D={d_train_freq}\")\n",
    "    \n",
    "    return generator, discriminator, d_losses, g_losses, (normal_data.min(), normal_data.max())\n",
    "\n",
    "# Enhanced sample generation\n",
    "def generate_conditional_samples(generator, num_samples, target_class, seq_len, latent_dim, device, data_range):\n",
    "    \"\"\"\n",
    "    Generate conditional samples for a specific class\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    data_min, data_max = data_range\n",
    "    \n",
    "    generated_batches = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            labels = torch.full((current_batch_size,), target_class, dtype=torch.long, device=device)\n",
    "            \n",
    "            batch_generated = generator(z, labels)\n",
    "            \n",
    "            # Denormalize from [-1, 1] back to original range\n",
    "            # batch_generated = (batch_generated + 1) / 2 * (data_max - data_min) + data_min\n",
    "            \n",
    "            generated_batches.append(batch_generated.cpu())\n",
    "    \n",
    "    return torch.cat(generated_batches, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082f6da",
   "metadata": {},
   "source": [
    "# Train, and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2478943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED TRAINING with better stability parameters\n",
    "generator, discriminator, d_history, g_history, data_range = train_conditional_gan_conv1d(\n",
    "    X_train_normal, \n",
    "    y_train_normal,\n",
    "    device, \n",
    "    epochs=300,         # More epochs for stable convergence\n",
    "    batch_size=64,      # Larger batch for stability  \n",
    "    lr_g=0.001,        # Balanced generator learning rate\n",
    "    lr_d=0.00001         # Balanced discriminator learning rate (2:1 ratio)\n",
    ")\n",
    "\n",
    "num_samples = len(X_train_normal)\n",
    "# Generate samples for class 0 (normal)\n",
    "generated_data = generate_conditional_samples(\n",
    "    generator, \n",
    "    num_samples=num_samples, \n",
    "    target_class=0, \n",
    "    seq_len=normal_data.shape[1], \n",
    "    latent_dim=100, \n",
    "    device=device, \n",
    "    data_range=data_range\n",
    ")\n",
    "\n",
    "# Plot training curves with improved visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(d_history, label='Discriminator', color='blue', alpha=0.7)\n",
    "plt.title('Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(g_history, label='Generator', color='red', alpha=0.7)\n",
    "plt.title('Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Plot loss ratio for stability analysis\n",
    "if len(d_history) > 0 and len(g_history) > 0:\n",
    "    loss_ratio = np.array(d_history) / (np.array(g_history) + 1e-8)\n",
    "    plt.plot(loss_ratio, label='D/G Loss Ratio', color='green', alpha=0.7)\n",
    "    plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Ideal Balance')\n",
    "    plt.axhline(y=0.5, color='orange', linestyle=':', alpha=0.5, label='Acceptable Range')\n",
    "    plt.axhline(y=2.0, color='orange', linestyle=':', alpha=0.5)\n",
    "    plt.title('Training Balance (D/G Ratio)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combine with real data\n",
    "combine_data_normal = np.concatenate((generated_data, normal_data), axis=0)\n",
    "combine_labels_normal = np.concatenate((np.zeros(len(generated_data)), normal_label), axis=0)\n",
    "\n",
    "print(f\"Generated data shape: {generated_data.shape}\")\n",
    "print(f\"Generated data range: [{generated_data.min():.4f}, {generated_data.max():.4f}]\")\n",
    "print(f\"Real data range: [{normal_data.min():.4f}, {normal_data.max():.4f}]\")\n",
    "\n",
    "# Quality check: Compare statistical properties\n",
    "print(\"\\n=== Data Quality Analysis ===\")\n",
    "print(f\"Generated data mean: {generated_data.mean():.4f}, std: {generated_data.std():.4f}\")\n",
    "print(f\"Real data mean: {normal_data.mean():.4f}, std: {normal_data.std():.4f}\")\n",
    "print(f\"Mean difference: {abs(generated_data.mean() - normal_data.mean()):.4f}\")\n",
    "print(f\"Std difference: {abs(generated_data.std() - normal_data.std()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27919b42",
   "metadata": {},
   "source": [
    "# FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = generated_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\n🎉 SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"❌ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, generated_data, epochs=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
