{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A30\n",
      "(872, 4500, 14) (872,)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, auc, classification_report, roc_auc_score\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, n_features=14, seq_len=4500):\n",
    "        super(Conv1DGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Calculate initial sequence length after upsampling\n",
    "        self.init_size = seq_len // 64  # Will be upsampled 6 times (2^6 = 64)\n",
    "        \n",
    "        # Project latent to initial feature map\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * self.init_size),\n",
    "            nn.BatchNorm1d(256 * self.init_size),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Transposed convolutions for upsampling\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 256 -> 128 channels, 2x upsampling\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 128 -> 64 channels, 2x upsampling  \n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 64 -> 32 channels, 2x upsampling\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 32 -> 16 channels, 2x upsampling\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 16 -> 8 channels, 2x upsampling\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 8 -> n_features, final upsampling\n",
    "            nn.ConvTranspose1d(8, n_features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Project and reshape\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], 256, self.init_size)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        out = self.conv_blocks(out)\n",
    "        \n",
    "        # Adjust to exact sequence length if needed\n",
    "        if out.shape[2] != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Transpose to (batch, seq_len, features)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "class Conv1DDiscriminator(nn.Module):\n",
    "    def __init__(self, n_features=14, seq_len=4500):\n",
    "        super(Conv1DDiscriminator, self).__init__()\n",
    "        \n",
    "        # Convolutional feature extraction\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Input: (batch, 14, 4500)\n",
    "            nn.Conv1d(n_features, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        self.conv_output_size = self._get_conv_output_size(seq_len)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * self.conv_output_size, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self, seq_len):\n",
    "        # Calculate output size after 6 conv layers with stride 2\n",
    "        size = seq_len\n",
    "        for _ in range(6):\n",
    "            size = (size - 4 + 2) // 2 + 1\n",
    "        return size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose from (batch, seq_len, features) to (batch, features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply conv blocks\n",
    "        features = self.conv_blocks(x)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        features = features.view(features.shape[0], -1)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "# Improved Gradient Penalty\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1).to(device)\n",
    "    \n",
    "    # Expand alpha to match sample dimensions\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    \n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    gradients = grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates).to(device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# WGANS Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Conv1D WGAN training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generator, discriminator\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Use the Conv1D version\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m trained_generator, trained_discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_conv1d_wgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Update the variable name for consistency with later cells\u001b[39;00m\n\u001b[1;32m     91\u001b[0m Generator \u001b[38;5;241m=\u001b[39m trained_generator\n",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m, in \u001b[0;36mtrain_conv1d_wgan\u001b[0;34m(normal_data, device, n_epochs, batch_size, lr_g, lr_d)\u001b[0m\n\u001b[1;32m     53\u001b[0m fake_validity \u001b[38;5;241m=\u001b[39m discriminator(fake_samples)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Gradient penalty\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m gp \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradient_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Discriminator loss\u001b[39;00m\n\u001b[1;32m     59\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(real_validity) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fake_validity) \u001b[38;5;241m+\u001b[39m lambda_gp \u001b[38;5;241m*\u001b[39m gp\n",
      "Cell \u001b[0;32mIn[2], line 150\u001b[0m, in \u001b[0;36mcompute_gradient_penalty\u001b[0;34m(discriminator, real_samples, fake_samples, device)\u001b[0m\n\u001b[1;32m    139\u001b[0m d_interpolates \u001b[38;5;241m=\u001b[39m discriminator(interpolates)\n\u001b[1;32m    141\u001b[0m gradients \u001b[38;5;241m=\u001b[39m grad(\n\u001b[1;32m    142\u001b[0m     outputs\u001b[38;5;241m=\u001b[39md_interpolates,\n\u001b[1;32m    143\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minterpolates,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     only_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    148\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mgradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m gradient_penalty \u001b[38;5;241m=\u001b[39m ((gradients\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient_penalty\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "def train_conv1d_wgan(normal_data, device, n_epochs=100, batch_size=16, lr_g=0.0001, lr_d=0.0004):\n",
    "    \"\"\"\n",
    "    Train WGAN with Conv1D architecture\n",
    "    \"\"\"\n",
    "    latent_dim = 100\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = Conv1DGenerator(latent_dim).to(device)\n",
    "    discriminator = Conv1DDiscriminator().to(device)\n",
    "    \n",
    "    # Initialize weights\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training parameters\n",
    "    lambda_gp = 10\n",
    "    n_critic = 5\n",
    "    \n",
    "    # Convert to tensor and create dataloader\n",
    "    dataset = TensorDataset(torch.tensor(normal_data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(\"Starting Conv1D WGAN training...\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for i, (real_samples,) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            batch_size_actual = real_samples.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_D.zero_grad()\n",
    "                \n",
    "                # Real samples\n",
    "                real_validity = discriminator(real_samples)\n",
    "                \n",
    "                # Fake samples\n",
    "                z = torch.randn(batch_size_actual, latent_dim).to(device)\n",
    "                fake_samples = generator(z).detach()\n",
    "                fake_validity = discriminator(fake_samples)\n",
    "                \n",
    "                # Gradient penalty\n",
    "                gp = compute_gradient_penalty(discriminator, real_samples, fake_samples, device)\n",
    "                \n",
    "                # Discriminator loss\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            if i % n_critic == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size_actual, latent_dim).to(device)\n",
    "                fake_samples = generator(z)\n",
    "                fake_validity = discriminator(fake_samples)\n",
    "                \n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "            \n",
    "            d_losses.append(d_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            avg_d_loss = np.mean(d_losses)\n",
    "            avg_g_loss = np.mean(g_losses) if g_losses else 0\n",
    "            print(f\"Epoch [{epoch}/{n_epochs}], D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "# Use the Conv1D version\n",
    "trained_generator, trained_discriminator = train_conv1d_wgan(normal_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ee542",
   "metadata": {},
   "source": [
    "# Generate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809752",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim).to(device)\n",
    "num_samples = len(data[label == 0])  # Number of synthetic samples to generate\n",
    "batch_size = 64  # Generate in batches to avoid memory issues\n",
    "\n",
    "generated_list = []\n",
    "for i in range(0, num_samples, batch_size):\n",
    "\tcurrent_batch = min(batch_size, num_samples - i)\n",
    "\tz = torch.randn(current_batch, latent_dim).to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\tbatch_samples = generator(z).cpu().numpy()\n",
    "\t# batch_samples shape: (current_batch, 14, 10, 15, 30)\n",
    "\tbatch_samples = batch_samples.reshape(current_batch, 14, -1).transpose(0, 2, 1)  # (current_batch, 4500, 14)\n",
    "\tgenerated_list.append(batch_samples)\n",
    "\n",
    "generated_samples = np.concatenate(generated_list, axis=0)  # (num_samples, 4500, 14)\n",
    "\n",
    "combine_data_normal = np.concatenate((generated_samples, normal_data), axis=0)  # Combine real and generated data\n",
    "combine_labels_normal = np.concatenate((np.zeros(num_samples), normal_label), axis=0)  # Labels: 0 for generated, 1 for real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901a15",
   "metadata": {},
   "source": [
    "# Processing: Mel Spec > Resizing > Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01061dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and convert to 3-channel image\n",
    "def resize_spectrogram(spectrogram):\n",
    "    spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-6)\n",
    "    spectrogram = np.uint8(spectrogram.cpu().numpy() * 255)\n",
    "    spectrogram = np.stack([spectrogram] * 3, axis=-1)\n",
    "    image = Image.fromarray(spectrogram)\n",
    "    image = transforms.Resize((224, 224))(image)\n",
    "    return transforms.ToTensor()(image)\n",
    "\n",
    "# Process dataset\n",
    "def process_dataset(data):\n",
    "    num_samples, _, num_channels = data.shape\n",
    "    features = np.zeros((num_samples, num_channels, 4096))\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=2500000, n_mels=128).to(device)\n",
    "    model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).to(device)\n",
    "    model.classifier = model.classifier[:-3]\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_channels):\n",
    "            ts = torch.tensor(data[i, :, j], dtype=torch.float32).to(device)\n",
    "            mel = mel_transform(ts)\n",
    "            img = resize_spectrogram(mel)\n",
    "            with torch.no_grad():\n",
    "                feat = model(img.unsqueeze(0).to(device))\n",
    "            features[i, j, :] = feat.squeeze().cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f39ba8",
   "metadata": {},
   "source": [
    "# AE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4096):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 4), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "# Train autoencoder\n",
    "def train_autoencoder(features, epochs=20, batch_size=128):\n",
    "    x = torch.tensor(features.reshape(-1, 4096), dtype=torch.float32).to(device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=batch_size, shuffle=True)\n",
    "    model = Autoencoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}\")\n",
    "    return model\n",
    "\n",
    "# Compute reconstruction errors\n",
    "def compute_reconstruction_loss(model, data):\n",
    "    model.eval()\n",
    "    x = torch.tensor(data.reshape(-1, 4096), dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    loader = DataLoader(TensorDataset(x), batch_size=64)\n",
    "    loss = []\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs)\n",
    "            batch_errors = criterion(outputs, inputs).mean(dim=1)\n",
    "            loss.extend(batch_errors.cpu().numpy())\n",
    "    return np.array(loss)\n",
    "\n",
    "# 2. Find best threshold based on F1 score\n",
    "def find_best_threshold(errors, labels):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(min(errors), max(errors), 100):\n",
    "        preds = (errors > threshold).astype(int)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def evaluate_on_test_with_threshold_search(model, X_test, y_test):\n",
    "    # Step 1: Flatten the test data\n",
    "    n_samples, n_segments, n_features = X_test.shape\n",
    "    X_test_flat = X_test.reshape(-1, n_features)\n",
    "\n",
    "    # Step 2: Compute reconstruction errors per segment\n",
    "    segment_errors = compute_reconstruction_loss(model, X_test_flat)\n",
    "\n",
    "    # Step 3: Aggregate errors per sample (mean over segments)\n",
    "    sample_errors = segment_errors.reshape(n_samples, n_segments).mean(axis=1)\n",
    "\n",
    "    # Step 4: Find best threshold based on F1 score\n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    for threshold in np.linspace(sample_errors.min(), sample_errors.max(), 100):\n",
    "        preds = (sample_errors > threshold).astype(int)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    # Step 5: Predict using best threshold\n",
    "    test_preds = (sample_errors > best_threshold).astype(int)\n",
    "\n",
    "    # Step 6: Evaluate\n",
    "    print(f\"Best Threshold = {best_threshold:.6f}, Best F1 Score = {best_f1:.4f}\")\n",
    "    print(\"Evaluation on Test Set:\")\n",
    "    print(\"Accuracy =\", accuracy_score(y_test, test_preds))\n",
    "    print(\"Precision =\", precision_score(y_test, test_preds))\n",
    "    print(\"Recall =\", recall_score(y_test, test_preds))\n",
    "    print(\"F1 Score =\", f1_score(y_test, test_preds))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81797ba0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data = np.concatenate((combine_data_normal, data[label == 1]), axis=0)  # Combine real and generated data\n",
    "combine_label = np.concatenate((np.zeros(len(combine_labels_normal)), label[label == 1]), axis=0)  # Labels: 0 for real, 0 for generated\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scaled_data = StandardScaler().fit_transform(combine_data.reshape(-1, combine_data.shape[-1])).reshape(combine_data.shape)\n",
    "features = process_dataset(scaled_data)\n",
    "print(\"Features shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64ca03",
   "metadata": {},
   "source": [
    "# Cross Validation with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(features, combine_label)):\n",
    "    \n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Get full fold data\n",
    "    fold_data, fold_labels = features[train_idx], combine_label[train_idx]\n",
    "    val_data, val_labels = features[val_idx], combine_label[val_idx]\n",
    "   \n",
    "    # Split into training and validation folds\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(fold_data, fold_labels, test_size=0.2, shuffle=True, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(val_data, val_labels, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Train autoencoder on the training fold\n",
    "    model = train_autoencoder(X_train[y_train == 0], epochs=15, batch_size=32)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    val_errors_normal = compute_reconstruction_loss(model, X_val[y_val == 0])\n",
    "    val_errors_abnormal = compute_reconstruction_loss(model, X_val[y_val == 1])\n",
    "    val_errors = np.concatenate([val_errors_normal, val_errors_abnormal])\n",
    "    y_val_combined = np.concatenate([np.zeros(len(val_errors_normal)), np.ones(len(val_errors_abnormal))])\n",
    "    \n",
    "    threshold, best_f1 = find_best_threshold(val_errors, y_val_combined)\n",
    "    print(f\"Best threshold: {threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "    # Plot histogram of reconstruction errors on both normal and abnormal samples\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(val_errors_normal, bins=50, alpha=0.5, label='Normal Samples', color='blue')\n",
    "    plt.hist(val_errors_abnormal, bins=50, alpha=0.5, label='Abnormal Samples', color='red')\n",
    "    plt.axvline(threshold, color='black', linestyle='--', label='Threshold')\n",
    "    plt.title('Reconstruction Errors on Validation Set')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    val_errors_test = compute_reconstruction_loss(model, X_test)\n",
    "    \n",
    "\n",
    "    # Evaluate on test set\n",
    "    evaluate_on_test_with_threshold_search(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25980ecb",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "\n",
    "Accuracy = 0.9166666666666666\n",
    "\n",
    "\n",
    "Precision = 0.7142857142857143\n",
    "\n",
    "Recall = 0.3125\n",
    "\n",
    "F1 Score = 0.43478260869565216\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    " [[138   2]\n",
    "\n",
    " [ 11   5]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ffa6f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
