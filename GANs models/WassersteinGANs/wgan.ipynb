{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio, torchvision.transforms as transforms, matplotlib.pyplot as plt, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ad_utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "cuda1 = torch.device(\"cuda:1\")\n",
    "device = cuda1\n",
    "print(torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"No GPU available\")\n",
    "\n",
    "data = np.load(\"../../hvcm/RFQ.npy\", allow_pickle=True)\n",
    "label = np.load(\"../../hvcm/RFQ_labels.npy\", allow_pickle=True)\n",
    "label = label[:, 1]  # Assuming the second column is the label\n",
    "label = (label == \"Fault\").astype(int)  # Convert to binary labels\n",
    "print(data.shape, label.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "\n",
    "normal_data = data[label == 0]\n",
    "faulty_data = data[label == 1]\n",
    "\n",
    "normal_label = label[label == 0]\n",
    "faulty_label = label[label == 1]\n",
    "\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(normal_data, normal_label, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(faulty_data, faulty_label, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14127475",
   "metadata": {},
   "source": [
    "# Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "\n",
    "# Attention mechanism for enhanced anomaly detection\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv1d(in_channels, in_channels // 8, 1)\n",
    "        self.key_conv = nn.Conv1d(in_channels, in_channels // 8, 1)\n",
    "        self.value_conv = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, length = x.size()\n",
    "        \n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, length).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, length)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, length)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, channels, length)\n",
    "        \n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# Enhanced WGAN Generator with attention and residual connections\n",
    "class EnhancedWGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, n_features=14, seq_len=4500):\n",
    "        super(EnhancedWGANGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Calculate reasonable initial size\n",
    "        self.init_size = max(seq_len // 64, 32)\n",
    "        \n",
    "        # Initial projection with residual connection\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * self.init_size),\n",
    "            nn.BatchNorm1d(256 * self.init_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks for better gradient flow\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            # Block 1: 256 -> 128 channels\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            # Block 2: 128 -> 64 channels with attention\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            # Block 3: 64 -> 32 channels\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ),\n",
    "            # Final block: 32 -> n_features\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(32, n_features, kernel_size=4, stride=2, padding=1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Self-attention for anomaly-aware generation\n",
    "        self.attention = SelfAttention(64)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Project latent to initial conv size\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], 256, self.init_size)\n",
    "        \n",
    "        # Progressive upsampling with attention\n",
    "        for i, block in enumerate(self.conv_blocks):\n",
    "            out = block(out)\n",
    "            # Apply attention after second block (64 channels)\n",
    "            if i == 1:\n",
    "                out = self.attention(out)\n",
    "        \n",
    "        # Ensure exact sequence length\n",
    "        if out.shape[2] != self.seq_len:\n",
    "            out = nn.functional.interpolate(out, size=self.seq_len, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Return as (batch_size, seq_len, n_features)\n",
    "        return out.transpose(1, 2)\n",
    "\n",
    "# Multi-scale WGAN Critic for better anomaly detection\n",
    "class MultiScaleWGANCritic(nn.Module):\n",
    "    def __init__(self, n_features=14, seq_len=4500):\n",
    "        super(MultiScaleWGANCritic, self).__init__()\n",
    "        \n",
    "        # Multi-scale convolutional paths\n",
    "        self.scale1_conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(n_features, 32, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.scale2_conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(n_features, 32, kernel_size=7, stride=1, padding=3)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.scale3_conv = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(n_features, 32, kernel_size=15, stride=1, padding=7)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(192, 128, kernel_size=3, stride=1, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # Further downsampling\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._get_conv_output_size(seq_len)\n",
    "        \n",
    "        # Classifier with better regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Linear(512 * self.conv_output_size, 256)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.utils.spectral_norm(nn.Linear(256, 64)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.utils.spectral_norm(nn.Linear(64, 1))\n",
    "        )\n",
    "        \n",
    "    def _get_conv_output_size(self, seq_len):\n",
    "        size = seq_len\n",
    "        # scale conv: stride=2, then downsample: stride=2, stride=2\n",
    "        size = (size - 4 + 2) // 2 + 1  # First downsample\n",
    "        size = (size - 4 + 2) // 2 + 1  # Second downsample\n",
    "        size = (size - 4 + 2) // 2 + 1  # Third downsample\n",
    "        return size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (batch_size, seq_len, n_features)\n",
    "        x = x.transpose(1, 2)  # Convert to (batch_size, n_features, seq_len)\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        scale1_features = self.scale1_conv(x)\n",
    "        scale2_features = self.scale2_conv(x)\n",
    "        scale3_features = self.scale3_conv(x)\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        multi_scale_features = torch.cat([scale1_features, scale2_features, scale3_features], dim=1)\n",
    "        \n",
    "        # Fusion and further processing\n",
    "        fused_features = self.fusion(multi_scale_features)\n",
    "        features = self.downsample(fused_features)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        features = features.view(features.size(0), -1)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Enhanced gradient penalty computation\n",
    "def compute_enhanced_gradient_penalty(critic, real_samples, fake_samples, device, lambda_gp=10):\n",
    "    batch_size = real_samples.size(0)\n",
    "    \n",
    "    # Random interpolation coefficient\n",
    "    alpha = torch.rand(batch_size, 1, 1, device=device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    \n",
    "    # Create interpolated samples\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    \n",
    "    # Get critic scores\n",
    "    d_interpolates = critic(interpolates)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates, device=device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Reshape gradients\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    \n",
    "    # Compute gradient penalty with small epsilon for stability\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# Enhanced training function with improved stability\n",
    "def train_enhanced_wgan(normal_data, device, n_epochs=100, batch_size=32, lr_g=1e-4, lr_d=1e-4):\n",
    "    \"\"\"\n",
    "    Enhanced WGAN training with multi-scale critic and attention generator\n",
    "    \"\"\"\n",
    "    print(f\"Starting Enhanced WGAN Training with Multi-Scale Architecture\")\n",
    "    print(f\"Data shape: {normal_data.shape}\")\n",
    "    print(f\"Data range: [{normal_data.min():.4f}, {normal_data.max():.4f}]\")\n",
    "    \n",
    "    # Model parameters\n",
    "    latent_dim = 100\n",
    "    n_features = normal_data.shape[-1]\n",
    "    seq_len = normal_data.shape[1]\n",
    "    \n",
    "    # Initialize enhanced models\n",
    "    generator = EnhancedWGANGenerator(latent_dim, n_features, seq_len).to(device)\n",
    "    critic = MultiScaleWGANCritic(n_features, seq_len).to(device)\n",
    "    \n",
    "    # Xavier initialization for stability\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.xavier_normal_(m.weight, gain=0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    generator.apply(init_weights)\n",
    "    critic.apply(init_weights)\n",
    "    \n",
    "    # Optimizers with different learning rates\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.0, 0.9))\n",
    "    optimizer_C = optim.Adam(critic.parameters(), lr=lr_d, betas=(0.0, 0.9))\n",
    "    \n",
    "    # Training parameters\n",
    "    n_critic = 5  # Train critic more often\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataset = TensorDataset(torch.tensor(normal_data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(f\"Training Parameters:\")\n",
    "    print(f\"  Epochs: {n_epochs}, Batch Size: {batch_size}\")\n",
    "    print(f\"  Generator LR: {lr_g}, Critic LR: {lr_d}\")\n",
    "    print(f\"  Critic Updates per Generator Update: {n_critic}\")\n",
    "    \n",
    "    # Training history\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    wasserstein_distances = []\n",
    "    \n",
    "    print(\"\\nStarting Training...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_d_losses = []\n",
    "        epoch_g_losses = []\n",
    "        epoch_wd = []\n",
    "        \n",
    "        for i, (real_samples,) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            batch_size_actual = real_samples.size(0)\n",
    "            \n",
    "            # ========================\n",
    "            # Train Critic\n",
    "            # ========================\n",
    "            for critic_iter in range(n_critic):\n",
    "                optimizer_C.zero_grad()\n",
    "                \n",
    "                # Real samples\n",
    "                real_validity = critic(real_samples)\n",
    "                \n",
    "                # Generate fake samples\n",
    "                z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "                fake_samples = generator(z).detach()\n",
    "                fake_validity = critic(fake_samples)\n",
    "                \n",
    "                # Wasserstein distance\n",
    "                wasserstein_distance = torch.mean(real_validity) - torch.mean(fake_validity)\n",
    "                \n",
    "                # Gradient penalty\n",
    "                gradient_penalty = compute_enhanced_gradient_penalty(\n",
    "                    critic, real_samples, fake_samples, device, lambda_gp\n",
    "                )\n",
    "                \n",
    "                # Critic loss\n",
    "                c_loss = -wasserstein_distance + lambda_gp * gradient_penalty\n",
    "                \n",
    "                c_loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "                \n",
    "                optimizer_C.step()\n",
    "                \n",
    "                if critic_iter == n_critic - 1:\n",
    "                    epoch_d_losses.append(c_loss.item())\n",
    "                    epoch_wd.append(wasserstein_distance.item())\n",
    "            \n",
    "            # ========================\n",
    "            # Train Generator\n",
    "            # ========================\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake samples\n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_samples = generator(z)\n",
    "            fake_validity = critic(fake_samples)\n",
    "            \n",
    "            # Generator loss\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)\n",
    "            \n",
    "            optimizer_G.step()\n",
    "            \n",
    "            epoch_g_losses.append(g_loss.item())\n",
    "        \n",
    "        # Calculate epoch averages\n",
    "        avg_d_loss = np.mean(epoch_d_losses)\n",
    "        avg_g_loss = np.mean(epoch_g_losses)\n",
    "        avg_wd = np.mean(epoch_wd)\n",
    "        \n",
    "        d_losses.append(avg_d_loss)\n",
    "        g_losses.append(avg_g_loss)\n",
    "        wasserstein_distances.append(avg_wd)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "            print(f\"Epoch [{epoch+1:3d}/{n_epochs}] | \"\n",
    "                  f\"C Loss: {avg_d_loss:8.4f} | \"\n",
    "                  f\"G Loss: {avg_g_loss:8.4f} | \"\n",
    "                  f\"W-Dist: {avg_wd:8.4f}\")\n",
    "            \n",
    "            # Enhanced stability check\n",
    "            if len(d_losses) >= 10:\n",
    "                recent_d_std = np.std(d_losses[-10:])\n",
    "                recent_g_std = np.std(g_losses[-10:])\n",
    "                \n",
    "                if recent_d_std < 10.0 and recent_g_std < 10.0:\n",
    "                    print(\"         âœ… Training highly stable\")\n",
    "                elif recent_d_std < 50.0 and recent_g_std < 50.0:\n",
    "                    print(\"         ðŸ”„ Training moderately stable\")\n",
    "                else:\n",
    "                    print(\"         âš ï¸  Training showing variation\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Enhanced WGAN training completed!\")\n",
    "    \n",
    "    return generator, critic, d_losses, g_losses, wasserstein_distances\n",
    "\n",
    "# Enhanced sample generation\n",
    "def generate_enhanced_samples(generator, num_samples, latent_dim, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate samples using enhanced generator\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    generated_batches = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            current_batch_size = end - start\n",
    "            \n",
    "            # Generate noise\n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            \n",
    "            # Generate samples\n",
    "            batch_generated = generator(z)\n",
    "            generated_batches.append(batch_generated.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(generated_batches, axis=0)\n",
    "\n",
    "# Enhanced visualization\n",
    "def plot_enhanced_training_curves(d_losses, g_losses, wasserstein_distances):\n",
    "    \"\"\"\n",
    "    Plot training curves with enhanced visualization\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Critic loss\n",
    "    axes[0, 0].plot(d_losses, label='Critic Loss', color='blue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Critic Loss Over Time')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Generator loss\n",
    "    axes[0, 1].plot(g_losses, label='Generator Loss', color='red', alpha=0.7)\n",
    "    axes[0, 1].set_title('Generator Loss Over Time')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wasserstein distance\n",
    "    axes[1, 0].plot(wasserstein_distances, label='Wasserstein Distance', color='green', alpha=0.7)\n",
    "    axes[1, 0].set_title('Wasserstein Distance Over Time')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Distance')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Moving average of losses for stability analysis\n",
    "    window_size = 10\n",
    "    if len(d_losses) >= window_size:\n",
    "        d_ma = pd.Series(d_losses).rolling(window=window_size).mean()\n",
    "        g_ma = pd.Series(g_losses).rolling(window=window_size).mean()\n",
    "        \n",
    "        axes[1, 1].plot(d_ma, label=f'Critic Loss (MA-{window_size})', color='blue', alpha=0.7)\n",
    "        axes[1, 1].plot(g_ma, label=f'Generator Loss (MA-{window_size})', color='red', alpha=0.7)\n",
    "        axes[1, 1].set_title('Training Stability (Moving Average)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd307",
   "metadata": {},
   "source": [
    "# WGANS Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0267ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the enhanced WGAN with multi-scale critic and attention generator\n",
    "print(\"Training Enhanced WGAN with Multi-Scale Architecture...\")\n",
    "generator, critic, d_history, g_history, wd_history = train_enhanced_wgan(\n",
    "    X_train_normal,\n",
    "    device,\n",
    "    n_epochs=80,\n",
    "    batch_size=32,\n",
    "    lr_g=1e-4,     # Optimized learning rates\n",
    "    lr_d=2e-4\n",
    ")\n",
    "\n",
    "# Plot enhanced training curves\n",
    "plot_enhanced_training_curves(d_history, g_history, wd_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf55b53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40ee542",
   "metadata": {},
   "source": [
    "# Generate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples using enhanced generator\n",
    "print(\"Generating synthetic samples with Enhanced WGAN...\")\n",
    "generated_data = generate_enhanced_samples(\n",
    "    generator,\n",
    "    len(X_train_normal),\n",
    "    latent_dim=100,\n",
    "    device=device,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Generated data shape: {generated_data.shape}\")\n",
    "print(f\"Generated data range: [{generated_data.min():.4f}, {generated_data.max():.4f}]\")\n",
    "print(f\"Original data range: [{normal_data.min():.4f}, {normal_data.max():.4f}]\")\n",
    "\n",
    "# Combine with real data\n",
    "combine_data_normal = np.concatenate((generated_data, normal_data), axis=0)\n",
    "combine_labels_normal = np.concatenate((np.zeros(len(generated_data)), normal_label), axis=0)\n",
    "\n",
    "print(\"âœ… Enhanced WGAN training and generation completed!\")\n",
    "\n",
    "# Visualize sample comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Select random samples for visualization\n",
    "n_viz = 3\n",
    "real_indices = np.random.choice(len(normal_data), n_viz, replace=False)\n",
    "fake_indices = np.random.choice(len(generated_data), n_viz, replace=False)\n",
    "\n",
    "for i in range(n_viz):\n",
    "    # Real samples\n",
    "    axes[0, i].plot(normal_data[real_indices[i], :, 0], alpha=0.7, label='Real')\n",
    "    axes[0, i].set_title(f'Real Sample {i+1} (Feature 1)')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Generated samples\n",
    "    axes[1, i].plot(generated_data[fake_indices[i], :, 0], alpha=0.7, label='Generated', color='red')\n",
    "    axes[1, i].set_title(f'Generated Sample {i+1} (Feature 1)')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc93f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FID SCORE EVALUATION\n",
    "# ===============================\n",
    "\n",
    "# Test the simplified FID calculation\n",
    "print(\"Testing simplified FID calculation...\")\n",
    "\n",
    "# Use smaller subsets for testing\n",
    "test_real = X_train_normal[:100]  # Use 100 samples for testing\n",
    "test_generated = generated_data[:100]\n",
    "\n",
    "print(f\"Test real data shape: {test_real.shape}\")\n",
    "print(f\"Test generated data shape: {test_generated.shape}\")\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid_score(\n",
    "    real_data=test_real,\n",
    "    fake_data=test_generated,\n",
    "    device=device,\n",
    "    sample_rate=1000,\n",
    ")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! FID Score: {fid_score:.4f}\")\n",
    "    \n",
    "    # Interpret the score\n",
    "    if fid_score < 10:\n",
    "        quality = \"Excellent\"\n",
    "    elif fid_score < 25:\n",
    "        quality = \"Good\"\n",
    "    elif fid_score < 50:\n",
    "        quality = \"Fair\"\n",
    "    elif fid_score < 100:\n",
    "        quality = \"Poor\"\n",
    "    else:\n",
    "        quality = \"Very Poor\"\n",
    "    \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "else:\n",
    "    print(\"âŒ FID calculation failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd511bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_cross_validation_experiment(X_test_normal, X_test_faulty, device, generated_data, epochs=200, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
